{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "cnn_model.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tsa599bdj2Vb",
        "YgDckS8BOsMz",
        "qle7DkoeZUmn",
        "F3hx_YGxAWQE"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mehJdtQSvbBN"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "names=['emotion','pixels','usage']"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oh8xVaYj2VP"
      },
      "source": [
        "# Load inputs and targets from fer2013.csv (Google Colab)\n",
        "\n",
        "Use one of the following 2 ways to load data into colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9an38nL-tkXJ"
      },
      "source": [
        "## From Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJBDKljNv0tD"
      },
      "source": [
        "# if you are using google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pm24sQrv3rC"
      },
      "source": [
        "# you may need to change this file path\n",
        "df=pd.read_csv('/content/drive/My Drive/project/fer2013/fer2013.csv',names=names, na_filter=False)\n",
        "train_df = df[df['usage']=='Training']\n",
        "validate_df = df[df['usage']=='PublicTest']\n",
        "test_df = df[df['usage']=='PrivateTest']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0PUSKPzta3v"
      },
      "source": [
        "## Upload data from Hard Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "Z4hlMhIrtO_s",
        "outputId": "8fac773b-0b8f-432a-b815-a46b71243abd"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ccd790fb-ac66-4318-9a70-35a00ad885d1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ccd790fb-ac66-4318-9a70-35a00ad885d1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving fer2013.csv to fer2013.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfKJjCNlt1yS"
      },
      "source": [
        "df = pd.read_csv('fer2013.csv')\n",
        "train_df = df[df['Usage']=='Training']\n",
        "validate_df = df[df['Usage']=='PublicTest']\n",
        "test_df = df[df['Usage']=='PrivateTest']"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOPTCQLWu_gO"
      },
      "source": [
        "# Running from Local (eg. with Jupyter)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37YJ_Se7vK61"
      },
      "source": [
        "df = pd.read_csv('../data/raw/fer2013/fer2013.csv')\n",
        "train_df = df[df['Usage']=='Training']\n",
        "validate_df = df[df['Usage']=='PublicTest']\n",
        "test_df = df[df['Usage']=='PrivateTest']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dprFIGKunmz"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oX5fVZcj2VT"
      },
      "source": [
        "from skimage.transform import resize\n",
        "def load_from_df(df):\n",
        "    imgs = df['pixels']\n",
        "    t = df['emotion']\n",
        "    inputs = []\n",
        "    targets = []\n",
        "    for i in imgs.keys():\n",
        "        x = [int(p) for p in imgs[i].split()]\n",
        "        x = np.reshape(x,(48,48))\n",
        "        # resize_x = resize(x, (112, 112), preserve_range=True)\n",
        "        y = [0 for i in range(7)]\n",
        "        y[int(t[i])] = 1\n",
        "        inputs.append(x)\n",
        "        targets.append(y)\n",
        "    inputs = np.array(inputs,dtype=np.float32)\n",
        "    inputs = np.expand_dims(inputs,axis=3)\n",
        "    inputs /= 255\n",
        "    targets = np.array(targets, dtype=np.float32)\n",
        "    return inputs, targets"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_rCiYKZj2VV"
      },
      "source": [
        "v_inputs, v_targets = load_from_df(validate_df)\n",
        "t_inputs, t_targets = load_from_df(test_df)\n",
        "tr_inputs, tr_targets = load_from_df(train_df)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P70ORGGvk4O"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsa599bdj2Vb"
      },
      "source": [
        "## Base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_FmhLE_j2Vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9326f899-360d-4ac9-9af8-a1de6e85e249"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "proto = Sequential()\n",
        "\n",
        "proto.add(Conv2D(filters=32,kernel_size=2,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "proto.add(MaxPooling2D(pool_size=2))\n",
        "proto.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'))\n",
        "proto.add(MaxPooling2D(pool_size=2))\n",
        "proto.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
        "proto.add(MaxPooling2D(pool_size=2))\n",
        "proto.add(Flatten())\n",
        "proto.add(Dense(256,activation='relu'))\n",
        "proto.add(Dense(512,activation='relu'))\n",
        "proto.add(Dense(7,activation='softmax'))\n",
        "\n",
        "proto.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_31\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_93 (Conv2D)           (None, 48, 48, 32)        160       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_85 (MaxPooling (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_94 (Conv2D)           (None, 24, 24, 64)        8256      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_86 (MaxPooling (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_95 (Conv2D)           (None, 12, 12, 128)       32896     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_87 (MaxPooling (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_31 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_93 (Dense)             (None, 256)               1179904   \n",
            "_________________________________________________________________\n",
            "dense_94 (Dense)             (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "dense_95 (Dense)             (None, 7)                 3591      \n",
            "=================================================================\n",
            "Total params: 1,356,391\n",
            "Trainable params: 1,356,391\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CikBHCY2j2Vg"
      },
      "source": [
        "Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odRnY0Gtj2Vg"
      },
      "source": [
        "proto.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8c5l-nmj2Vj"
      },
      "source": [
        "train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2r1QmlHj2Vj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "927066a3-8f58-4317-a4ef-d9033fd92d73"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='mycnn_prototype.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist = proto.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(v_inputs , v_targets),\n",
        "          epochs= epoch, batch_size=128, callbacks=[checkpointer], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 1.7084 - accuracy: 0.3104\n",
            "Epoch 00001: val_loss improved from inf to 1.58743, saving model to mycnn_prototype.hdf5\n",
            "225/225 [==============================] - 3s 11ms/step - loss: 1.7068 - accuracy: 0.3110 - val_loss: 1.5874 - val_accuracy: 0.3792\n",
            "Epoch 2/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.4923 - accuracy: 0.4227\n",
            "Epoch 00002: val_loss improved from 1.58743 to 1.40583, saving model to mycnn_prototype.hdf5\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 1.4910 - accuracy: 0.4232 - val_loss: 1.4058 - val_accuracy: 0.4539\n",
            "Epoch 3/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.3540 - accuracy: 0.4753\n",
            "Epoch 00003: val_loss improved from 1.40583 to 1.32423, saving model to mycnn_prototype.hdf5\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 1.3540 - accuracy: 0.4751 - val_loss: 1.3242 - val_accuracy: 0.4887\n",
            "Epoch 4/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 1.2540 - accuracy: 0.5208\n",
            "Epoch 00004: val_loss improved from 1.32423 to 1.27978, saving model to mycnn_prototype.hdf5\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 1.2527 - accuracy: 0.5213 - val_loss: 1.2798 - val_accuracy: 0.5071\n",
            "Epoch 5/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.1680 - accuracy: 0.5560\n",
            "Epoch 00005: val_loss improved from 1.27978 to 1.23584, saving model to mycnn_prototype.hdf5\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 1.1680 - accuracy: 0.5560 - val_loss: 1.2358 - val_accuracy: 0.5269\n",
            "Epoch 6/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0900 - accuracy: 0.5876\n",
            "Epoch 00006: val_loss improved from 1.23584 to 1.22838, saving model to mycnn_prototype.hdf5\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 1.0900 - accuracy: 0.5876 - val_loss: 1.2284 - val_accuracy: 0.5366\n",
            "Epoch 7/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 1.0088 - accuracy: 0.6202\n",
            "Epoch 00007: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 1.0093 - accuracy: 0.6198 - val_loss: 1.2323 - val_accuracy: 0.5436\n",
            "Epoch 8/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9356 - accuracy: 0.6512\n",
            "Epoch 00008: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 0.9354 - accuracy: 0.6514 - val_loss: 1.2562 - val_accuracy: 0.5481\n",
            "Epoch 9/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8483 - accuracy: 0.6871\n",
            "Epoch 00009: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 0.8483 - accuracy: 0.6871 - val_loss: 1.2906 - val_accuracy: 0.5556\n",
            "Epoch 10/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7574 - accuracy: 0.7211\n",
            "Epoch 00010: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.7564 - accuracy: 0.7212 - val_loss: 1.3280 - val_accuracy: 0.5469\n",
            "Epoch 11/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.6545 - accuracy: 0.7598\n",
            "Epoch 00011: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.6566 - accuracy: 0.7584 - val_loss: 1.4213 - val_accuracy: 0.5550\n",
            "Epoch 12/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.5634 - accuracy: 0.7939\n",
            "Epoch 00012: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.5634 - accuracy: 0.7938 - val_loss: 1.5749 - val_accuracy: 0.5545\n",
            "Epoch 13/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4684 - accuracy: 0.8312\n",
            "Epoch 00013: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.4692 - accuracy: 0.8310 - val_loss: 1.7175 - val_accuracy: 0.5375\n",
            "Epoch 14/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3780 - accuracy: 0.8649\n",
            "Epoch 00014: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.3779 - accuracy: 0.8650 - val_loss: 1.9216 - val_accuracy: 0.5559\n",
            "Epoch 15/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.3099 - accuracy: 0.8885\n",
            "Epoch 00015: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 0.3108 - accuracy: 0.8881 - val_loss: 2.0988 - val_accuracy: 0.5433\n",
            "Epoch 16/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.2464 - accuracy: 0.9154\n",
            "Epoch 00016: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 0.2463 - accuracy: 0.9153 - val_loss: 2.3248 - val_accuracy: 0.5383\n",
            "Epoch 17/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.1910 - accuracy: 0.9343\n",
            "Epoch 00017: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 11ms/step - loss: 0.1919 - accuracy: 0.9337 - val_loss: 2.5976 - val_accuracy: 0.5500\n",
            "Epoch 18/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.1511 - accuracy: 0.9484\n",
            "Epoch 00018: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.1514 - accuracy: 0.9482 - val_loss: 2.7116 - val_accuracy: 0.5386\n",
            "Epoch 19/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1394 - accuracy: 0.9522\n",
            "Epoch 00019: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.1394 - accuracy: 0.9522 - val_loss: 2.8485 - val_accuracy: 0.5419\n",
            "Epoch 20/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9596\n",
            "Epoch 00020: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.1222 - accuracy: 0.9596 - val_loss: 3.0846 - val_accuracy: 0.5414\n",
            "Epoch 21/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0962 - accuracy: 0.9700\n",
            "Epoch 00021: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0964 - accuracy: 0.9698 - val_loss: 3.3220 - val_accuracy: 0.5550\n",
            "Epoch 22/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9572\n",
            "Epoch 00022: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.1239 - accuracy: 0.9573 - val_loss: 3.3314 - val_accuracy: 0.5383\n",
            "Epoch 23/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0840 - accuracy: 0.9738\n",
            "Epoch 00023: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0839 - accuracy: 0.9738 - val_loss: 3.7465 - val_accuracy: 0.5294\n",
            "Epoch 24/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.0843 - accuracy: 0.9743\n",
            "Epoch 00024: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0845 - accuracy: 0.9742 - val_loss: 3.6638 - val_accuracy: 0.5430\n",
            "Epoch 25/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.0795 - accuracy: 0.9749\n",
            "Epoch 00025: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0797 - accuracy: 0.9748 - val_loss: 3.5961 - val_accuracy: 0.5464\n",
            "Epoch 26/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0613 - accuracy: 0.9815\n",
            "Epoch 00026: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0613 - accuracy: 0.9814 - val_loss: 3.8546 - val_accuracy: 0.5386\n",
            "Epoch 27/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0804 - accuracy: 0.9738\n",
            "Epoch 00027: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0804 - accuracy: 0.9738 - val_loss: 3.6886 - val_accuracy: 0.5333\n",
            "Epoch 28/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.0823 - accuracy: 0.9727\n",
            "Epoch 00028: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0822 - accuracy: 0.9728 - val_loss: 3.7604 - val_accuracy: 0.5403\n",
            "Epoch 29/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9782\n",
            "Epoch 00029: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0715 - accuracy: 0.9782 - val_loss: 3.9532 - val_accuracy: 0.5450\n",
            "Epoch 30/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.0655 - accuracy: 0.9800\n",
            "Epoch 00030: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0653 - accuracy: 0.9800 - val_loss: 3.9642 - val_accuracy: 0.5506\n",
            "Epoch 31/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0547 - accuracy: 0.9831\n",
            "Epoch 00031: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0548 - accuracy: 0.9831 - val_loss: 4.0298 - val_accuracy: 0.5442\n",
            "Epoch 32/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9795\n",
            "Epoch 00032: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0647 - accuracy: 0.9795 - val_loss: 4.0883 - val_accuracy: 0.5369\n",
            "Epoch 33/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9771\n",
            "Epoch 00033: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0736 - accuracy: 0.9770 - val_loss: 4.0648 - val_accuracy: 0.5319\n",
            "Epoch 34/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9781\n",
            "Epoch 00034: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0669 - accuracy: 0.9780 - val_loss: 4.1379 - val_accuracy: 0.5327\n",
            "Epoch 35/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0620 - accuracy: 0.9811\n",
            "Epoch 00035: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0618 - accuracy: 0.9811 - val_loss: 4.0584 - val_accuracy: 0.5397\n",
            "Epoch 36/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0485 - accuracy: 0.9846\n",
            "Epoch 00036: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0490 - accuracy: 0.9845 - val_loss: 4.1356 - val_accuracy: 0.5397\n",
            "Epoch 37/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0362 - accuracy: 0.9891\n",
            "Epoch 00037: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0364 - accuracy: 0.9891 - val_loss: 4.4227 - val_accuracy: 0.5497\n",
            "Epoch 38/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.0681 - accuracy: 0.9769\n",
            "Epoch 00038: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0678 - accuracy: 0.9770 - val_loss: 4.3260 - val_accuracy: 0.5400\n",
            "Epoch 39/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0571 - accuracy: 0.9818\n",
            "Epoch 00039: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0572 - accuracy: 0.9818 - val_loss: 4.3343 - val_accuracy: 0.5425\n",
            "Epoch 40/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.0501 - accuracy: 0.9838\n",
            "Epoch 00040: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0504 - accuracy: 0.9837 - val_loss: 4.4289 - val_accuracy: 0.5478\n",
            "Epoch 41/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0542 - accuracy: 0.9819\n",
            "Epoch 00041: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0542 - accuracy: 0.9819 - val_loss: 4.4300 - val_accuracy: 0.5311\n",
            "Epoch 42/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0492 - accuracy: 0.9850\n",
            "Epoch 00042: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0492 - accuracy: 0.9850 - val_loss: 4.6808 - val_accuracy: 0.5205\n",
            "Epoch 43/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.0552 - accuracy: 0.9820\n",
            "Epoch 00043: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0552 - accuracy: 0.9819 - val_loss: 4.5441 - val_accuracy: 0.5436\n",
            "Epoch 44/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9855\n",
            "Epoch 00044: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0429 - accuracy: 0.9855 - val_loss: 4.3515 - val_accuracy: 0.5447\n",
            "Epoch 45/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9834\n",
            "Epoch 00045: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0497 - accuracy: 0.9834 - val_loss: 4.6089 - val_accuracy: 0.5478\n",
            "Epoch 46/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0535 - accuracy: 0.9826\n",
            "Epoch 00046: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0535 - accuracy: 0.9826 - val_loss: 4.3386 - val_accuracy: 0.5269\n",
            "Epoch 47/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0528 - accuracy: 0.9823\n",
            "Epoch 00047: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0527 - accuracy: 0.9823 - val_loss: 4.4588 - val_accuracy: 0.5486\n",
            "Epoch 48/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9897\n",
            "Epoch 00048: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0327 - accuracy: 0.9898 - val_loss: 4.6593 - val_accuracy: 0.5453\n",
            "Epoch 49/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.9886\n",
            "Epoch 00049: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0364 - accuracy: 0.9886 - val_loss: 4.6560 - val_accuracy: 0.5411\n",
            "Epoch 50/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.0535 - accuracy: 0.9830\n",
            "Epoch 00050: val_loss did not improve from 1.22838\n",
            "225/225 [==============================] - 2s 10ms/step - loss: 0.0544 - accuracy: 0.9826 - val_loss: 4.3839 - val_accuracy: 0.5511\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKuh-1q4j2Vo"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "proto_dropout = Sequential()\n",
        "\n",
        "proto_dropout.add(Conv2D(filters=32,kernel_size=2,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "proto_dropout.add(MaxPooling2D(pool_size=2))\n",
        "proto_dropout.add(Dropout(0.2))\n",
        "proto_dropout.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'))\n",
        "proto_dropout.add(MaxPooling2D(pool_size=2))\n",
        "proto_dropout.add(Dropout(0.2))\n",
        "proto_dropout.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
        "proto_dropout.add(MaxPooling2D(pool_size=2))\n",
        "proto_dropout.add(Dropout(0.2))\n",
        "proto_dropout.add(Flatten())\n",
        "proto_dropout.add(Dense(256,activation='relu'))\n",
        "proto_dropout.add(Dropout(0.2))\n",
        "proto_dropout.add(Dense(512,activation='relu'))\n",
        "proto_dropout.add(Dropout(0.2))\n",
        "proto_dropout.add(Dense(7,activation='softmax'))\n",
        "\n",
        "# proto_dropout.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuaEvVeBdQTN"
      },
      "source": [
        "proto_dropout.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IreE3r3adQdR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7135fd-2dc5-4ebb-97c2-abe318f796e5"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "checkpointer_dropout = ModelCheckpoint(filepath='mycnn_dropout.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_dropout = proto_dropout.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(v_inputs , v_targets),\n",
        "          epochs= epoch, batch_size=128, callbacks=[checkpointer_dropout], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 1.7872 - accuracy: 0.2627\n",
            "Epoch 00001: val_loss improved from inf to 1.69202, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 13ms/step - loss: 1.7863 - accuracy: 0.2634 - val_loss: 1.6920 - val_accuracy: 0.3525\n",
            "Epoch 2/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.6160 - accuracy: 0.3665\n",
            "Epoch 00002: val_loss improved from 1.69202 to 1.51261, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.6160 - accuracy: 0.3665 - val_loss: 1.5126 - val_accuracy: 0.4249\n",
            "Epoch 3/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.5066 - accuracy: 0.4165\n",
            "Epoch 00003: val_loss improved from 1.51261 to 1.42536, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.5066 - accuracy: 0.4165 - val_loss: 1.4254 - val_accuracy: 0.4450\n",
            "Epoch 4/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 1.4220 - accuracy: 0.4513\n",
            "Epoch 00004: val_loss improved from 1.42536 to 1.34880, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 13ms/step - loss: 1.4214 - accuracy: 0.4517 - val_loss: 1.3488 - val_accuracy: 0.4804\n",
            "Epoch 5/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.3648 - accuracy: 0.4791\n",
            "Epoch 00005: val_loss improved from 1.34880 to 1.31459, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.3648 - accuracy: 0.4791 - val_loss: 1.3146 - val_accuracy: 0.4974\n",
            "Epoch 6/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.3066 - accuracy: 0.5008\n",
            "Epoch 00006: val_loss improved from 1.31459 to 1.24975, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.3066 - accuracy: 0.5008 - val_loss: 1.2497 - val_accuracy: 0.5194\n",
            "Epoch 7/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.2669 - accuracy: 0.5148\n",
            "Epoch 00007: val_loss improved from 1.24975 to 1.24491, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.2669 - accuracy: 0.5148 - val_loss: 1.2449 - val_accuracy: 0.5230\n",
            "Epoch 8/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 1.2358 - accuracy: 0.5279\n",
            "Epoch 00008: val_loss improved from 1.24491 to 1.21499, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.2368 - accuracy: 0.5273 - val_loss: 1.2150 - val_accuracy: 0.5355\n",
            "Epoch 9/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.1936 - accuracy: 0.5469\n",
            "Epoch 00009: val_loss improved from 1.21499 to 1.20351, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.1935 - accuracy: 0.5470 - val_loss: 1.2035 - val_accuracy: 0.5400\n",
            "Epoch 10/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.1716 - accuracy: 0.5517\n",
            "Epoch 00010: val_loss improved from 1.20351 to 1.19773, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.1719 - accuracy: 0.5518 - val_loss: 1.1977 - val_accuracy: 0.5339\n",
            "Epoch 11/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.1358 - accuracy: 0.5652\n",
            "Epoch 00011: val_loss improved from 1.19773 to 1.17730, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.1353 - accuracy: 0.5657 - val_loss: 1.1773 - val_accuracy: 0.5467\n",
            "Epoch 12/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.1175 - accuracy: 0.5781\n",
            "Epoch 00012: val_loss improved from 1.17730 to 1.17415, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.1178 - accuracy: 0.5774 - val_loss: 1.1741 - val_accuracy: 0.5584\n",
            "Epoch 13/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.0874 - accuracy: 0.5857\n",
            "Epoch 00013: val_loss improved from 1.17415 to 1.15679, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.0884 - accuracy: 0.5855 - val_loss: 1.1568 - val_accuracy: 0.5653\n",
            "Epoch 14/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.0658 - accuracy: 0.5945\n",
            "Epoch 00014: val_loss improved from 1.15679 to 1.15644, saving model to mycnn_dropout.hdf5\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.0664 - accuracy: 0.5939 - val_loss: 1.1564 - val_accuracy: 0.5584\n",
            "Epoch 15/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 1.0430 - accuracy: 0.6058\n",
            "Epoch 00015: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.0441 - accuracy: 0.6052 - val_loss: 1.1860 - val_accuracy: 0.5503\n",
            "Epoch 16/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.0185 - accuracy: 0.6140\n",
            "Epoch 00016: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 1.0184 - accuracy: 0.6141 - val_loss: 1.1775 - val_accuracy: 0.5581\n",
            "Epoch 17/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9968 - accuracy: 0.6212\n",
            "Epoch 00017: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.9965 - accuracy: 0.6209 - val_loss: 1.1569 - val_accuracy: 0.5659\n",
            "Epoch 18/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.9721 - accuracy: 0.6353\n",
            "Epoch 00018: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.9733 - accuracy: 0.6351 - val_loss: 1.1870 - val_accuracy: 0.5559\n",
            "Epoch 19/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9498 - accuracy: 0.6407\n",
            "Epoch 00019: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.9514 - accuracy: 0.6399 - val_loss: 1.1767 - val_accuracy: 0.5595\n",
            "Epoch 20/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9287 - accuracy: 0.6505\n",
            "Epoch 00020: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.9295 - accuracy: 0.6501 - val_loss: 1.1751 - val_accuracy: 0.5653\n",
            "Epoch 21/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9211 - accuracy: 0.6508\n",
            "Epoch 00021: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.9216 - accuracy: 0.6504 - val_loss: 1.1800 - val_accuracy: 0.5637\n",
            "Epoch 22/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9030 - accuracy: 0.6583\n",
            "Epoch 00022: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.9030 - accuracy: 0.6583 - val_loss: 1.1862 - val_accuracy: 0.5667\n",
            "Epoch 23/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8786 - accuracy: 0.6684\n",
            "Epoch 00023: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.8786 - accuracy: 0.6684 - val_loss: 1.1891 - val_accuracy: 0.5690\n",
            "Epoch 24/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.8673 - accuracy: 0.6713\n",
            "Epoch 00024: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.8668 - accuracy: 0.6717 - val_loss: 1.1768 - val_accuracy: 0.5809\n",
            "Epoch 25/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.8555 - accuracy: 0.6773\n",
            "Epoch 00025: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.8552 - accuracy: 0.6776 - val_loss: 1.1914 - val_accuracy: 0.5729\n",
            "Epoch 26/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.8355 - accuracy: 0.6852\n",
            "Epoch 00026: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.8363 - accuracy: 0.6848 - val_loss: 1.1909 - val_accuracy: 0.5768\n",
            "Epoch 27/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.8211 - accuracy: 0.6929\n",
            "Epoch 00027: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.8207 - accuracy: 0.6929 - val_loss: 1.1940 - val_accuracy: 0.5748\n",
            "Epoch 28/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.8055 - accuracy: 0.6960\n",
            "Epoch 00028: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.8056 - accuracy: 0.6961 - val_loss: 1.1803 - val_accuracy: 0.5770\n",
            "Epoch 29/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.7863 - accuracy: 0.7045\n",
            "Epoch 00029: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.7874 - accuracy: 0.7042 - val_loss: 1.2079 - val_accuracy: 0.5801\n",
            "Epoch 30/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7747 - accuracy: 0.7078\n",
            "Epoch 00030: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.7752 - accuracy: 0.7078 - val_loss: 1.2008 - val_accuracy: 0.5787\n",
            "Epoch 31/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7607 - accuracy: 0.7146\n",
            "Epoch 00031: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.7600 - accuracy: 0.7150 - val_loss: 1.2106 - val_accuracy: 0.5770\n",
            "Epoch 32/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7542 - accuracy: 0.7184\n",
            "Epoch 00032: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.7542 - accuracy: 0.7184 - val_loss: 1.2358 - val_accuracy: 0.5723\n",
            "Epoch 33/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7360 - accuracy: 0.7259\n",
            "Epoch 00033: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.7361 - accuracy: 0.7260 - val_loss: 1.2263 - val_accuracy: 0.5770\n",
            "Epoch 34/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.7228 - accuracy: 0.7320\n",
            "Epoch 00034: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.7230 - accuracy: 0.7322 - val_loss: 1.2684 - val_accuracy: 0.5670\n",
            "Epoch 35/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7058 - accuracy: 0.7343\n",
            "Epoch 00035: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.7058 - accuracy: 0.7343 - val_loss: 1.2534 - val_accuracy: 0.5782\n",
            "Epoch 36/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6926 - accuracy: 0.7392\n",
            "Epoch 00036: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6919 - accuracy: 0.7396 - val_loss: 1.2707 - val_accuracy: 0.5698\n",
            "Epoch 37/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6934 - accuracy: 0.7429\n",
            "Epoch 00037: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6950 - accuracy: 0.7425 - val_loss: 1.2811 - val_accuracy: 0.5684\n",
            "Epoch 38/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.6847 - accuracy: 0.7451\n",
            "Epoch 00038: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6844 - accuracy: 0.7448 - val_loss: 1.2716 - val_accuracy: 0.5704\n",
            "Epoch 39/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6725 - accuracy: 0.7492\n",
            "Epoch 00039: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6730 - accuracy: 0.7489 - val_loss: 1.2833 - val_accuracy: 0.5687\n",
            "Epoch 40/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6612 - accuracy: 0.7554\n",
            "Epoch 00040: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6612 - accuracy: 0.7554 - val_loss: 1.2651 - val_accuracy: 0.5773\n",
            "Epoch 41/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.6473 - accuracy: 0.7609\n",
            "Epoch 00041: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6484 - accuracy: 0.7606 - val_loss: 1.3209 - val_accuracy: 0.5659\n",
            "Epoch 42/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6358 - accuracy: 0.7657\n",
            "Epoch 00042: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6356 - accuracy: 0.7657 - val_loss: 1.2969 - val_accuracy: 0.5754\n",
            "Epoch 43/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6296 - accuracy: 0.7661\n",
            "Epoch 00043: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6302 - accuracy: 0.7660 - val_loss: 1.3185 - val_accuracy: 0.5717\n",
            "Epoch 44/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.6174 - accuracy: 0.7714\n",
            "Epoch 00044: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6170 - accuracy: 0.7712 - val_loss: 1.3235 - val_accuracy: 0.5695\n",
            "Epoch 45/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6158 - accuracy: 0.7727\n",
            "Epoch 00045: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6158 - accuracy: 0.7727 - val_loss: 1.3120 - val_accuracy: 0.5751\n",
            "Epoch 46/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6059 - accuracy: 0.7805\n",
            "Epoch 00046: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6060 - accuracy: 0.7805 - val_loss: 1.3342 - val_accuracy: 0.5690\n",
            "Epoch 47/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6039 - accuracy: 0.7764\n",
            "Epoch 00047: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.6041 - accuracy: 0.7763 - val_loss: 1.3390 - val_accuracy: 0.5673\n",
            "Epoch 48/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.5871 - accuracy: 0.7829\n",
            "Epoch 00048: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.5871 - accuracy: 0.7828 - val_loss: 1.3461 - val_accuracy: 0.5706\n",
            "Epoch 49/50\n",
            "220/225 [============================>.] - ETA: 0s - loss: 0.5941 - accuracy: 0.7814\n",
            "Epoch 00049: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.5941 - accuracy: 0.7813 - val_loss: 1.3435 - val_accuracy: 0.5715\n",
            "Epoch 50/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.5808 - accuracy: 0.7871\n",
            "Epoch 00050: val_loss did not improve from 1.15644\n",
            "225/225 [==============================] - 3s 12ms/step - loss: 0.5812 - accuracy: 0.7870 - val_loss: 1.3566 - val_accuracy: 0.5695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lJzkgU8ihAV"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "proto_batchnorm = Sequential()\n",
        "\n",
        "proto_batchnorm.add(Conv2D(filters=32,kernel_size=2,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "proto_batchnorm.add(BatchNormalization())\n",
        "proto_batchnorm.add(MaxPooling2D(pool_size=2))\n",
        "proto_batchnorm.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'))\n",
        "proto_batchnorm.add(BatchNormalization())\n",
        "proto_batchnorm.add(MaxPooling2D(pool_size=2))\n",
        "proto_batchnorm.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
        "proto_batchnorm.add(BatchNormalization())\n",
        "proto_batchnorm.add(MaxPooling2D(pool_size=2))\n",
        "proto_batchnorm.add(Flatten())\n",
        "proto_batchnorm.add(Dense(256,activation='relu'))\n",
        "proto_batchnorm.add(BatchNormalization())\n",
        "proto_batchnorm.add(Dense(512,activation='relu'))\n",
        "proto_batchnorm.add(BatchNormalization())\n",
        "proto_batchnorm.add(Dense(7,activation='softmax'))\n",
        "\n",
        "# proto_batchnorm.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZNP3NwIihFG"
      },
      "source": [
        "proto_batchnorm.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hu9GdGW0ihI2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84594a20-e977-40fe-a266-b4e44a8f994d"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "checkpointer_batchnorm = ModelCheckpoint(filepath='mycnn_batchnorm.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_batchnorm = proto_batchnorm.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(v_inputs , v_targets),\n",
        "          epochs= epoch, batch_size=128, callbacks=[checkpointer_batchnorm], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.5635 - accuracy: 0.4201\n",
            "Epoch 00001: val_loss improved from inf to 3.56709, saving model to mycnn_batchnorm.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.5623 - accuracy: 0.4207 - val_loss: 3.5671 - val_accuracy: 0.1833\n",
            "Epoch 2/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.2150 - accuracy: 0.5431\n",
            "Epoch 00002: val_loss improved from 3.56709 to 2.04593, saving model to mycnn_batchnorm.hdf5\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 1.2154 - accuracy: 0.5430 - val_loss: 2.0459 - val_accuracy: 0.2329\n",
            "Epoch 3/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.0368 - accuracy: 0.6148\n",
            "Epoch 00003: val_loss improved from 2.04593 to 1.57881, saving model to mycnn_batchnorm.hdf5\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 1.0366 - accuracy: 0.6148 - val_loss: 1.5788 - val_accuracy: 0.4695\n",
            "Epoch 4/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8638 - accuracy: 0.6822\n",
            "Epoch 00004: val_loss improved from 1.57881 to 1.40626, saving model to mycnn_batchnorm.hdf5\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.8645 - accuracy: 0.6818 - val_loss: 1.4063 - val_accuracy: 0.5110\n",
            "Epoch 5/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6581 - accuracy: 0.7635\n",
            "Epoch 00005: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 13ms/step - loss: 0.6601 - accuracy: 0.7627 - val_loss: 1.8558 - val_accuracy: 0.4617\n",
            "Epoch 6/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4895 - accuracy: 0.8284\n",
            "Epoch 00006: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.4915 - accuracy: 0.8278 - val_loss: 1.8536 - val_accuracy: 0.4801\n",
            "Epoch 7/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3264 - accuracy: 0.8898\n",
            "Epoch 00007: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.3264 - accuracy: 0.8898 - val_loss: 1.9272 - val_accuracy: 0.5266\n",
            "Epoch 8/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.2030 - accuracy: 0.9355\n",
            "Epoch 00008: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.2044 - accuracy: 0.9347 - val_loss: 2.3123 - val_accuracy: 0.4946\n",
            "Epoch 9/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1630 - accuracy: 0.9503\n",
            "Epoch 00009: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.1630 - accuracy: 0.9504 - val_loss: 2.0677 - val_accuracy: 0.5536\n",
            "Epoch 10/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0981 - accuracy: 0.9732\n",
            "Epoch 00010: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0986 - accuracy: 0.9730 - val_loss: 2.2792 - val_accuracy: 0.5194\n",
            "Epoch 11/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1448 - accuracy: 0.9554\n",
            "Epoch 00011: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.1454 - accuracy: 0.9551 - val_loss: 2.6042 - val_accuracy: 0.5116\n",
            "Epoch 12/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1363 - accuracy: 0.9546\n",
            "Epoch 00012: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.1372 - accuracy: 0.9543 - val_loss: 2.5411 - val_accuracy: 0.5391\n",
            "Epoch 13/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.1012 - accuracy: 0.9694\n",
            "Epoch 00013: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.1013 - accuracy: 0.9694 - val_loss: 2.4141 - val_accuracy: 0.5372\n",
            "Epoch 14/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0683 - accuracy: 0.9807\n",
            "Epoch 00014: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0690 - accuracy: 0.9805 - val_loss: 2.5478 - val_accuracy: 0.5364\n",
            "Epoch 15/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0545 - accuracy: 0.9851\n",
            "Epoch 00015: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0547 - accuracy: 0.9849 - val_loss: 2.5157 - val_accuracy: 0.5489\n",
            "Epoch 16/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0575 - accuracy: 0.9847\n",
            "Epoch 00016: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0577 - accuracy: 0.9846 - val_loss: 2.5625 - val_accuracy: 0.5500\n",
            "Epoch 17/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0945 - accuracy: 0.9707\n",
            "Epoch 00017: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0952 - accuracy: 0.9706 - val_loss: 2.9058 - val_accuracy: 0.5104\n",
            "Epoch 18/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9573\n",
            "Epoch 00018: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.1286 - accuracy: 0.9572 - val_loss: 3.0549 - val_accuracy: 0.5155\n",
            "Epoch 19/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.0959 - accuracy: 0.9677\n",
            "Epoch 00019: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0962 - accuracy: 0.9676 - val_loss: 2.9165 - val_accuracy: 0.5288\n",
            "Epoch 20/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0852 - accuracy: 0.9721\n",
            "Epoch 00020: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0852 - accuracy: 0.9721 - val_loss: 2.6612 - val_accuracy: 0.5517\n",
            "Epoch 21/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0469 - accuracy: 0.9868\n",
            "Epoch 00021: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0467 - accuracy: 0.9868 - val_loss: 2.7450 - val_accuracy: 0.5433\n",
            "Epoch 22/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0328 - accuracy: 0.9915\n",
            "Epoch 00022: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0328 - accuracy: 0.9915 - val_loss: 2.7961 - val_accuracy: 0.5447\n",
            "Epoch 23/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0309 - accuracy: 0.9921\n",
            "Epoch 00023: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0315 - accuracy: 0.9920 - val_loss: 2.7480 - val_accuracy: 0.5506\n",
            "Epoch 24/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0301 - accuracy: 0.9918\n",
            "Epoch 00024: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0305 - accuracy: 0.9917 - val_loss: 3.0063 - val_accuracy: 0.5550\n",
            "Epoch 25/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9878\n",
            "Epoch 00025: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0387 - accuracy: 0.9877 - val_loss: 3.0905 - val_accuracy: 0.5397\n",
            "Epoch 26/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0779 - accuracy: 0.9734\n",
            "Epoch 00026: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0779 - accuracy: 0.9734 - val_loss: 3.6624 - val_accuracy: 0.4848\n",
            "Epoch 27/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.9524\n",
            "Epoch 00027: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.1388 - accuracy: 0.9525 - val_loss: 3.4110 - val_accuracy: 0.5021\n",
            "Epoch 28/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0974 - accuracy: 0.9672\n",
            "Epoch 00028: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0980 - accuracy: 0.9670 - val_loss: 9.6622 - val_accuracy: 0.3943\n",
            "Epoch 29/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0600 - accuracy: 0.9810\n",
            "Epoch 00029: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0602 - accuracy: 0.9809 - val_loss: 3.4017 - val_accuracy: 0.4567\n",
            "Epoch 30/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0402 - accuracy: 0.9868\n",
            "Epoch 00030: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0399 - accuracy: 0.9869 - val_loss: 2.8323 - val_accuracy: 0.5517\n",
            "Epoch 31/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0246 - accuracy: 0.9927\n",
            "Epoch 00031: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0245 - accuracy: 0.9927 - val_loss: 2.9533 - val_accuracy: 0.5528\n",
            "Epoch 32/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0158 - accuracy: 0.9959\n",
            "Epoch 00032: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0158 - accuracy: 0.9959 - val_loss: 2.9431 - val_accuracy: 0.5609\n",
            "Epoch 33/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0266 - accuracy: 0.9923\n",
            "Epoch 00033: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0269 - accuracy: 0.9922 - val_loss: 3.1361 - val_accuracy: 0.5280\n",
            "Epoch 34/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0375 - accuracy: 0.9874\n",
            "Epoch 00034: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0375 - accuracy: 0.9874 - val_loss: 3.1838 - val_accuracy: 0.5266\n",
            "Epoch 35/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0414 - accuracy: 0.9868\n",
            "Epoch 00035: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0412 - accuracy: 0.9868 - val_loss: 3.3306 - val_accuracy: 0.5372\n",
            "Epoch 36/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.0438 - accuracy: 0.9851\n",
            "Epoch 00036: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0439 - accuracy: 0.9851 - val_loss: 3.2920 - val_accuracy: 0.5397\n",
            "Epoch 37/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0667 - accuracy: 0.9771\n",
            "Epoch 00037: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0670 - accuracy: 0.9772 - val_loss: 3.3824 - val_accuracy: 0.5135\n",
            "Epoch 38/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9743\n",
            "Epoch 00038: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0737 - accuracy: 0.9742 - val_loss: 4.0264 - val_accuracy: 0.4851\n",
            "Epoch 39/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0538 - accuracy: 0.9820\n",
            "Epoch 00039: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0539 - accuracy: 0.9820 - val_loss: 3.7128 - val_accuracy: 0.5442\n",
            "Epoch 40/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0327 - accuracy: 0.9893\n",
            "Epoch 00040: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0328 - accuracy: 0.9893 - val_loss: 3.0766 - val_accuracy: 0.5414\n",
            "Epoch 41/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9928\n",
            "Epoch 00041: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0205 - accuracy: 0.9927 - val_loss: 3.2858 - val_accuracy: 0.5464\n",
            "Epoch 42/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9957\n",
            "Epoch 00042: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0155 - accuracy: 0.9957 - val_loss: 3.0742 - val_accuracy: 0.5500\n",
            "Epoch 43/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9965\n",
            "Epoch 00043: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0115 - accuracy: 0.9964 - val_loss: 3.0975 - val_accuracy: 0.5469\n",
            "Epoch 44/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9959\n",
            "Epoch 00044: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0126 - accuracy: 0.9959 - val_loss: 3.2341 - val_accuracy: 0.5497\n",
            "Epoch 45/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0669 - accuracy: 0.9788\n",
            "Epoch 00045: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0681 - accuracy: 0.9783 - val_loss: 7.9101 - val_accuracy: 0.4283\n",
            "Epoch 46/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1168 - accuracy: 0.9616\n",
            "Epoch 00046: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.1173 - accuracy: 0.9615 - val_loss: 3.9456 - val_accuracy: 0.5249\n",
            "Epoch 47/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0530 - accuracy: 0.9821\n",
            "Epoch 00047: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0529 - accuracy: 0.9821 - val_loss: 2.9543 - val_accuracy: 0.5464\n",
            "Epoch 48/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0301 - accuracy: 0.9900\n",
            "Epoch 00048: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0302 - accuracy: 0.9900 - val_loss: 3.2324 - val_accuracy: 0.5469\n",
            "Epoch 49/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0182 - accuracy: 0.9939\n",
            "Epoch 00049: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0183 - accuracy: 0.9939 - val_loss: 3.6610 - val_accuracy: 0.5573\n",
            "Epoch 50/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0146 - accuracy: 0.9949\n",
            "Epoch 00050: val_loss did not improve from 1.40626\n",
            "225/225 [==============================] - 3s 14ms/step - loss: 0.0146 - accuracy: 0.9949 - val_loss: 3.0738 - val_accuracy: 0.5626\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-8jmhOhj7el"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "\n",
        "proto_both = Sequential()\n",
        "\n",
        "proto_both.add(Conv2D(filters=32,kernel_size=2,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "proto_both.add(BatchNormalization())\n",
        "proto_both.add(MaxPooling2D(pool_size=2))\n",
        "proto_both.add(Dropout(0.2))\n",
        "proto_both.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'))\n",
        "proto_both.add(BatchNormalization())\n",
        "proto_both.add(MaxPooling2D(pool_size=2))\n",
        "proto_both.add(Dropout(0.2))\n",
        "proto_both.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
        "proto_both.add(BatchNormalization())\n",
        "proto_both.add(MaxPooling2D(pool_size=2))\n",
        "proto_both.add(Dropout(0.2))\n",
        "proto_both.add(Flatten())\n",
        "proto_both.add(Dense(256,activation='relu'))\n",
        "proto_both.add(BatchNormalization())\n",
        "proto_both.add(Dropout(0.2))\n",
        "proto_both.add(Dense(512,activation='relu'))\n",
        "proto_both.add(BatchNormalization())\n",
        "proto_both.add(Dropout(0.2))\n",
        "proto_both.add(Dense(7,activation='softmax'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_4BK2kKj7kY"
      },
      "source": [
        "proto_both.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWDpt9BNj7mL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa249ee9-8c70-40f4-ab7d-f8406338dbd4"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "checkpointer_both = ModelCheckpoint(filepath='mycnn_both.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_both = proto_both.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(v_inputs , v_targets),\n",
        "          epochs= epoch, batch_size=128, callbacks=[checkpointer_both], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.9237 - accuracy: 0.3055\n",
            "Epoch 00001: val_loss improved from inf to 2.34057, saving model to mycnn_both.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.9235 - accuracy: 0.3055 - val_loss: 2.3406 - val_accuracy: 0.1691\n",
            "Epoch 2/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.5567 - accuracy: 0.4128\n",
            "Epoch 00002: val_loss improved from 2.34057 to 1.65208, saving model to mycnn_both.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.5551 - accuracy: 0.4132 - val_loss: 1.6521 - val_accuracy: 0.3647\n",
            "Epoch 3/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.4003 - accuracy: 0.4687\n",
            "Epoch 00003: val_loss improved from 1.65208 to 1.40666, saving model to mycnn_both.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.4001 - accuracy: 0.4687 - val_loss: 1.4067 - val_accuracy: 0.4525\n",
            "Epoch 4/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.3037 - accuracy: 0.5010\n",
            "Epoch 00004: val_loss improved from 1.40666 to 1.30786, saving model to mycnn_both.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.3037 - accuracy: 0.5010 - val_loss: 1.3079 - val_accuracy: 0.5026\n",
            "Epoch 5/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.2392 - accuracy: 0.5267\n",
            "Epoch 00005: val_loss improved from 1.30786 to 1.22835, saving model to mycnn_both.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2392 - accuracy: 0.5268 - val_loss: 1.2283 - val_accuracy: 0.5327\n",
            "Epoch 6/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.1815 - accuracy: 0.5537\n",
            "Epoch 00006: val_loss did not improve from 1.22835\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1813 - accuracy: 0.5537 - val_loss: 1.2781 - val_accuracy: 0.5252\n",
            "Epoch 7/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.1277 - accuracy: 0.5686\n",
            "Epoch 00007: val_loss did not improve from 1.22835\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1293 - accuracy: 0.5680 - val_loss: 1.2742 - val_accuracy: 0.5205\n",
            "Epoch 8/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.0856 - accuracy: 0.5904\n",
            "Epoch 00008: val_loss improved from 1.22835 to 1.22631, saving model to mycnn_both.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0857 - accuracy: 0.5907 - val_loss: 1.2263 - val_accuracy: 0.5447\n",
            "Epoch 9/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0367 - accuracy: 0.6081\n",
            "Epoch 00009: val_loss did not improve from 1.22631\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0367 - accuracy: 0.6081 - val_loss: 1.3632 - val_accuracy: 0.5116\n",
            "Epoch 10/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9968 - accuracy: 0.6224\n",
            "Epoch 00010: val_loss improved from 1.22631 to 1.15453, saving model to mycnn_both.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9961 - accuracy: 0.6228 - val_loss: 1.1545 - val_accuracy: 0.5706\n",
            "Epoch 11/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9511 - accuracy: 0.6400\n",
            "Epoch 00011: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9511 - accuracy: 0.6400 - val_loss: 1.2622 - val_accuracy: 0.5428\n",
            "Epoch 12/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9069 - accuracy: 0.6599\n",
            "Epoch 00012: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9072 - accuracy: 0.6598 - val_loss: 1.3351 - val_accuracy: 0.5274\n",
            "Epoch 13/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8584 - accuracy: 0.6806\n",
            "Epoch 00013: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8584 - accuracy: 0.6806 - val_loss: 1.2820 - val_accuracy: 0.5539\n",
            "Epoch 14/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8167 - accuracy: 0.6959\n",
            "Epoch 00014: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8167 - accuracy: 0.6959 - val_loss: 1.2942 - val_accuracy: 0.5592\n",
            "Epoch 15/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7704 - accuracy: 0.7135\n",
            "Epoch 00015: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7699 - accuracy: 0.7137 - val_loss: 1.2450 - val_accuracy: 0.5748\n",
            "Epoch 16/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7296 - accuracy: 0.7260\n",
            "Epoch 00016: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7304 - accuracy: 0.7259 - val_loss: 1.3130 - val_accuracy: 0.5706\n",
            "Epoch 17/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6909 - accuracy: 0.7394\n",
            "Epoch 00017: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6909 - accuracy: 0.7394 - val_loss: 1.2605 - val_accuracy: 0.5887\n",
            "Epoch 18/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6594 - accuracy: 0.7559\n",
            "Epoch 00018: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6594 - accuracy: 0.7559 - val_loss: 1.3588 - val_accuracy: 0.5756\n",
            "Epoch 19/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6342 - accuracy: 0.7658\n",
            "Epoch 00019: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6343 - accuracy: 0.7658 - val_loss: 1.4903 - val_accuracy: 0.5548\n",
            "Epoch 20/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.5885 - accuracy: 0.7813\n",
            "Epoch 00020: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5891 - accuracy: 0.7811 - val_loss: 1.4357 - val_accuracy: 0.5784\n",
            "Epoch 21/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7924\n",
            "Epoch 00021: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5612 - accuracy: 0.7924 - val_loss: 1.4216 - val_accuracy: 0.5695\n",
            "Epoch 22/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5361 - accuracy: 0.8016\n",
            "Epoch 00022: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5361 - accuracy: 0.8016 - val_loss: 1.5120 - val_accuracy: 0.5584\n",
            "Epoch 23/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.8103\n",
            "Epoch 00023: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5125 - accuracy: 0.8103 - val_loss: 1.3746 - val_accuracy: 0.5890\n",
            "Epoch 24/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4786 - accuracy: 0.8262\n",
            "Epoch 00024: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4786 - accuracy: 0.8262 - val_loss: 1.4742 - val_accuracy: 0.5865\n",
            "Epoch 25/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4565 - accuracy: 0.8355\n",
            "Epoch 00025: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4574 - accuracy: 0.8349 - val_loss: 1.5107 - val_accuracy: 0.5991\n",
            "Epoch 26/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4483 - accuracy: 0.8359\n",
            "Epoch 00026: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4489 - accuracy: 0.8356 - val_loss: 1.5592 - val_accuracy: 0.5754\n",
            "Epoch 27/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4183 - accuracy: 0.8483\n",
            "Epoch 00027: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4197 - accuracy: 0.8477 - val_loss: 1.5855 - val_accuracy: 0.5918\n",
            "Epoch 28/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4138 - accuracy: 0.8506\n",
            "Epoch 00028: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4144 - accuracy: 0.8504 - val_loss: 1.4933 - val_accuracy: 0.5787\n",
            "Epoch 29/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3856 - accuracy: 0.8586\n",
            "Epoch 00029: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3857 - accuracy: 0.8585 - val_loss: 1.6025 - val_accuracy: 0.5826\n",
            "Epoch 30/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3725 - accuracy: 0.8644\n",
            "Epoch 00030: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3727 - accuracy: 0.8642 - val_loss: 1.5293 - val_accuracy: 0.5901\n",
            "Epoch 31/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3568 - accuracy: 0.8732\n",
            "Epoch 00031: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3572 - accuracy: 0.8729 - val_loss: 1.5865 - val_accuracy: 0.5834\n",
            "Epoch 32/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3552 - accuracy: 0.8733\n",
            "Epoch 00032: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3558 - accuracy: 0.8731 - val_loss: 1.9602 - val_accuracy: 0.5325\n",
            "Epoch 33/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3386 - accuracy: 0.8750\n",
            "Epoch 00033: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3395 - accuracy: 0.8746 - val_loss: 1.7159 - val_accuracy: 0.5857\n",
            "Epoch 34/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3413 - accuracy: 0.8762\n",
            "Epoch 00034: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3422 - accuracy: 0.8758 - val_loss: 1.6225 - val_accuracy: 0.5868\n",
            "Epoch 35/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3243 - accuracy: 0.8838\n",
            "Epoch 00035: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3243 - accuracy: 0.8838 - val_loss: 1.7810 - val_accuracy: 0.5848\n",
            "Epoch 36/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3183 - accuracy: 0.8863\n",
            "Epoch 00036: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3186 - accuracy: 0.8862 - val_loss: 1.6293 - val_accuracy: 0.5874\n",
            "Epoch 37/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3052 - accuracy: 0.8894\n",
            "Epoch 00037: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3052 - accuracy: 0.8894 - val_loss: 1.8021 - val_accuracy: 0.5623\n",
            "Epoch 38/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.8944\n",
            "Epoch 00038: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2984 - accuracy: 0.8944 - val_loss: 1.6587 - val_accuracy: 0.5887\n",
            "Epoch 39/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2880 - accuracy: 0.8973\n",
            "Epoch 00039: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2889 - accuracy: 0.8972 - val_loss: 1.6778 - val_accuracy: 0.5899\n",
            "Epoch 40/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2748 - accuracy: 0.9026\n",
            "Epoch 00040: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2751 - accuracy: 0.9025 - val_loss: 1.7905 - val_accuracy: 0.5809\n",
            "Epoch 41/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2731 - accuracy: 0.9023\n",
            "Epoch 00041: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2731 - accuracy: 0.9023 - val_loss: 1.7096 - val_accuracy: 0.5943\n",
            "Epoch 42/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2608 - accuracy: 0.9071\n",
            "Epoch 00042: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2612 - accuracy: 0.9069 - val_loss: 1.8686 - val_accuracy: 0.5720\n",
            "Epoch 43/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2657 - accuracy: 0.9055\n",
            "Epoch 00043: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2657 - accuracy: 0.9055 - val_loss: 1.7917 - val_accuracy: 0.5876\n",
            "Epoch 44/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2599 - accuracy: 0.9085\n",
            "Epoch 00044: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2599 - accuracy: 0.9085 - val_loss: 1.7880 - val_accuracy: 0.5887\n",
            "Epoch 45/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2490 - accuracy: 0.9139\n",
            "Epoch 00045: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2490 - accuracy: 0.9139 - val_loss: 1.7768 - val_accuracy: 0.5943\n",
            "Epoch 46/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2522 - accuracy: 0.9106\n",
            "Epoch 00046: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2523 - accuracy: 0.9106 - val_loss: 1.8252 - val_accuracy: 0.5890\n",
            "Epoch 47/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2341 - accuracy: 0.9161\n",
            "Epoch 00047: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2337 - accuracy: 0.9162 - val_loss: 1.8452 - val_accuracy: 0.5837\n",
            "Epoch 48/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2327 - accuracy: 0.9170\n",
            "Epoch 00048: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2327 - accuracy: 0.9170 - val_loss: 1.8428 - val_accuracy: 0.5782\n",
            "Epoch 49/50\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2341 - accuracy: 0.9187\n",
            "Epoch 00049: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2347 - accuracy: 0.9184 - val_loss: 1.7992 - val_accuracy: 0.5882\n",
            "Epoch 50/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2338 - accuracy: 0.9164\n",
            "Epoch 00050: val_loss did not improve from 1.15453\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2338 - accuracy: 0.9164 - val_loss: 2.4508 - val_accuracy: 0.5035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00nooGYZOrtc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94dbbc81-ef42-4048-d7c8-39134d850145"
      },
      "source": [
        "from keras.models import load_model\n",
        "loaded = load_model(\"mycnn_prototype.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])\n",
        "\n",
        "loaded = load_model(\"mycnn_dropout.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])\n",
        "\n",
        "loaded = load_model(\"mycnn_batchnorm.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])\n",
        "\n",
        "loaded = load_model(\"mycnn_both.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113/113 [==============================] - 0s 3ms/step - loss: 1.2049 - accuracy: 0.5380\n",
            "0.5380328893661499\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 1.1286 - accuracy: 0.5620\n",
            "0.5619949698448181\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 1.3694 - accuracy: 0.5138\n",
            "0.5137921571731567\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 1.1412 - accuracy: 0.5812\n",
            "0.5812203884124756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aT2kXGR9Or9p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "c6245ede-b95d-46d0-c9e6-9ffc60dc119b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist.history['val_loss'], label='base model')\n",
        "plt.plot(hist_dropout.history['val_loss'], label='with dropout')\n",
        "plt.plot(hist_batchnorm.history['val_loss'], label='with batch norm')\n",
        "plt.plot(hist_both.history['val_loss'], label='with both')\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"validation losses\")\n",
        "plt.show()\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(hist.history['val_accuracy'], label='base model')\n",
        "plt.plot(hist_dropout.history['val_accuracy'], label='with dropout')\n",
        "plt.plot(hist_batchnorm.history['val_accuracy'], label='with batch norm')\n",
        "plt.plot(hist_both.history['val_accuracy'], label='with both')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"validation accuracy\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'validation accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 156
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgDckS8BOsMz"
      },
      "source": [
        "## Test dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nZEko5rYpTq"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "dropout1 = Sequential()\n",
        "\n",
        "dropout1.add(Conv2D(filters=32,kernel_size=2,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "dropout1.add(BatchNormalization())\n",
        "dropout1.add(MaxPooling2D(pool_size=2))\n",
        "dropout1.add(Dropout(0.1))\n",
        "dropout1.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'))\n",
        "dropout1.add(BatchNormalization())\n",
        "dropout1.add(MaxPooling2D(pool_size=2))\n",
        "dropout1.add(Dropout(0.1))\n",
        "dropout1.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
        "dropout1.add(BatchNormalization())\n",
        "dropout1.add(MaxPooling2D(pool_size=2))\n",
        "dropout1.add(Dropout(0.1))\n",
        "dropout1.add(Flatten())\n",
        "dropout1.add(Dense(256,activation='relu'))\n",
        "dropout1.add(BatchNormalization())\n",
        "dropout1.add(Dropout(0.1))\n",
        "dropout1.add(Dense(512,activation='relu'))\n",
        "dropout1.add(BatchNormalization())\n",
        "dropout1.add(Dropout(0.1))\n",
        "dropout1.add(Dense(7,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-5wAnyfYpXG"
      },
      "source": [
        "dropout1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mIGige9YpaI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b58e278-4ac8-4d71-e652-aa162c53eae5"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 100\n",
        "\n",
        "checkpointer_dropout1 = ModelCheckpoint(filepath='dropout1.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_dropout1 = dropout1.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(v_inputs , v_targets),\n",
        "          epochs= epoch, batch_size=128, callbacks=[checkpointer_dropout1], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "222/225 [============================>.] - ETA: 0s - loss: 1.7739 - accuracy: 0.3484\n",
            "Epoch 00001: val_loss improved from inf to 2.15136, saving model to dropout1.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.7718 - accuracy: 0.3490 - val_loss: 2.1514 - val_accuracy: 0.1725\n",
            "Epoch 2/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.4394 - accuracy: 0.4533\n",
            "Epoch 00002: val_loss improved from 2.15136 to 1.58176, saving model to dropout1.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.4376 - accuracy: 0.4536 - val_loss: 1.5818 - val_accuracy: 0.3801\n",
            "Epoch 3/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.2891 - accuracy: 0.5104\n",
            "Epoch 00003: val_loss improved from 1.58176 to 1.33263, saving model to dropout1.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2897 - accuracy: 0.5105 - val_loss: 1.3326 - val_accuracy: 0.4879\n",
            "Epoch 4/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.1792 - accuracy: 0.5532\n",
            "Epoch 00004: val_loss improved from 1.33263 to 1.30123, saving model to dropout1.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1787 - accuracy: 0.5535 - val_loss: 1.3012 - val_accuracy: 0.5093\n",
            "Epoch 5/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0857 - accuracy: 0.5924\n",
            "Epoch 00005: val_loss did not improve from 1.30123\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0857 - accuracy: 0.5924 - val_loss: 1.3048 - val_accuracy: 0.5166\n",
            "Epoch 6/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9948 - accuracy: 0.6273\n",
            "Epoch 00006: val_loss improved from 1.30123 to 1.26256, saving model to dropout1.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9948 - accuracy: 0.6273 - val_loss: 1.2626 - val_accuracy: 0.5453\n",
            "Epoch 7/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9076 - accuracy: 0.6640\n",
            "Epoch 00007: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9081 - accuracy: 0.6632 - val_loss: 1.2830 - val_accuracy: 0.5444\n",
            "Epoch 8/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8132 - accuracy: 0.7015\n",
            "Epoch 00008: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8132 - accuracy: 0.7015 - val_loss: 1.5101 - val_accuracy: 0.4948\n",
            "Epoch 9/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7250 - accuracy: 0.7378\n",
            "Epoch 00009: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7262 - accuracy: 0.7371 - val_loss: 1.3997 - val_accuracy: 0.5439\n",
            "Epoch 10/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6409 - accuracy: 0.7638\n",
            "Epoch 00010: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6413 - accuracy: 0.7636 - val_loss: 1.3708 - val_accuracy: 0.5609\n",
            "Epoch 11/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.5491 - accuracy: 0.8018\n",
            "Epoch 00011: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5503 - accuracy: 0.8010 - val_loss: 1.4441 - val_accuracy: 0.5570\n",
            "Epoch 12/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4760 - accuracy: 0.8285\n",
            "Epoch 00012: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4758 - accuracy: 0.8287 - val_loss: 1.7229 - val_accuracy: 0.5266\n",
            "Epoch 13/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4184 - accuracy: 0.8491\n",
            "Epoch 00013: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4185 - accuracy: 0.8490 - val_loss: 1.8527 - val_accuracy: 0.5444\n",
            "Epoch 14/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3693 - accuracy: 0.8680\n",
            "Epoch 00014: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3699 - accuracy: 0.8678 - val_loss: 1.6682 - val_accuracy: 0.5737\n",
            "Epoch 15/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3228 - accuracy: 0.8836\n",
            "Epoch 00015: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3228 - accuracy: 0.8836 - val_loss: 1.7248 - val_accuracy: 0.5514\n",
            "Epoch 16/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.8955\n",
            "Epoch 00016: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2961 - accuracy: 0.8950 - val_loss: 1.7892 - val_accuracy: 0.5695\n",
            "Epoch 17/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2728 - accuracy: 0.9042\n",
            "Epoch 00017: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2740 - accuracy: 0.9035 - val_loss: 2.0267 - val_accuracy: 0.5508\n",
            "Epoch 18/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2453 - accuracy: 0.9140\n",
            "Epoch 00018: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2453 - accuracy: 0.9138 - val_loss: 1.8508 - val_accuracy: 0.5628\n",
            "Epoch 19/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2344 - accuracy: 0.9187\n",
            "Epoch 00019: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2344 - accuracy: 0.9187 - val_loss: 1.9129 - val_accuracy: 0.5751\n",
            "Epoch 20/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2065 - accuracy: 0.9288\n",
            "Epoch 00020: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2065 - accuracy: 0.9288 - val_loss: 2.0401 - val_accuracy: 0.5701\n",
            "Epoch 21/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1982 - accuracy: 0.9308\n",
            "Epoch 00021: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1984 - accuracy: 0.9306 - val_loss: 1.9976 - val_accuracy: 0.5637\n",
            "Epoch 22/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1856 - accuracy: 0.9356\n",
            "Epoch 00022: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1856 - accuracy: 0.9357 - val_loss: 2.0265 - val_accuracy: 0.5561\n",
            "Epoch 23/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1728 - accuracy: 0.9409\n",
            "Epoch 00023: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1728 - accuracy: 0.9409 - val_loss: 2.1754 - val_accuracy: 0.5734\n",
            "Epoch 24/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1811 - accuracy: 0.9372\n",
            "Epoch 00024: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1811 - accuracy: 0.9372 - val_loss: 2.2627 - val_accuracy: 0.5690\n",
            "Epoch 25/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1641 - accuracy: 0.9436\n",
            "Epoch 00025: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1641 - accuracy: 0.9436 - val_loss: 2.1631 - val_accuracy: 0.5729\n",
            "Epoch 26/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1667 - accuracy: 0.9413\n",
            "Epoch 00026: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1672 - accuracy: 0.9412 - val_loss: 2.1779 - val_accuracy: 0.5720\n",
            "Epoch 27/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1446 - accuracy: 0.9482\n",
            "Epoch 00027: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1450 - accuracy: 0.9482 - val_loss: 2.3031 - val_accuracy: 0.5656\n",
            "Epoch 28/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1469 - accuracy: 0.9494\n",
            "Epoch 00028: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1470 - accuracy: 0.9496 - val_loss: 2.4033 - val_accuracy: 0.5704\n",
            "Epoch 29/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9524\n",
            "Epoch 00029: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1365 - accuracy: 0.9524 - val_loss: 2.5033 - val_accuracy: 0.5676\n",
            "Epoch 30/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1400 - accuracy: 0.9508\n",
            "Epoch 00030: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1404 - accuracy: 0.9507 - val_loss: 2.2359 - val_accuracy: 0.5653\n",
            "Epoch 31/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1359 - accuracy: 0.9527\n",
            "Epoch 00031: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1356 - accuracy: 0.9528 - val_loss: 2.2569 - val_accuracy: 0.5759\n",
            "Epoch 32/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9572\n",
            "Epoch 00032: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1244 - accuracy: 0.9574 - val_loss: 3.1841 - val_accuracy: 0.4960\n",
            "Epoch 33/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1296 - accuracy: 0.9559\n",
            "Epoch 00033: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1296 - accuracy: 0.9559 - val_loss: 2.2969 - val_accuracy: 0.5698\n",
            "Epoch 34/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1201 - accuracy: 0.9590\n",
            "Epoch 00034: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1207 - accuracy: 0.9585 - val_loss: 2.4326 - val_accuracy: 0.5784\n",
            "Epoch 35/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.9597\n",
            "Epoch 00035: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1208 - accuracy: 0.9597 - val_loss: 2.3162 - val_accuracy: 0.5678\n",
            "Epoch 36/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1228 - accuracy: 0.9561\n",
            "Epoch 00036: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1230 - accuracy: 0.9561 - val_loss: 2.4043 - val_accuracy: 0.5745\n",
            "Epoch 37/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1087 - accuracy: 0.9631\n",
            "Epoch 00037: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1087 - accuracy: 0.9631 - val_loss: 2.4218 - val_accuracy: 0.5720\n",
            "Epoch 38/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1088 - accuracy: 0.9624\n",
            "Epoch 00038: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1088 - accuracy: 0.9625 - val_loss: 2.2514 - val_accuracy: 0.5614\n",
            "Epoch 39/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1053 - accuracy: 0.9648\n",
            "Epoch 00039: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1053 - accuracy: 0.9648 - val_loss: 2.3354 - val_accuracy: 0.5634\n",
            "Epoch 40/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1032 - accuracy: 0.9649\n",
            "Epoch 00040: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1035 - accuracy: 0.9649 - val_loss: 2.4617 - val_accuracy: 0.5600\n",
            "Epoch 41/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9640\n",
            "Epoch 00041: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1029 - accuracy: 0.9640 - val_loss: 2.3662 - val_accuracy: 0.5684\n",
            "Epoch 42/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9636\n",
            "Epoch 00042: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1046 - accuracy: 0.9635 - val_loss: 2.4333 - val_accuracy: 0.5578\n",
            "Epoch 43/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9661\n",
            "Epoch 00043: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1005 - accuracy: 0.9661 - val_loss: 2.5819 - val_accuracy: 0.5756\n",
            "Epoch 44/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9670\n",
            "Epoch 00044: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0936 - accuracy: 0.9670 - val_loss: 2.5202 - val_accuracy: 0.5798\n",
            "Epoch 45/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1083 - accuracy: 0.9635\n",
            "Epoch 00045: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1083 - accuracy: 0.9635 - val_loss: 2.5517 - val_accuracy: 0.5678\n",
            "Epoch 46/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0912 - accuracy: 0.9685\n",
            "Epoch 00046: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0912 - accuracy: 0.9685 - val_loss: 2.4710 - val_accuracy: 0.5653\n",
            "Epoch 47/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0883 - accuracy: 0.9699\n",
            "Epoch 00047: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0883 - accuracy: 0.9699 - val_loss: 2.5803 - val_accuracy: 0.5731\n",
            "Epoch 48/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9676\n",
            "Epoch 00048: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0946 - accuracy: 0.9676 - val_loss: 2.4302 - val_accuracy: 0.5612\n",
            "Epoch 49/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0898 - accuracy: 0.9697\n",
            "Epoch 00049: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0898 - accuracy: 0.9697 - val_loss: 2.5357 - val_accuracy: 0.5642\n",
            "Epoch 50/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9711\n",
            "Epoch 00050: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0857 - accuracy: 0.9711 - val_loss: 2.5011 - val_accuracy: 0.5573\n",
            "Epoch 51/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9699\n",
            "Epoch 00051: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0875 - accuracy: 0.9699 - val_loss: 2.5333 - val_accuracy: 0.5614\n",
            "Epoch 52/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9727\n",
            "Epoch 00052: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0819 - accuracy: 0.9727 - val_loss: 2.4400 - val_accuracy: 0.5720\n",
            "Epoch 53/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0816 - accuracy: 0.9731\n",
            "Epoch 00053: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0815 - accuracy: 0.9731 - val_loss: 2.6272 - val_accuracy: 0.5626\n",
            "Epoch 54/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9716\n",
            "Epoch 00054: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0835 - accuracy: 0.9716 - val_loss: 2.8722 - val_accuracy: 0.5422\n",
            "Epoch 55/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0800 - accuracy: 0.9720\n",
            "Epoch 00055: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0807 - accuracy: 0.9718 - val_loss: 2.4661 - val_accuracy: 0.5720\n",
            "Epoch 56/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.9721\n",
            "Epoch 00056: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0817 - accuracy: 0.9721 - val_loss: 2.5429 - val_accuracy: 0.5740\n",
            "Epoch 57/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0747 - accuracy: 0.9739\n",
            "Epoch 00057: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0747 - accuracy: 0.9739 - val_loss: 2.8045 - val_accuracy: 0.5648\n",
            "Epoch 58/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.0730 - accuracy: 0.9757\n",
            "Epoch 00058: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0736 - accuracy: 0.9755 - val_loss: 2.7163 - val_accuracy: 0.5645\n",
            "Epoch 59/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9723\n",
            "Epoch 00059: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0812 - accuracy: 0.9723 - val_loss: 2.6578 - val_accuracy: 0.5704\n",
            "Epoch 60/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0751 - accuracy: 0.9745\n",
            "Epoch 00060: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0758 - accuracy: 0.9743 - val_loss: 2.6341 - val_accuracy: 0.5598\n",
            "Epoch 61/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0818 - accuracy: 0.9717\n",
            "Epoch 00061: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0818 - accuracy: 0.9717 - val_loss: 2.5571 - val_accuracy: 0.5851\n",
            "Epoch 62/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0742 - accuracy: 0.9736\n",
            "Epoch 00062: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0743 - accuracy: 0.9735 - val_loss: 2.4520 - val_accuracy: 0.5667\n",
            "Epoch 63/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9751\n",
            "Epoch 00063: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0715 - accuracy: 0.9751 - val_loss: 2.7827 - val_accuracy: 0.5684\n",
            "Epoch 64/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0707 - accuracy: 0.9764\n",
            "Epoch 00064: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0710 - accuracy: 0.9762 - val_loss: 2.6721 - val_accuracy: 0.5759\n",
            "Epoch 65/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0766 - accuracy: 0.9743\n",
            "Epoch 00065: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0769 - accuracy: 0.9743 - val_loss: 2.6094 - val_accuracy: 0.5740\n",
            "Epoch 66/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9750\n",
            "Epoch 00066: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0714 - accuracy: 0.9750 - val_loss: 2.5574 - val_accuracy: 0.5706\n",
            "Epoch 67/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0657 - accuracy: 0.9767\n",
            "Epoch 00067: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0657 - accuracy: 0.9767 - val_loss: 2.6827 - val_accuracy: 0.5737\n",
            "Epoch 68/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9771\n",
            "Epoch 00068: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0676 - accuracy: 0.9771 - val_loss: 2.7164 - val_accuracy: 0.5656\n",
            "Epoch 69/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0712 - accuracy: 0.9752\n",
            "Epoch 00069: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0713 - accuracy: 0.9753 - val_loss: 2.8007 - val_accuracy: 0.5592\n",
            "Epoch 70/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0633 - accuracy: 0.9783\n",
            "Epoch 00070: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0632 - accuracy: 0.9783 - val_loss: 2.6692 - val_accuracy: 0.5756\n",
            "Epoch 71/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0633 - accuracy: 0.9780\n",
            "Epoch 00071: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0636 - accuracy: 0.9778 - val_loss: 2.7106 - val_accuracy: 0.5807\n",
            "Epoch 72/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.0643 - accuracy: 0.9782\n",
            "Epoch 00072: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0650 - accuracy: 0.9780 - val_loss: 2.7892 - val_accuracy: 0.5795\n",
            "Epoch 73/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0688 - accuracy: 0.9764\n",
            "Epoch 00073: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0689 - accuracy: 0.9764 - val_loss: 2.6698 - val_accuracy: 0.5815\n",
            "Epoch 74/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0603 - accuracy: 0.9794\n",
            "Epoch 00074: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0608 - accuracy: 0.9791 - val_loss: 2.5286 - val_accuracy: 0.5600\n",
            "Epoch 75/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0625 - accuracy: 0.9782\n",
            "Epoch 00075: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0625 - accuracy: 0.9782 - val_loss: 3.0109 - val_accuracy: 0.5634\n",
            "Epoch 76/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0674 - accuracy: 0.9768\n",
            "Epoch 00076: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0674 - accuracy: 0.9768 - val_loss: 2.7814 - val_accuracy: 0.5717\n",
            "Epoch 77/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9769\n",
            "Epoch 00077: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0648 - accuracy: 0.9769 - val_loss: 2.7369 - val_accuracy: 0.5709\n",
            "Epoch 78/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0606 - accuracy: 0.9792\n",
            "Epoch 00078: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0607 - accuracy: 0.9792 - val_loss: 2.6997 - val_accuracy: 0.5648\n",
            "Epoch 79/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0576 - accuracy: 0.9802\n",
            "Epoch 00079: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0577 - accuracy: 0.9801 - val_loss: 2.7060 - val_accuracy: 0.5567\n",
            "Epoch 80/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.9815\n",
            "Epoch 00080: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0540 - accuracy: 0.9815 - val_loss: 2.7996 - val_accuracy: 0.5779\n",
            "Epoch 81/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0593 - accuracy: 0.9799\n",
            "Epoch 00081: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0597 - accuracy: 0.9799 - val_loss: 2.8114 - val_accuracy: 0.5720\n",
            "Epoch 82/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9800\n",
            "Epoch 00082: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0587 - accuracy: 0.9800 - val_loss: 2.7815 - val_accuracy: 0.5743\n",
            "Epoch 83/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0549 - accuracy: 0.9806\n",
            "Epoch 00083: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0548 - accuracy: 0.9807 - val_loss: 2.8001 - val_accuracy: 0.5651\n",
            "Epoch 84/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.0557 - accuracy: 0.9807\n",
            "Epoch 00084: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0557 - accuracy: 0.9807 - val_loss: 2.6745 - val_accuracy: 0.5609\n",
            "Epoch 85/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0529 - accuracy: 0.9810\n",
            "Epoch 00085: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0530 - accuracy: 0.9809 - val_loss: 2.7518 - val_accuracy: 0.5745\n",
            "Epoch 86/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0515 - accuracy: 0.9814\n",
            "Epoch 00086: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0516 - accuracy: 0.9814 - val_loss: 2.7623 - val_accuracy: 0.5770\n",
            "Epoch 87/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0529 - accuracy: 0.9818\n",
            "Epoch 00087: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0529 - accuracy: 0.9818 - val_loss: 2.8758 - val_accuracy: 0.5695\n",
            "Epoch 88/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0529 - accuracy: 0.9814\n",
            "Epoch 00088: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0530 - accuracy: 0.9814 - val_loss: 2.7545 - val_accuracy: 0.5653\n",
            "Epoch 89/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0610 - accuracy: 0.9790\n",
            "Epoch 00089: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0609 - accuracy: 0.9790 - val_loss: 2.7114 - val_accuracy: 0.5790\n",
            "Epoch 90/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0593 - accuracy: 0.9793\n",
            "Epoch 00090: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0594 - accuracy: 0.9793 - val_loss: 2.9229 - val_accuracy: 0.5754\n",
            "Epoch 91/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0601 - accuracy: 0.9803\n",
            "Epoch 00091: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0602 - accuracy: 0.9803 - val_loss: 2.6407 - val_accuracy: 0.5595\n",
            "Epoch 92/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9811\n",
            "Epoch 00092: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0535 - accuracy: 0.9811 - val_loss: 2.8017 - val_accuracy: 0.5776\n",
            "Epoch 93/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0471 - accuracy: 0.9835\n",
            "Epoch 00093: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0474 - accuracy: 0.9834 - val_loss: 2.7451 - val_accuracy: 0.5731\n",
            "Epoch 94/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.0517 - accuracy: 0.9818\n",
            "Epoch 00094: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0516 - accuracy: 0.9818 - val_loss: 2.9589 - val_accuracy: 0.5701\n",
            "Epoch 95/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0507 - accuracy: 0.9819\n",
            "Epoch 00095: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0510 - accuracy: 0.9817 - val_loss: 2.6092 - val_accuracy: 0.5623\n",
            "Epoch 96/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0563 - accuracy: 0.9807\n",
            "Epoch 00096: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0563 - accuracy: 0.9807 - val_loss: 2.6732 - val_accuracy: 0.5745\n",
            "Epoch 97/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.0519 - accuracy: 0.9815\n",
            "Epoch 00097: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0518 - accuracy: 0.9816 - val_loss: 3.0868 - val_accuracy: 0.5542\n",
            "Epoch 98/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0544 - accuracy: 0.9809\n",
            "Epoch 00098: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0544 - accuracy: 0.9808 - val_loss: 2.7302 - val_accuracy: 0.5759\n",
            "Epoch 99/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9835\n",
            "Epoch 00099: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0480 - accuracy: 0.9835 - val_loss: 2.9568 - val_accuracy: 0.5600\n",
            "Epoch 100/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.0482 - accuracy: 0.9836\n",
            "Epoch 00100: val_loss did not improve from 1.26256\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.0482 - accuracy: 0.9837 - val_loss: 2.8050 - val_accuracy: 0.5743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OkjDzxQeOSS"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "dropout2 = Sequential()\n",
        "\n",
        "dropout2.add(Conv2D(filters=32,kernel_size=2,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "dropout2.add(BatchNormalization())\n",
        "dropout2.add(MaxPooling2D(pool_size=2))\n",
        "dropout2.add(Dropout(0.2))\n",
        "dropout2.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'))\n",
        "dropout2.add(BatchNormalization())\n",
        "dropout2.add(MaxPooling2D(pool_size=2))\n",
        "dropout2.add(Dropout(0.2))\n",
        "dropout2.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
        "dropout2.add(BatchNormalization())\n",
        "dropout2.add(MaxPooling2D(pool_size=2))\n",
        "dropout2.add(Dropout(0.2))\n",
        "dropout2.add(Flatten())\n",
        "dropout2.add(Dense(256,activation='relu'))\n",
        "dropout2.add(BatchNormalization())\n",
        "dropout2.add(Dropout(0.2))\n",
        "dropout2.add(Dense(512,activation='relu'))\n",
        "dropout2.add(BatchNormalization())\n",
        "dropout2.add(Dropout(0.2))\n",
        "dropout2.add(Dense(7,activation='softmax'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koyL2YaZeOUi"
      },
      "source": [
        "dropout2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyH17mzzeOW9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f1a6c2c-dfdb-41da-9cf7-46ae50d00a5e"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 100\n",
        "\n",
        "checkpointer_dropout2 = ModelCheckpoint(filepath='dropout2.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_dropout2 = dropout2.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(v_inputs , v_targets),\n",
        "          epochs= epoch, batch_size=128, callbacks=[checkpointer_dropout2], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "222/225 [============================>.] - ETA: 0s - loss: 1.8995 - accuracy: 0.3119\n",
            "Epoch 00001: val_loss improved from inf to 3.05499, saving model to dropout2.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.8966 - accuracy: 0.3126 - val_loss: 3.0550 - val_accuracy: 0.2494\n",
            "Epoch 2/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.5336 - accuracy: 0.4180\n",
            "Epoch 00002: val_loss improved from 3.05499 to 2.06833, saving model to dropout2.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.5336 - accuracy: 0.4179 - val_loss: 2.0683 - val_accuracy: 0.2694\n",
            "Epoch 3/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.3929 - accuracy: 0.4681\n",
            "Epoch 00003: val_loss improved from 2.06833 to 1.47347, saving model to dropout2.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.3935 - accuracy: 0.4680 - val_loss: 1.4735 - val_accuracy: 0.4536\n",
            "Epoch 4/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.3023 - accuracy: 0.4994\n",
            "Epoch 00004: val_loss improved from 1.47347 to 1.32441, saving model to dropout2.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.3020 - accuracy: 0.4994 - val_loss: 1.3244 - val_accuracy: 0.5065\n",
            "Epoch 5/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.2326 - accuracy: 0.5308\n",
            "Epoch 00005: val_loss did not improve from 1.32441\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2325 - accuracy: 0.5306 - val_loss: 1.4486 - val_accuracy: 0.4466\n",
            "Epoch 6/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.1775 - accuracy: 0.5498\n",
            "Epoch 00006: val_loss improved from 1.32441 to 1.21587, saving model to dropout2.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1777 - accuracy: 0.5498 - val_loss: 1.2159 - val_accuracy: 0.5469\n",
            "Epoch 7/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.1263 - accuracy: 0.5690\n",
            "Epoch 00007: val_loss did not improve from 1.21587\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1263 - accuracy: 0.5690 - val_loss: 1.3094 - val_accuracy: 0.5124\n",
            "Epoch 8/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0875 - accuracy: 0.5880\n",
            "Epoch 00008: val_loss improved from 1.21587 to 1.21006, saving model to dropout2.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0875 - accuracy: 0.5880 - val_loss: 1.2101 - val_accuracy: 0.5542\n",
            "Epoch 9/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.0423 - accuracy: 0.6064\n",
            "Epoch 00009: val_loss did not improve from 1.21006\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0431 - accuracy: 0.6061 - val_loss: 1.3097 - val_accuracy: 0.5294\n",
            "Epoch 10/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9987 - accuracy: 0.6228\n",
            "Epoch 00010: val_loss improved from 1.21006 to 1.20271, saving model to dropout2.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9987 - accuracy: 0.6228 - val_loss: 1.2027 - val_accuracy: 0.5690\n",
            "Epoch 11/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9625 - accuracy: 0.6389\n",
            "Epoch 00011: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9625 - accuracy: 0.6389 - val_loss: 1.2304 - val_accuracy: 0.5665\n",
            "Epoch 12/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9093 - accuracy: 0.6577\n",
            "Epoch 00012: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9093 - accuracy: 0.6577 - val_loss: 1.2209 - val_accuracy: 0.5709\n",
            "Epoch 13/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8646 - accuracy: 0.6727\n",
            "Epoch 00013: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8646 - accuracy: 0.6727 - val_loss: 1.8207 - val_accuracy: 0.5188\n",
            "Epoch 14/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.8339 - accuracy: 0.6888\n",
            "Epoch 00014: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8358 - accuracy: 0.6882 - val_loss: 1.2239 - val_accuracy: 0.5717\n",
            "Epoch 15/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7840 - accuracy: 0.7065\n",
            "Epoch 00015: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7840 - accuracy: 0.7065 - val_loss: 1.3256 - val_accuracy: 0.5520\n",
            "Epoch 16/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7475 - accuracy: 0.7208\n",
            "Epoch 00016: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7492 - accuracy: 0.7198 - val_loss: 1.2793 - val_accuracy: 0.5921\n",
            "Epoch 17/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7084 - accuracy: 0.7380\n",
            "Epoch 00017: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7091 - accuracy: 0.7376 - val_loss: 1.2718 - val_accuracy: 0.5868\n",
            "Epoch 18/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6624 - accuracy: 0.7550\n",
            "Epoch 00018: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6622 - accuracy: 0.7553 - val_loss: 1.2541 - val_accuracy: 0.5887\n",
            "Epoch 19/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6326 - accuracy: 0.7656\n",
            "Epoch 00019: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6333 - accuracy: 0.7651 - val_loss: 1.2757 - val_accuracy: 0.5901\n",
            "Epoch 20/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6045 - accuracy: 0.7777\n",
            "Epoch 00020: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6045 - accuracy: 0.7777 - val_loss: 1.3268 - val_accuracy: 0.5885\n",
            "Epoch 21/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5754 - accuracy: 0.7865\n",
            "Epoch 00021: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5754 - accuracy: 0.7865 - val_loss: 1.4572 - val_accuracy: 0.5637\n",
            "Epoch 22/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5485 - accuracy: 0.8000\n",
            "Epoch 00022: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5489 - accuracy: 0.7999 - val_loss: 1.3816 - val_accuracy: 0.5882\n",
            "Epoch 23/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.5160 - accuracy: 0.8115\n",
            "Epoch 00023: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5163 - accuracy: 0.8112 - val_loss: 1.3786 - val_accuracy: 0.6004\n",
            "Epoch 24/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4959 - accuracy: 0.8170\n",
            "Epoch 00024: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4976 - accuracy: 0.8165 - val_loss: 1.5483 - val_accuracy: 0.5701\n",
            "Epoch 25/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4781 - accuracy: 0.8249\n",
            "Epoch 00025: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4781 - accuracy: 0.8249 - val_loss: 1.6295 - val_accuracy: 0.5729\n",
            "Epoch 26/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4620 - accuracy: 0.8315\n",
            "Epoch 00026: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4623 - accuracy: 0.8312 - val_loss: 1.4351 - val_accuracy: 0.5952\n",
            "Epoch 27/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4337 - accuracy: 0.8409\n",
            "Epoch 00027: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4340 - accuracy: 0.8408 - val_loss: 1.5632 - val_accuracy: 0.5829\n",
            "Epoch 28/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4158 - accuracy: 0.8503\n",
            "Epoch 00028: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4158 - accuracy: 0.8503 - val_loss: 1.4801 - val_accuracy: 0.5982\n",
            "Epoch 29/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4092 - accuracy: 0.8526\n",
            "Epoch 00029: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4102 - accuracy: 0.8523 - val_loss: 1.4730 - val_accuracy: 0.5991\n",
            "Epoch 30/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.8615\n",
            "Epoch 00030: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3819 - accuracy: 0.8609 - val_loss: 1.5744 - val_accuracy: 0.6046\n",
            "Epoch 31/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3714 - accuracy: 0.8647\n",
            "Epoch 00031: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3715 - accuracy: 0.8647 - val_loss: 1.5084 - val_accuracy: 0.6024\n",
            "Epoch 32/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3525 - accuracy: 0.8716\n",
            "Epoch 00032: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3548 - accuracy: 0.8709 - val_loss: 1.6373 - val_accuracy: 0.5893\n",
            "Epoch 33/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3506 - accuracy: 0.8718\n",
            "Epoch 00033: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3509 - accuracy: 0.8716 - val_loss: 1.6033 - val_accuracy: 0.6024\n",
            "Epoch 34/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3469 - accuracy: 0.8757\n",
            "Epoch 00034: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3469 - accuracy: 0.8757 - val_loss: 1.5276 - val_accuracy: 0.6035\n",
            "Epoch 35/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3371 - accuracy: 0.8790\n",
            "Epoch 00035: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3373 - accuracy: 0.8791 - val_loss: 1.6346 - val_accuracy: 0.5857\n",
            "Epoch 36/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3336 - accuracy: 0.8812\n",
            "Epoch 00036: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3337 - accuracy: 0.8811 - val_loss: 1.6524 - val_accuracy: 0.5899\n",
            "Epoch 37/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3153 - accuracy: 0.8859\n",
            "Epoch 00037: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3157 - accuracy: 0.8858 - val_loss: 1.6237 - val_accuracy: 0.5991\n",
            "Epoch 38/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3141 - accuracy: 0.8873\n",
            "Epoch 00038: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3158 - accuracy: 0.8870 - val_loss: 1.6510 - val_accuracy: 0.5940\n",
            "Epoch 39/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2941 - accuracy: 0.8957\n",
            "Epoch 00039: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2952 - accuracy: 0.8955 - val_loss: 1.6282 - val_accuracy: 0.5985\n",
            "Epoch 40/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2972 - accuracy: 0.8922\n",
            "Epoch 00040: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2972 - accuracy: 0.8921 - val_loss: 1.9898 - val_accuracy: 0.5684\n",
            "Epoch 41/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3053 - accuracy: 0.8901\n",
            "Epoch 00041: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3049 - accuracy: 0.8902 - val_loss: 1.6824 - val_accuracy: 0.6060\n",
            "Epoch 42/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 0.8982\n",
            "Epoch 00042: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2825 - accuracy: 0.8982 - val_loss: 1.8163 - val_accuracy: 0.5862\n",
            "Epoch 43/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2766 - accuracy: 0.9016\n",
            "Epoch 00043: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2766 - accuracy: 0.9016 - val_loss: 1.6690 - val_accuracy: 0.6063\n",
            "Epoch 44/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2713 - accuracy: 0.9033\n",
            "Epoch 00044: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2714 - accuracy: 0.9031 - val_loss: 1.7394 - val_accuracy: 0.5991\n",
            "Epoch 45/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2559 - accuracy: 0.9104\n",
            "Epoch 00045: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2559 - accuracy: 0.9104 - val_loss: 1.6810 - val_accuracy: 0.6046\n",
            "Epoch 46/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2518 - accuracy: 0.9099\n",
            "Epoch 00046: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2524 - accuracy: 0.9099 - val_loss: 1.7364 - val_accuracy: 0.5874\n",
            "Epoch 47/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2597 - accuracy: 0.9070\n",
            "Epoch 00047: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2597 - accuracy: 0.9070 - val_loss: 1.7948 - val_accuracy: 0.5899\n",
            "Epoch 48/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2424 - accuracy: 0.9120\n",
            "Epoch 00048: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2438 - accuracy: 0.9115 - val_loss: 1.7447 - val_accuracy: 0.6013\n",
            "Epoch 49/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2497 - accuracy: 0.9129\n",
            "Epoch 00049: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2506 - accuracy: 0.9127 - val_loss: 1.7765 - val_accuracy: 0.6024\n",
            "Epoch 50/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2373 - accuracy: 0.9176\n",
            "Epoch 00050: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2372 - accuracy: 0.9176 - val_loss: 1.8405 - val_accuracy: 0.5943\n",
            "Epoch 51/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2317 - accuracy: 0.9161\n",
            "Epoch 00051: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2317 - accuracy: 0.9161 - val_loss: 1.8152 - val_accuracy: 0.6024\n",
            "Epoch 52/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2296 - accuracy: 0.9176\n",
            "Epoch 00052: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2296 - accuracy: 0.9176 - val_loss: 1.8550 - val_accuracy: 0.5890\n",
            "Epoch 53/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2251 - accuracy: 0.9234\n",
            "Epoch 00053: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2251 - accuracy: 0.9234 - val_loss: 1.8197 - val_accuracy: 0.6046\n",
            "Epoch 54/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2244 - accuracy: 0.9216\n",
            "Epoch 00054: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2244 - accuracy: 0.9216 - val_loss: 1.7604 - val_accuracy: 0.6002\n",
            "Epoch 55/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2122 - accuracy: 0.9244\n",
            "Epoch 00055: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2122 - accuracy: 0.9244 - val_loss: 1.9819 - val_accuracy: 0.5899\n",
            "Epoch 56/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2179 - accuracy: 0.9240\n",
            "Epoch 00056: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2179 - accuracy: 0.9240 - val_loss: 1.9318 - val_accuracy: 0.5924\n",
            "Epoch 57/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2154 - accuracy: 0.9226\n",
            "Epoch 00057: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2154 - accuracy: 0.9226 - val_loss: 1.8256 - val_accuracy: 0.5882\n",
            "Epoch 58/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2173 - accuracy: 0.9236\n",
            "Epoch 00058: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2178 - accuracy: 0.9235 - val_loss: 1.7479 - val_accuracy: 0.5940\n",
            "Epoch 59/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2117 - accuracy: 0.9252\n",
            "Epoch 00059: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2129 - accuracy: 0.9249 - val_loss: 1.8513 - val_accuracy: 0.5748\n",
            "Epoch 60/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.2047 - accuracy: 0.9285\n",
            "Epoch 00060: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2047 - accuracy: 0.9285 - val_loss: 1.9188 - val_accuracy: 0.5926\n",
            "Epoch 61/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.2055 - accuracy: 0.9286\n",
            "Epoch 00061: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.2060 - accuracy: 0.9285 - val_loss: 1.8808 - val_accuracy: 0.6063\n",
            "Epoch 62/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1972 - accuracy: 0.9313\n",
            "Epoch 00062: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1972 - accuracy: 0.9313 - val_loss: 1.8739 - val_accuracy: 0.5996\n",
            "Epoch 63/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1941 - accuracy: 0.9320\n",
            "Epoch 00063: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1947 - accuracy: 0.9320 - val_loss: 1.9634 - val_accuracy: 0.6007\n",
            "Epoch 64/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1903 - accuracy: 0.9335\n",
            "Epoch 00064: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1902 - accuracy: 0.9334 - val_loss: 1.8333 - val_accuracy: 0.5985\n",
            "Epoch 65/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1953 - accuracy: 0.9323\n",
            "Epoch 00065: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1955 - accuracy: 0.9320 - val_loss: 2.2395 - val_accuracy: 0.5662\n",
            "Epoch 66/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1914 - accuracy: 0.9318\n",
            "Epoch 00066: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1922 - accuracy: 0.9316 - val_loss: 1.9546 - val_accuracy: 0.5965\n",
            "Epoch 67/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1842 - accuracy: 0.9357\n",
            "Epoch 00067: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1842 - accuracy: 0.9357 - val_loss: 1.9903 - val_accuracy: 0.5996\n",
            "Epoch 68/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1885 - accuracy: 0.9348\n",
            "Epoch 00068: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1884 - accuracy: 0.9348 - val_loss: 1.8050 - val_accuracy: 0.5963\n",
            "Epoch 69/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1750 - accuracy: 0.9382\n",
            "Epoch 00069: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1749 - accuracy: 0.9382 - val_loss: 2.1433 - val_accuracy: 0.5954\n",
            "Epoch 70/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1758 - accuracy: 0.9395\n",
            "Epoch 00070: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1761 - accuracy: 0.9394 - val_loss: 2.0809 - val_accuracy: 0.5782\n",
            "Epoch 71/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1804 - accuracy: 0.9376\n",
            "Epoch 00071: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1805 - accuracy: 0.9376 - val_loss: 1.8974 - val_accuracy: 0.6007\n",
            "Epoch 72/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1787 - accuracy: 0.9381\n",
            "Epoch 00072: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1787 - accuracy: 0.9381 - val_loss: 1.8915 - val_accuracy: 0.6052\n",
            "Epoch 73/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1715 - accuracy: 0.9381\n",
            "Epoch 00073: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1715 - accuracy: 0.9381 - val_loss: 2.1107 - val_accuracy: 0.5837\n",
            "Epoch 74/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1687 - accuracy: 0.9402\n",
            "Epoch 00074: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1688 - accuracy: 0.9402 - val_loss: 1.9512 - val_accuracy: 0.5974\n",
            "Epoch 75/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1838 - accuracy: 0.9367\n",
            "Epoch 00075: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1838 - accuracy: 0.9367 - val_loss: 2.3075 - val_accuracy: 0.5748\n",
            "Epoch 76/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1765 - accuracy: 0.9381\n",
            "Epoch 00076: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1766 - accuracy: 0.9381 - val_loss: 2.0653 - val_accuracy: 0.5743\n",
            "Epoch 77/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1690 - accuracy: 0.9421\n",
            "Epoch 00077: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1697 - accuracy: 0.9418 - val_loss: 2.0092 - val_accuracy: 0.5899\n",
            "Epoch 78/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1694 - accuracy: 0.9421\n",
            "Epoch 00078: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1694 - accuracy: 0.9421 - val_loss: 2.0667 - val_accuracy: 0.5851\n",
            "Epoch 79/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1612 - accuracy: 0.9450\n",
            "Epoch 00079: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1619 - accuracy: 0.9449 - val_loss: 1.9708 - val_accuracy: 0.5957\n",
            "Epoch 80/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1558 - accuracy: 0.9444\n",
            "Epoch 00080: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1570 - accuracy: 0.9443 - val_loss: 1.9812 - val_accuracy: 0.6130\n",
            "Epoch 81/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1606 - accuracy: 0.9447\n",
            "Epoch 00081: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1606 - accuracy: 0.9447 - val_loss: 2.2532 - val_accuracy: 0.5770\n",
            "Epoch 82/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1556 - accuracy: 0.9450\n",
            "Epoch 00082: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1559 - accuracy: 0.9449 - val_loss: 2.0738 - val_accuracy: 0.6013\n",
            "Epoch 83/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1587 - accuracy: 0.9463\n",
            "Epoch 00083: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1587 - accuracy: 0.9463 - val_loss: 2.1513 - val_accuracy: 0.5938\n",
            "Epoch 84/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1565 - accuracy: 0.9458\n",
            "Epoch 00084: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1570 - accuracy: 0.9457 - val_loss: 1.9664 - val_accuracy: 0.5901\n",
            "Epoch 85/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1501 - accuracy: 0.9472\n",
            "Epoch 00085: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1501 - accuracy: 0.9472 - val_loss: 2.0702 - val_accuracy: 0.5904\n",
            "Epoch 86/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1559 - accuracy: 0.9449\n",
            "Epoch 00086: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1561 - accuracy: 0.9449 - val_loss: 2.0240 - val_accuracy: 0.5784\n",
            "Epoch 87/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1471 - accuracy: 0.9497\n",
            "Epoch 00087: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1471 - accuracy: 0.9497 - val_loss: 1.9923 - val_accuracy: 0.5968\n",
            "Epoch 88/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1540 - accuracy: 0.9467\n",
            "Epoch 00088: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1544 - accuracy: 0.9466 - val_loss: 2.0407 - val_accuracy: 0.5874\n",
            "Epoch 89/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1486 - accuracy: 0.9498\n",
            "Epoch 00089: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1486 - accuracy: 0.9498 - val_loss: 2.1619 - val_accuracy: 0.5904\n",
            "Epoch 90/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1465 - accuracy: 0.9480\n",
            "Epoch 00090: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1465 - accuracy: 0.9480 - val_loss: 2.0728 - val_accuracy: 0.5896\n",
            "Epoch 91/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1470 - accuracy: 0.9495\n",
            "Epoch 00091: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1470 - accuracy: 0.9495 - val_loss: 1.9704 - val_accuracy: 0.5999\n",
            "Epoch 92/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1429 - accuracy: 0.9502\n",
            "Epoch 00092: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1430 - accuracy: 0.9502 - val_loss: 2.0365 - val_accuracy: 0.6066\n",
            "Epoch 93/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1465 - accuracy: 0.9491\n",
            "Epoch 00093: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1467 - accuracy: 0.9490 - val_loss: 2.0758 - val_accuracy: 0.6002\n",
            "Epoch 94/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1416 - accuracy: 0.9501\n",
            "Epoch 00094: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1420 - accuracy: 0.9499 - val_loss: 1.9986 - val_accuracy: 0.5993\n",
            "Epoch 95/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1436 - accuracy: 0.9513\n",
            "Epoch 00095: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1438 - accuracy: 0.9511 - val_loss: 2.2670 - val_accuracy: 0.5684\n",
            "Epoch 96/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9502\n",
            "Epoch 00096: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1412 - accuracy: 0.9502 - val_loss: 2.0262 - val_accuracy: 0.5818\n",
            "Epoch 97/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1405 - accuracy: 0.9514\n",
            "Epoch 00097: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1404 - accuracy: 0.9515 - val_loss: 2.0813 - val_accuracy: 0.6013\n",
            "Epoch 98/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.1375 - accuracy: 0.9519\n",
            "Epoch 00098: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1375 - accuracy: 0.9519 - val_loss: 1.9941 - val_accuracy: 0.5874\n",
            "Epoch 99/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.9462\n",
            "Epoch 00099: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1536 - accuracy: 0.9465 - val_loss: 2.3984 - val_accuracy: 0.5578\n",
            "Epoch 100/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.1352 - accuracy: 0.9535\n",
            "Epoch 00100: val_loss did not improve from 1.20271\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.1351 - accuracy: 0.9535 - val_loss: 2.0035 - val_accuracy: 0.5993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8F6tX7mEps5"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "dropout3 = Sequential()\n",
        "\n",
        "dropout3.add(Conv2D(filters=32,kernel_size=2,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "dropout3.add(BatchNormalization())\n",
        "dropout3.add(MaxPooling2D(pool_size=2))\n",
        "dropout3.add(Dropout(0.3))\n",
        "dropout3.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'))\n",
        "dropout3.add(BatchNormalization())\n",
        "dropout3.add(MaxPooling2D(pool_size=2))\n",
        "dropout3.add(Dropout(0.3))\n",
        "dropout3.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
        "dropout3.add(BatchNormalization())\n",
        "dropout3.add(MaxPooling2D(pool_size=2))\n",
        "dropout3.add(Dropout(0.3))\n",
        "dropout3.add(Flatten())\n",
        "dropout3.add(Dense(256,activation='relu'))\n",
        "dropout3.add(BatchNormalization())\n",
        "dropout3.add(Dropout(0.3))\n",
        "dropout3.add(Dense(512,activation='relu'))\n",
        "dropout3.add(BatchNormalization())\n",
        "dropout3.add(Dropout(0.3))\n",
        "dropout3.add(Dense(7,activation='softmax'))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUg20h1WEpxK"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "dropout3.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONE-xYH3Ep1U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf7868d7-3f13-48b1-984c-08414d92070b"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 100\n",
        "\n",
        "checkpointer_dropout3 = ModelCheckpoint(filepath='dropout3.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_dropout3 = dropout3.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(v_inputs , v_targets),\n",
        "          epochs= epoch, batch_size=128, callbacks=[checkpointer_dropout3], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "222/225 [============================>.] - ETA: 0s - loss: 2.0384 - accuracy: 0.2758\n",
            "Epoch 00001: val_loss improved from inf to 5.50062, saving model to dropout3.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 2.0348 - accuracy: 0.2767 - val_loss: 5.5006 - val_accuracy: 0.1156\n",
            "Epoch 2/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.6470 - accuracy: 0.3767\n",
            "Epoch 00002: val_loss improved from 5.50062 to 3.95179, saving model to dropout3.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.6470 - accuracy: 0.3767 - val_loss: 3.9518 - val_accuracy: 0.1808\n",
            "Epoch 3/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.4994 - accuracy: 0.4256\n",
            "Epoch 00003: val_loss improved from 3.95179 to 1.54345, saving model to dropout3.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.4994 - accuracy: 0.4256 - val_loss: 1.5435 - val_accuracy: 0.4099\n",
            "Epoch 4/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.4108 - accuracy: 0.4586\n",
            "Epoch 00004: val_loss improved from 1.54345 to 1.39914, saving model to dropout3.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.4096 - accuracy: 0.4590 - val_loss: 1.3991 - val_accuracy: 0.4759\n",
            "Epoch 5/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.3481 - accuracy: 0.4812\n",
            "Epoch 00005: val_loss improved from 1.39914 to 1.31665, saving model to dropout3.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.3481 - accuracy: 0.4815 - val_loss: 1.3167 - val_accuracy: 0.4957\n",
            "Epoch 6/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.3050 - accuracy: 0.5010\n",
            "Epoch 00006: val_loss improved from 1.31665 to 1.24645, saving model to dropout3.hdf5\n",
            "225/225 [==============================] - 3s 16ms/step - loss: 1.3050 - accuracy: 0.5010 - val_loss: 1.2465 - val_accuracy: 0.5277\n",
            "Epoch 7/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.2621 - accuracy: 0.5170\n",
            "Epoch 00007: val_loss did not improve from 1.24645\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2621 - accuracy: 0.5170 - val_loss: 1.3002 - val_accuracy: 0.5091\n",
            "Epoch 8/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.2388 - accuracy: 0.5288\n",
            "Epoch 00008: val_loss improved from 1.24645 to 1.24290, saving model to dropout3.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2378 - accuracy: 0.5293 - val_loss: 1.2429 - val_accuracy: 0.5294\n",
            "Epoch 9/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.2084 - accuracy: 0.5393\n",
            "Epoch 00009: val_loss did not improve from 1.24290\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2084 - accuracy: 0.5393 - val_loss: 1.2698 - val_accuracy: 0.5216\n",
            "Epoch 10/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.1806 - accuracy: 0.5492\n",
            "Epoch 00010: val_loss did not improve from 1.24290\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1806 - accuracy: 0.5492 - val_loss: 1.2607 - val_accuracy: 0.5305\n",
            "Epoch 11/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.1515 - accuracy: 0.5598\n",
            "Epoch 00011: val_loss improved from 1.24290 to 1.20171, saving model to dropout3.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1512 - accuracy: 0.5600 - val_loss: 1.2017 - val_accuracy: 0.5553\n",
            "Epoch 12/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.1269 - accuracy: 0.5726\n",
            "Epoch 00012: val_loss did not improve from 1.20171\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1276 - accuracy: 0.5724 - val_loss: 1.2140 - val_accuracy: 0.5422\n",
            "Epoch 13/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.1005 - accuracy: 0.5810\n",
            "Epoch 00013: val_loss did not improve from 1.20171\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1005 - accuracy: 0.5810 - val_loss: 1.2567 - val_accuracy: 0.5419\n",
            "Epoch 14/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0762 - accuracy: 0.5912\n",
            "Epoch 00014: val_loss did not improve from 1.20171\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0762 - accuracy: 0.5912 - val_loss: 1.2399 - val_accuracy: 0.5414\n",
            "Epoch 15/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0538 - accuracy: 0.6019\n",
            "Epoch 00015: val_loss improved from 1.20171 to 1.16819, saving model to dropout3.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0538 - accuracy: 0.6019 - val_loss: 1.1682 - val_accuracy: 0.5692\n",
            "Epoch 16/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.0259 - accuracy: 0.6123\n",
            "Epoch 00016: val_loss did not improve from 1.16819\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0256 - accuracy: 0.6125 - val_loss: 1.2304 - val_accuracy: 0.5503\n",
            "Epoch 17/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.0043 - accuracy: 0.6225\n",
            "Epoch 00017: val_loss did not improve from 1.16819\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0043 - accuracy: 0.6226 - val_loss: 1.2747 - val_accuracy: 0.5447\n",
            "Epoch 18/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9831 - accuracy: 0.6293\n",
            "Epoch 00018: val_loss did not improve from 1.16819\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9831 - accuracy: 0.6293 - val_loss: 1.3511 - val_accuracy: 0.5191\n",
            "Epoch 19/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9567 - accuracy: 0.6438\n",
            "Epoch 00019: val_loss did not improve from 1.16819\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9567 - accuracy: 0.6438 - val_loss: 1.2363 - val_accuracy: 0.5489\n",
            "Epoch 20/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9297 - accuracy: 0.6522\n",
            "Epoch 00020: val_loss did not improve from 1.16819\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9300 - accuracy: 0.6521 - val_loss: 1.1906 - val_accuracy: 0.5651\n",
            "Epoch 21/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9082 - accuracy: 0.6571\n",
            "Epoch 00021: val_loss did not improve from 1.16819\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9082 - accuracy: 0.6571 - val_loss: 1.1763 - val_accuracy: 0.5804\n",
            "Epoch 22/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8838 - accuracy: 0.6721\n",
            "Epoch 00022: val_loss did not improve from 1.16819\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8838 - accuracy: 0.6721 - val_loss: 1.3130 - val_accuracy: 0.5500\n",
            "Epoch 23/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8666 - accuracy: 0.6733\n",
            "Epoch 00023: val_loss improved from 1.16819 to 1.16680, saving model to dropout3.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8671 - accuracy: 0.6730 - val_loss: 1.1668 - val_accuracy: 0.5913\n",
            "Epoch 24/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8459 - accuracy: 0.6831\n",
            "Epoch 00024: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8459 - accuracy: 0.6831 - val_loss: 1.1911 - val_accuracy: 0.5815\n",
            "Epoch 25/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.8225 - accuracy: 0.6910\n",
            "Epoch 00025: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8224 - accuracy: 0.6911 - val_loss: 1.1807 - val_accuracy: 0.5938\n",
            "Epoch 26/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8051 - accuracy: 0.6986\n",
            "Epoch 00026: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8051 - accuracy: 0.6986 - val_loss: 1.1911 - val_accuracy: 0.5913\n",
            "Epoch 27/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7802 - accuracy: 0.7071\n",
            "Epoch 00027: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7809 - accuracy: 0.7069 - val_loss: 1.1876 - val_accuracy: 0.5907\n",
            "Epoch 28/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7661 - accuracy: 0.7118\n",
            "Epoch 00028: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7670 - accuracy: 0.7114 - val_loss: 1.1684 - val_accuracy: 0.6024\n",
            "Epoch 29/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7496 - accuracy: 0.7215\n",
            "Epoch 00029: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7494 - accuracy: 0.7214 - val_loss: 1.1907 - val_accuracy: 0.5963\n",
            "Epoch 30/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7258 - accuracy: 0.7325\n",
            "Epoch 00030: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7252 - accuracy: 0.7326 - val_loss: 1.1736 - val_accuracy: 0.5965\n",
            "Epoch 31/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7128 - accuracy: 0.7355\n",
            "Epoch 00031: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7123 - accuracy: 0.7354 - val_loss: 1.3066 - val_accuracy: 0.5614\n",
            "Epoch 32/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7037 - accuracy: 0.7362\n",
            "Epoch 00032: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7040 - accuracy: 0.7361 - val_loss: 1.2988 - val_accuracy: 0.5826\n",
            "Epoch 33/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6774 - accuracy: 0.7480\n",
            "Epoch 00033: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6774 - accuracy: 0.7480 - val_loss: 1.2031 - val_accuracy: 0.5882\n",
            "Epoch 34/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6615 - accuracy: 0.7524\n",
            "Epoch 00034: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6615 - accuracy: 0.7524 - val_loss: 1.2239 - val_accuracy: 0.5971\n",
            "Epoch 35/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6501 - accuracy: 0.7592\n",
            "Epoch 00035: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6500 - accuracy: 0.7592 - val_loss: 1.2852 - val_accuracy: 0.5913\n",
            "Epoch 36/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6356 - accuracy: 0.7663\n",
            "Epoch 00036: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6358 - accuracy: 0.7662 - val_loss: 1.3185 - val_accuracy: 0.5712\n",
            "Epoch 37/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6150 - accuracy: 0.7704\n",
            "Epoch 00037: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6152 - accuracy: 0.7703 - val_loss: 1.3267 - val_accuracy: 0.5662\n",
            "Epoch 38/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6091 - accuracy: 0.7728\n",
            "Epoch 00038: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6091 - accuracy: 0.7729 - val_loss: 1.2476 - val_accuracy: 0.6035\n",
            "Epoch 39/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6020 - accuracy: 0.7788\n",
            "Epoch 00039: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6020 - accuracy: 0.7788 - val_loss: 1.2500 - val_accuracy: 0.5832\n",
            "Epoch 40/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.5932 - accuracy: 0.7802\n",
            "Epoch 00040: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5929 - accuracy: 0.7804 - val_loss: 1.2626 - val_accuracy: 0.5954\n",
            "Epoch 41/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5713 - accuracy: 0.7899\n",
            "Epoch 00041: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5713 - accuracy: 0.7899 - val_loss: 1.2459 - val_accuracy: 0.6035\n",
            "Epoch 42/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5633 - accuracy: 0.7919\n",
            "Epoch 00042: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5633 - accuracy: 0.7919 - val_loss: 1.2473 - val_accuracy: 0.6144\n",
            "Epoch 43/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.5608 - accuracy: 0.7920\n",
            "Epoch 00043: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5611 - accuracy: 0.7922 - val_loss: 1.2963 - val_accuracy: 0.6004\n",
            "Epoch 44/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5491 - accuracy: 0.7996\n",
            "Epoch 00044: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5491 - accuracy: 0.7996 - val_loss: 1.4861 - val_accuracy: 0.5612\n",
            "Epoch 45/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.5279 - accuracy: 0.8051\n",
            "Epoch 00045: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5278 - accuracy: 0.8049 - val_loss: 1.2632 - val_accuracy: 0.6038\n",
            "Epoch 46/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.5244 - accuracy: 0.8069\n",
            "Epoch 00046: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5259 - accuracy: 0.8064 - val_loss: 1.3561 - val_accuracy: 0.5954\n",
            "Epoch 47/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5241 - accuracy: 0.8071\n",
            "Epoch 00047: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5241 - accuracy: 0.8071 - val_loss: 1.3535 - val_accuracy: 0.5957\n",
            "Epoch 48/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5023 - accuracy: 0.8145\n",
            "Epoch 00048: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5023 - accuracy: 0.8145 - val_loss: 1.3350 - val_accuracy: 0.6018\n",
            "Epoch 49/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4957 - accuracy: 0.8184\n",
            "Epoch 00049: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4957 - accuracy: 0.8183 - val_loss: 1.2952 - val_accuracy: 0.6077\n",
            "Epoch 50/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4915 - accuracy: 0.8171\n",
            "Epoch 00050: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4915 - accuracy: 0.8171 - val_loss: 1.3183 - val_accuracy: 0.6041\n",
            "Epoch 51/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4859 - accuracy: 0.8225\n",
            "Epoch 00051: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4859 - accuracy: 0.8225 - val_loss: 1.3634 - val_accuracy: 0.6024\n",
            "Epoch 52/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4792 - accuracy: 0.8227\n",
            "Epoch 00052: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4788 - accuracy: 0.8229 - val_loss: 1.3251 - val_accuracy: 0.6043\n",
            "Epoch 53/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4753 - accuracy: 0.8271\n",
            "Epoch 00053: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4756 - accuracy: 0.8268 - val_loss: 1.4058 - val_accuracy: 0.6160\n",
            "Epoch 54/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4656 - accuracy: 0.8279\n",
            "Epoch 00054: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4656 - accuracy: 0.8279 - val_loss: 1.3418 - val_accuracy: 0.6121\n",
            "Epoch 55/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4608 - accuracy: 0.8310\n",
            "Epoch 00055: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4612 - accuracy: 0.8308 - val_loss: 1.3488 - val_accuracy: 0.6021\n",
            "Epoch 56/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4543 - accuracy: 0.8345\n",
            "Epoch 00056: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4545 - accuracy: 0.8343 - val_loss: 1.3675 - val_accuracy: 0.6041\n",
            "Epoch 57/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4334 - accuracy: 0.8421\n",
            "Epoch 00057: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4340 - accuracy: 0.8419 - val_loss: 1.4388 - val_accuracy: 0.6010\n",
            "Epoch 58/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4471 - accuracy: 0.8383\n",
            "Epoch 00058: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4471 - accuracy: 0.8383 - val_loss: 1.4262 - val_accuracy: 0.6018\n",
            "Epoch 59/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4269 - accuracy: 0.8446\n",
            "Epoch 00059: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4276 - accuracy: 0.8443 - val_loss: 1.4450 - val_accuracy: 0.5988\n",
            "Epoch 60/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4315 - accuracy: 0.8416\n",
            "Epoch 00060: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4315 - accuracy: 0.8416 - val_loss: 1.4454 - val_accuracy: 0.6060\n",
            "Epoch 61/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4253 - accuracy: 0.8464\n",
            "Epoch 00061: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4254 - accuracy: 0.8464 - val_loss: 1.3621 - val_accuracy: 0.6085\n",
            "Epoch 62/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4166 - accuracy: 0.8499\n",
            "Epoch 00062: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4166 - accuracy: 0.8499 - val_loss: 1.4530 - val_accuracy: 0.6088\n",
            "Epoch 63/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4165 - accuracy: 0.8491\n",
            "Epoch 00063: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4165 - accuracy: 0.8491 - val_loss: 1.4154 - val_accuracy: 0.5996\n",
            "Epoch 64/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4054 - accuracy: 0.8551\n",
            "Epoch 00064: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4065 - accuracy: 0.8546 - val_loss: 1.4001 - val_accuracy: 0.6119\n",
            "Epoch 65/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4103 - accuracy: 0.8504\n",
            "Epoch 00065: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4096 - accuracy: 0.8508 - val_loss: 1.5044 - val_accuracy: 0.5918\n",
            "Epoch 66/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4044 - accuracy: 0.8551\n",
            "Epoch 00066: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4049 - accuracy: 0.8550 - val_loss: 1.4078 - val_accuracy: 0.6124\n",
            "Epoch 67/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4005 - accuracy: 0.8532\n",
            "Epoch 00067: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4006 - accuracy: 0.8533 - val_loss: 1.4638 - val_accuracy: 0.6018\n",
            "Epoch 68/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3962 - accuracy: 0.8559\n",
            "Epoch 00068: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3970 - accuracy: 0.8556 - val_loss: 1.4625 - val_accuracy: 0.6046\n",
            "Epoch 69/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3944 - accuracy: 0.8587\n",
            "Epoch 00069: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3948 - accuracy: 0.8587 - val_loss: 1.5043 - val_accuracy: 0.5893\n",
            "Epoch 70/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3775 - accuracy: 0.8643\n",
            "Epoch 00070: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3775 - accuracy: 0.8643 - val_loss: 1.5046 - val_accuracy: 0.5952\n",
            "Epoch 71/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3828 - accuracy: 0.8610\n",
            "Epoch 00071: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3829 - accuracy: 0.8610 - val_loss: 1.4353 - val_accuracy: 0.6046\n",
            "Epoch 72/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3735 - accuracy: 0.8666\n",
            "Epoch 00072: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3735 - accuracy: 0.8666 - val_loss: 1.4674 - val_accuracy: 0.6099\n",
            "Epoch 73/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3682 - accuracy: 0.8688\n",
            "Epoch 00073: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3697 - accuracy: 0.8683 - val_loss: 1.4403 - val_accuracy: 0.6124\n",
            "Epoch 74/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3658 - accuracy: 0.8690\n",
            "Epoch 00074: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3672 - accuracy: 0.8684 - val_loss: 1.4558 - val_accuracy: 0.6066\n",
            "Epoch 75/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3640 - accuracy: 0.8685\n",
            "Epoch 00075: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3640 - accuracy: 0.8685 - val_loss: 1.4409 - val_accuracy: 0.6088\n",
            "Epoch 76/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3646 - accuracy: 0.8663\n",
            "Epoch 00076: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3644 - accuracy: 0.8665 - val_loss: 1.5492 - val_accuracy: 0.5879\n",
            "Epoch 77/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3535 - accuracy: 0.8709\n",
            "Epoch 00077: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3535 - accuracy: 0.8709 - val_loss: 1.4418 - val_accuracy: 0.6082\n",
            "Epoch 78/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3576 - accuracy: 0.8716\n",
            "Epoch 00078: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3579 - accuracy: 0.8716 - val_loss: 1.4369 - val_accuracy: 0.6094\n",
            "Epoch 79/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3516 - accuracy: 0.8744\n",
            "Epoch 00079: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3516 - accuracy: 0.8744 - val_loss: 2.1851 - val_accuracy: 0.4923\n",
            "Epoch 80/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3520 - accuracy: 0.8722\n",
            "Epoch 00080: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3520 - accuracy: 0.8722 - val_loss: 1.5820 - val_accuracy: 0.5843\n",
            "Epoch 81/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3565 - accuracy: 0.8714\n",
            "Epoch 00081: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3564 - accuracy: 0.8715 - val_loss: 1.5347 - val_accuracy: 0.6030\n",
            "Epoch 82/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3464 - accuracy: 0.8748\n",
            "Epoch 00082: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3469 - accuracy: 0.8748 - val_loss: 1.6532 - val_accuracy: 0.5896\n",
            "Epoch 83/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3481 - accuracy: 0.8733\n",
            "Epoch 00083: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3481 - accuracy: 0.8733 - val_loss: 1.7264 - val_accuracy: 0.5801\n",
            "Epoch 84/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3400 - accuracy: 0.8779\n",
            "Epoch 00084: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3401 - accuracy: 0.8780 - val_loss: 1.6026 - val_accuracy: 0.5985\n",
            "Epoch 85/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3353 - accuracy: 0.8810\n",
            "Epoch 00085: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3353 - accuracy: 0.8809 - val_loss: 1.6927 - val_accuracy: 0.5776\n",
            "Epoch 86/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3376 - accuracy: 0.8775\n",
            "Epoch 00086: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3374 - accuracy: 0.8775 - val_loss: 1.5269 - val_accuracy: 0.6096\n",
            "Epoch 87/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3359 - accuracy: 0.8797\n",
            "Epoch 00087: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3360 - accuracy: 0.8797 - val_loss: 1.5067 - val_accuracy: 0.6030\n",
            "Epoch 88/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3299 - accuracy: 0.8817\n",
            "Epoch 00088: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3299 - accuracy: 0.8817 - val_loss: 1.5999 - val_accuracy: 0.5907\n",
            "Epoch 89/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3239 - accuracy: 0.8824\n",
            "Epoch 00089: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3239 - accuracy: 0.8824 - val_loss: 1.6455 - val_accuracy: 0.5874\n",
            "Epoch 90/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3233 - accuracy: 0.8854\n",
            "Epoch 00090: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3233 - accuracy: 0.8854 - val_loss: 1.5256 - val_accuracy: 0.6105\n",
            "Epoch 91/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3307 - accuracy: 0.8816\n",
            "Epoch 00091: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3307 - accuracy: 0.8814 - val_loss: 1.5227 - val_accuracy: 0.6091\n",
            "Epoch 92/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3198 - accuracy: 0.8850\n",
            "Epoch 00092: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3206 - accuracy: 0.8846 - val_loss: 1.5621 - val_accuracy: 0.6041\n",
            "Epoch 93/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3221 - accuracy: 0.8850\n",
            "Epoch 00093: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3222 - accuracy: 0.8850 - val_loss: 1.4978 - val_accuracy: 0.6127\n",
            "Epoch 94/100\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.8881\n",
            "Epoch 00094: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3129 - accuracy: 0.8879 - val_loss: 1.8145 - val_accuracy: 0.5740\n",
            "Epoch 95/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3261 - accuracy: 0.8816\n",
            "Epoch 00095: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3257 - accuracy: 0.8818 - val_loss: 1.5567 - val_accuracy: 0.5848\n",
            "Epoch 96/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3121 - accuracy: 0.8883\n",
            "Epoch 00096: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3119 - accuracy: 0.8884 - val_loss: 1.5206 - val_accuracy: 0.6080\n",
            "Epoch 97/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3055 - accuracy: 0.8901\n",
            "Epoch 00097: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3057 - accuracy: 0.8900 - val_loss: 1.5439 - val_accuracy: 0.6041\n",
            "Epoch 98/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.3024 - accuracy: 0.8893\n",
            "Epoch 00098: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3024 - accuracy: 0.8893 - val_loss: 1.5410 - val_accuracy: 0.6055\n",
            "Epoch 99/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3041 - accuracy: 0.8923\n",
            "Epoch 00099: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3053 - accuracy: 0.8919 - val_loss: 1.6734 - val_accuracy: 0.5926\n",
            "Epoch 100/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.8909\n",
            "Epoch 00100: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.3053 - accuracy: 0.8908 - val_loss: 1.5645 - val_accuracy: 0.6060\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGrcf8RdEp9v"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "dropout4 = Sequential()\n",
        "\n",
        "dropout4.add(Conv2D(filters=32,kernel_size=2,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "dropout4.add(BatchNormalization())\n",
        "dropout4.add(MaxPooling2D(pool_size=2))\n",
        "dropout4.add(Dropout(0.4))\n",
        "dropout4.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'))\n",
        "dropout4.add(BatchNormalization())\n",
        "dropout4.add(MaxPooling2D(pool_size=2))\n",
        "dropout4.add(Dropout(0.4))\n",
        "dropout4.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
        "dropout4.add(BatchNormalization())\n",
        "dropout4.add(MaxPooling2D(pool_size=2))\n",
        "dropout4.add(Dropout(0.4))\n",
        "dropout4.add(Flatten())\n",
        "dropout4.add(Dense(256,activation='relu'))\n",
        "dropout4.add(BatchNormalization())\n",
        "dropout4.add(Dropout(0.4))\n",
        "dropout4.add(Dense(512,activation='relu'))\n",
        "dropout4.add(BatchNormalization())\n",
        "dropout4.add(Dropout(0.4))\n",
        "dropout4.add(Dense(7,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ui6k087bEqCs"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "dropout4.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQ_AcLskEqA2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d02301d-c49c-4d6e-c2ba-332094ddef13"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 100\n",
        "\n",
        "checkpointer_dropout4 = ModelCheckpoint(filepath='dropout4.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_dropout4 = dropout4.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(v_inputs , v_targets),\n",
        "          epochs= epoch, batch_size=128, callbacks=[checkpointer_dropout4], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "222/225 [============================>.] - ETA: 0s - loss: 2.1900 - accuracy: 0.2356\n",
            "Epoch 00001: val_loss improved from inf to 6.89517, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 2.1872 - accuracy: 0.2355 - val_loss: 6.8952 - val_accuracy: 0.2494\n",
            "Epoch 2/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.7561 - accuracy: 0.3317\n",
            "Epoch 00002: val_loss improved from 6.89517 to 2.59276, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.7561 - accuracy: 0.3317 - val_loss: 2.5928 - val_accuracy: 0.1758\n",
            "Epoch 3/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.6141 - accuracy: 0.3787\n",
            "Epoch 00003: val_loss improved from 2.59276 to 1.69450, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.6141 - accuracy: 0.3787 - val_loss: 1.6945 - val_accuracy: 0.3915\n",
            "Epoch 4/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.5386 - accuracy: 0.4060\n",
            "Epoch 00004: val_loss improved from 1.69450 to 1.49743, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.5382 - accuracy: 0.4060 - val_loss: 1.4974 - val_accuracy: 0.4355\n",
            "Epoch 5/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.4841 - accuracy: 0.4252\n",
            "Epoch 00005: val_loss improved from 1.49743 to 1.49196, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.4844 - accuracy: 0.4247 - val_loss: 1.4920 - val_accuracy: 0.4238\n",
            "Epoch 6/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.4383 - accuracy: 0.4462\n",
            "Epoch 00006: val_loss improved from 1.49196 to 1.36374, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.4383 - accuracy: 0.4462 - val_loss: 1.3637 - val_accuracy: 0.4798\n",
            "Epoch 7/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.4059 - accuracy: 0.4579\n",
            "Epoch 00007: val_loss improved from 1.36374 to 1.34963, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.4059 - accuracy: 0.4579 - val_loss: 1.3496 - val_accuracy: 0.4859\n",
            "Epoch 8/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.3761 - accuracy: 0.4711\n",
            "Epoch 00008: val_loss improved from 1.34963 to 1.30720, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.3765 - accuracy: 0.4710 - val_loss: 1.3072 - val_accuracy: 0.5040\n",
            "Epoch 9/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.3403 - accuracy: 0.4856\n",
            "Epoch 00009: val_loss did not improve from 1.30720\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.3418 - accuracy: 0.4846 - val_loss: 1.3089 - val_accuracy: 0.4968\n",
            "Epoch 10/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.3229 - accuracy: 0.4946\n",
            "Epoch 00010: val_loss did not improve from 1.30720\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.3230 - accuracy: 0.4944 - val_loss: 1.3697 - val_accuracy: 0.4893\n",
            "Epoch 11/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.3018 - accuracy: 0.5029\n",
            "Epoch 00011: val_loss improved from 1.30720 to 1.30111, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.3018 - accuracy: 0.5029 - val_loss: 1.3011 - val_accuracy: 0.4999\n",
            "Epoch 12/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.2841 - accuracy: 0.5071\n",
            "Epoch 00012: val_loss improved from 1.30111 to 1.25863, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2840 - accuracy: 0.5072 - val_loss: 1.2586 - val_accuracy: 0.5194\n",
            "Epoch 13/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.2615 - accuracy: 0.5158\n",
            "Epoch 00013: val_loss improved from 1.25863 to 1.24942, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2615 - accuracy: 0.5158 - val_loss: 1.2494 - val_accuracy: 0.5274\n",
            "Epoch 14/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.2481 - accuracy: 0.5253\n",
            "Epoch 00014: val_loss improved from 1.24942 to 1.21364, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2475 - accuracy: 0.5253 - val_loss: 1.2136 - val_accuracy: 0.5358\n",
            "Epoch 15/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.2235 - accuracy: 0.5302\n",
            "Epoch 00015: val_loss did not improve from 1.21364\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2231 - accuracy: 0.5301 - val_loss: 1.2924 - val_accuracy: 0.5113\n",
            "Epoch 16/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.2075 - accuracy: 0.5422\n",
            "Epoch 00016: val_loss did not improve from 1.21364\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2075 - accuracy: 0.5422 - val_loss: 1.2301 - val_accuracy: 0.5313\n",
            "Epoch 17/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.1837 - accuracy: 0.5510\n",
            "Epoch 00017: val_loss did not improve from 1.21364\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1849 - accuracy: 0.5508 - val_loss: 1.2348 - val_accuracy: 0.5417\n",
            "Epoch 18/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.1675 - accuracy: 0.5572\n",
            "Epoch 00018: val_loss improved from 1.21364 to 1.20354, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1679 - accuracy: 0.5572 - val_loss: 1.2035 - val_accuracy: 0.5444\n",
            "Epoch 19/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.1580 - accuracy: 0.5599\n",
            "Epoch 00019: val_loss did not improve from 1.20354\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1580 - accuracy: 0.5599 - val_loss: 1.2189 - val_accuracy: 0.5458\n",
            "Epoch 20/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.1505 - accuracy: 0.5639\n",
            "Epoch 00020: val_loss improved from 1.20354 to 1.19419, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1505 - accuracy: 0.5639 - val_loss: 1.1942 - val_accuracy: 0.5528\n",
            "Epoch 21/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.1382 - accuracy: 0.5707\n",
            "Epoch 00021: val_loss improved from 1.19419 to 1.13087, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1382 - accuracy: 0.5707 - val_loss: 1.1309 - val_accuracy: 0.5687\n",
            "Epoch 22/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.1234 - accuracy: 0.5743\n",
            "Epoch 00022: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1232 - accuracy: 0.5744 - val_loss: 1.1696 - val_accuracy: 0.5642\n",
            "Epoch 23/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.1039 - accuracy: 0.5821\n",
            "Epoch 00023: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1039 - accuracy: 0.5821 - val_loss: 1.2269 - val_accuracy: 0.5575\n",
            "Epoch 24/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0832 - accuracy: 0.5921\n",
            "Epoch 00024: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0832 - accuracy: 0.5921 - val_loss: 1.1895 - val_accuracy: 0.5520\n",
            "Epoch 25/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.0790 - accuracy: 0.5922\n",
            "Epoch 00025: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0805 - accuracy: 0.5915 - val_loss: 1.1737 - val_accuracy: 0.5578\n",
            "Epoch 26/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.0684 - accuracy: 0.5922\n",
            "Epoch 00026: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0668 - accuracy: 0.5926 - val_loss: 1.1549 - val_accuracy: 0.5653\n",
            "Epoch 27/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0485 - accuracy: 0.6009\n",
            "Epoch 00027: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0485 - accuracy: 0.6009 - val_loss: 1.2240 - val_accuracy: 0.5450\n",
            "Epoch 28/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.0368 - accuracy: 0.6066\n",
            "Epoch 00028: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0371 - accuracy: 0.6064 - val_loss: 1.1744 - val_accuracy: 0.5620\n",
            "Epoch 29/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.0328 - accuracy: 0.6099\n",
            "Epoch 00029: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0331 - accuracy: 0.6101 - val_loss: 1.1892 - val_accuracy: 0.5511\n",
            "Epoch 30/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0081 - accuracy: 0.6191\n",
            "Epoch 00030: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0081 - accuracy: 0.6191 - val_loss: 1.1453 - val_accuracy: 0.5768\n",
            "Epoch 31/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9991 - accuracy: 0.6260\n",
            "Epoch 00031: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9991 - accuracy: 0.6260 - val_loss: 1.2004 - val_accuracy: 0.5678\n",
            "Epoch 32/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9847 - accuracy: 0.6275\n",
            "Epoch 00032: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9852 - accuracy: 0.6271 - val_loss: 1.3162 - val_accuracy: 0.5364\n",
            "Epoch 33/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9750 - accuracy: 0.6316\n",
            "Epoch 00033: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9750 - accuracy: 0.6317 - val_loss: 1.3164 - val_accuracy: 0.5316\n",
            "Epoch 34/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9610 - accuracy: 0.6380\n",
            "Epoch 00034: val_loss did not improve from 1.13087\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9610 - accuracy: 0.6380 - val_loss: 1.1833 - val_accuracy: 0.5737\n",
            "Epoch 35/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9552 - accuracy: 0.6384\n",
            "Epoch 00035: val_loss improved from 1.13087 to 1.12376, saving model to dropout4.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9552 - accuracy: 0.6385 - val_loss: 1.1238 - val_accuracy: 0.6049\n",
            "Epoch 36/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9440 - accuracy: 0.6425\n",
            "Epoch 00036: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9445 - accuracy: 0.6424 - val_loss: 1.2946 - val_accuracy: 0.5405\n",
            "Epoch 37/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9345 - accuracy: 0.6517\n",
            "Epoch 00037: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9345 - accuracy: 0.6517 - val_loss: 1.1897 - val_accuracy: 0.5712\n",
            "Epoch 38/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9255 - accuracy: 0.6536\n",
            "Epoch 00038: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9259 - accuracy: 0.6534 - val_loss: 1.1439 - val_accuracy: 0.5957\n",
            "Epoch 39/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9107 - accuracy: 0.6569\n",
            "Epoch 00039: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9111 - accuracy: 0.6569 - val_loss: 1.4717 - val_accuracy: 0.5146\n",
            "Epoch 40/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9013 - accuracy: 0.6613\n",
            "Epoch 00040: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9006 - accuracy: 0.6613 - val_loss: 1.1569 - val_accuracy: 0.5910\n",
            "Epoch 41/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9010 - accuracy: 0.6638\n",
            "Epoch 00041: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9011 - accuracy: 0.6638 - val_loss: 1.1389 - val_accuracy: 0.5957\n",
            "Epoch 42/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8855 - accuracy: 0.6686\n",
            "Epoch 00042: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8855 - accuracy: 0.6686 - val_loss: 1.1382 - val_accuracy: 0.5940\n",
            "Epoch 43/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.8759 - accuracy: 0.6732\n",
            "Epoch 00043: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8759 - accuracy: 0.6731 - val_loss: 1.1251 - val_accuracy: 0.6018\n",
            "Epoch 44/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8697 - accuracy: 0.6755\n",
            "Epoch 00044: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8697 - accuracy: 0.6755 - val_loss: 1.1354 - val_accuracy: 0.5968\n",
            "Epoch 45/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8577 - accuracy: 0.6799\n",
            "Epoch 00045: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8577 - accuracy: 0.6799 - val_loss: 1.1330 - val_accuracy: 0.5977\n",
            "Epoch 46/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.8533 - accuracy: 0.6774\n",
            "Epoch 00046: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8531 - accuracy: 0.6776 - val_loss: 1.3836 - val_accuracy: 0.5464\n",
            "Epoch 47/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8545 - accuracy: 0.6794\n",
            "Epoch 00047: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8545 - accuracy: 0.6794 - val_loss: 1.2003 - val_accuracy: 0.5963\n",
            "Epoch 48/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.8475 - accuracy: 0.6876\n",
            "Epoch 00048: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8473 - accuracy: 0.6878 - val_loss: 1.1840 - val_accuracy: 0.5907\n",
            "Epoch 49/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8345 - accuracy: 0.6877\n",
            "Epoch 00049: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8345 - accuracy: 0.6877 - val_loss: 1.2734 - val_accuracy: 0.5570\n",
            "Epoch 50/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8307 - accuracy: 0.6926\n",
            "Epoch 00050: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8307 - accuracy: 0.6926 - val_loss: 1.1484 - val_accuracy: 0.5988\n",
            "Epoch 51/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.8208 - accuracy: 0.6927\n",
            "Epoch 00051: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8206 - accuracy: 0.6929 - val_loss: 1.1444 - val_accuracy: 0.5979\n",
            "Epoch 52/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8132 - accuracy: 0.6939\n",
            "Epoch 00052: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8132 - accuracy: 0.6939 - val_loss: 1.1308 - val_accuracy: 0.6021\n",
            "Epoch 53/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7988 - accuracy: 0.7021\n",
            "Epoch 00053: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7988 - accuracy: 0.7021 - val_loss: 1.1433 - val_accuracy: 0.6024\n",
            "Epoch 54/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7939 - accuracy: 0.7031\n",
            "Epoch 00054: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7948 - accuracy: 0.7027 - val_loss: 1.1993 - val_accuracy: 0.5943\n",
            "Epoch 55/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7912 - accuracy: 0.7068\n",
            "Epoch 00055: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7912 - accuracy: 0.7068 - val_loss: 1.1240 - val_accuracy: 0.6082\n",
            "Epoch 56/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7828 - accuracy: 0.7102\n",
            "Epoch 00056: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7828 - accuracy: 0.7102 - val_loss: 1.1496 - val_accuracy: 0.6013\n",
            "Epoch 57/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7765 - accuracy: 0.7126\n",
            "Epoch 00057: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7764 - accuracy: 0.7126 - val_loss: 1.1928 - val_accuracy: 0.5904\n",
            "Epoch 58/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7728 - accuracy: 0.7124\n",
            "Epoch 00058: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7728 - accuracy: 0.7124 - val_loss: 1.2185 - val_accuracy: 0.5857\n",
            "Epoch 59/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7646 - accuracy: 0.7174\n",
            "Epoch 00059: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7646 - accuracy: 0.7174 - val_loss: 1.2180 - val_accuracy: 0.5717\n",
            "Epoch 60/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7644 - accuracy: 0.7125\n",
            "Epoch 00060: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7648 - accuracy: 0.7124 - val_loss: 1.1865 - val_accuracy: 0.5949\n",
            "Epoch 61/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7474 - accuracy: 0.7248\n",
            "Epoch 00061: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7474 - accuracy: 0.7248 - val_loss: 1.1665 - val_accuracy: 0.6035\n",
            "Epoch 62/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7490 - accuracy: 0.7199\n",
            "Epoch 00062: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7490 - accuracy: 0.7198 - val_loss: 1.2138 - val_accuracy: 0.5915\n",
            "Epoch 63/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7369 - accuracy: 0.7267\n",
            "Epoch 00063: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7366 - accuracy: 0.7266 - val_loss: 1.1671 - val_accuracy: 0.6055\n",
            "Epoch 64/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7359 - accuracy: 0.7301\n",
            "Epoch 00064: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7364 - accuracy: 0.7295 - val_loss: 1.1577 - val_accuracy: 0.6046\n",
            "Epoch 65/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7334 - accuracy: 0.7258\n",
            "Epoch 00065: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7333 - accuracy: 0.7258 - val_loss: 1.1633 - val_accuracy: 0.6088\n",
            "Epoch 66/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7323 - accuracy: 0.7289\n",
            "Epoch 00066: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7323 - accuracy: 0.7289 - val_loss: 1.1728 - val_accuracy: 0.5857\n",
            "Epoch 67/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7197 - accuracy: 0.7338\n",
            "Epoch 00067: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7197 - accuracy: 0.7338 - val_loss: 1.2448 - val_accuracy: 0.5815\n",
            "Epoch 68/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7239 - accuracy: 0.7319\n",
            "Epoch 00068: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7238 - accuracy: 0.7320 - val_loss: 1.2227 - val_accuracy: 0.6013\n",
            "Epoch 69/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7149 - accuracy: 0.7339\n",
            "Epoch 00069: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7150 - accuracy: 0.7340 - val_loss: 1.1565 - val_accuracy: 0.6102\n",
            "Epoch 70/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7010 - accuracy: 0.7425\n",
            "Epoch 00070: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7010 - accuracy: 0.7424 - val_loss: 1.2044 - val_accuracy: 0.5996\n",
            "Epoch 71/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7014 - accuracy: 0.7390\n",
            "Epoch 00071: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7003 - accuracy: 0.7394 - val_loss: 1.1633 - val_accuracy: 0.6105\n",
            "Epoch 72/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6928 - accuracy: 0.7409\n",
            "Epoch 00072: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6933 - accuracy: 0.7407 - val_loss: 1.1887 - val_accuracy: 0.6110\n",
            "Epoch 73/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6964 - accuracy: 0.7425\n",
            "Epoch 00073: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6968 - accuracy: 0.7424 - val_loss: 1.1691 - val_accuracy: 0.6121\n",
            "Epoch 74/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6913 - accuracy: 0.7461\n",
            "Epoch 00074: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6913 - accuracy: 0.7461 - val_loss: 1.1836 - val_accuracy: 0.6121\n",
            "Epoch 75/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6905 - accuracy: 0.7456\n",
            "Epoch 00075: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6905 - accuracy: 0.7456 - val_loss: 1.1853 - val_accuracy: 0.6099\n",
            "Epoch 76/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6770 - accuracy: 0.7487\n",
            "Epoch 00076: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6779 - accuracy: 0.7486 - val_loss: 1.1748 - val_accuracy: 0.6177\n",
            "Epoch 77/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6776 - accuracy: 0.7513\n",
            "Epoch 00077: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6781 - accuracy: 0.7512 - val_loss: 1.1834 - val_accuracy: 0.6060\n",
            "Epoch 78/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6772 - accuracy: 0.7497\n",
            "Epoch 00078: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6775 - accuracy: 0.7495 - val_loss: 1.2148 - val_accuracy: 0.6046\n",
            "Epoch 79/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6765 - accuracy: 0.7551\n",
            "Epoch 00079: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6765 - accuracy: 0.7551 - val_loss: 1.2439 - val_accuracy: 0.5938\n",
            "Epoch 80/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6571 - accuracy: 0.7577\n",
            "Epoch 00080: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6571 - accuracy: 0.7577 - val_loss: 1.1688 - val_accuracy: 0.6133\n",
            "Epoch 81/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6632 - accuracy: 0.7570\n",
            "Epoch 00081: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6649 - accuracy: 0.7565 - val_loss: 1.1786 - val_accuracy: 0.6099\n",
            "Epoch 82/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6569 - accuracy: 0.7564\n",
            "Epoch 00082: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6572 - accuracy: 0.7566 - val_loss: 1.2039 - val_accuracy: 0.6032\n",
            "Epoch 83/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6511 - accuracy: 0.7582\n",
            "Epoch 00083: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6511 - accuracy: 0.7582 - val_loss: 1.1860 - val_accuracy: 0.6094\n",
            "Epoch 84/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6510 - accuracy: 0.7618\n",
            "Epoch 00084: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6510 - accuracy: 0.7619 - val_loss: 1.1926 - val_accuracy: 0.6049\n",
            "Epoch 85/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6616 - accuracy: 0.7555\n",
            "Epoch 00085: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6624 - accuracy: 0.7551 - val_loss: 1.1789 - val_accuracy: 0.6041\n",
            "Epoch 86/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6442 - accuracy: 0.7599\n",
            "Epoch 00086: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6449 - accuracy: 0.7598 - val_loss: 1.2221 - val_accuracy: 0.5993\n",
            "Epoch 87/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6426 - accuracy: 0.7621\n",
            "Epoch 00087: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6420 - accuracy: 0.7622 - val_loss: 1.2679 - val_accuracy: 0.6007\n",
            "Epoch 88/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6435 - accuracy: 0.7625\n",
            "Epoch 00088: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6431 - accuracy: 0.7626 - val_loss: 1.2068 - val_accuracy: 0.6121\n",
            "Epoch 89/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6261 - accuracy: 0.7698\n",
            "Epoch 00089: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6262 - accuracy: 0.7699 - val_loss: 1.2222 - val_accuracy: 0.6035\n",
            "Epoch 90/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6291 - accuracy: 0.7664\n",
            "Epoch 00090: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6291 - accuracy: 0.7664 - val_loss: 1.2204 - val_accuracy: 0.6057\n",
            "Epoch 91/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6355 - accuracy: 0.7655\n",
            "Epoch 00091: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6355 - accuracy: 0.7655 - val_loss: 1.2850 - val_accuracy: 0.5795\n",
            "Epoch 92/100\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.6219 - accuracy: 0.7714\n",
            "Epoch 00092: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6213 - accuracy: 0.7716 - val_loss: 1.2296 - val_accuracy: 0.6021\n",
            "Epoch 93/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6243 - accuracy: 0.7719\n",
            "Epoch 00093: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6248 - accuracy: 0.7718 - val_loss: 1.2227 - val_accuracy: 0.6021\n",
            "Epoch 94/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6241 - accuracy: 0.7706\n",
            "Epoch 00094: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6241 - accuracy: 0.7706 - val_loss: 1.2257 - val_accuracy: 0.6024\n",
            "Epoch 95/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6179 - accuracy: 0.7708\n",
            "Epoch 00095: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6178 - accuracy: 0.7704 - val_loss: 1.2240 - val_accuracy: 0.5954\n",
            "Epoch 96/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6189 - accuracy: 0.7724\n",
            "Epoch 00096: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6190 - accuracy: 0.7721 - val_loss: 1.2053 - val_accuracy: 0.6032\n",
            "Epoch 97/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6106 - accuracy: 0.7768\n",
            "Epoch 00097: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6106 - accuracy: 0.7768 - val_loss: 1.2026 - val_accuracy: 0.6158\n",
            "Epoch 98/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6096 - accuracy: 0.7766\n",
            "Epoch 00098: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6097 - accuracy: 0.7766 - val_loss: 1.2265 - val_accuracy: 0.6144\n",
            "Epoch 99/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6126 - accuracy: 0.7730\n",
            "Epoch 00099: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6129 - accuracy: 0.7730 - val_loss: 1.2221 - val_accuracy: 0.6016\n",
            "Epoch 100/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5998 - accuracy: 0.7796\n",
            "Epoch 00100: val_loss did not improve from 1.12376\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6000 - accuracy: 0.7795 - val_loss: 1.2031 - val_accuracy: 0.6091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WciJr88aZTU5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38a67df0-6a47-458f-b0e6-b36b1a780614"
      },
      "source": [
        "from keras.models import load_model\n",
        "loaded = load_model(\"dropout1.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])\n",
        "\n",
        "loaded = load_model(\"dropout2.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])\n",
        "\n",
        "loaded = load_model(\"dropout3.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])\n",
        "\n",
        "loaded = load_model(\"dropout4.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113/113 [==============================] - 0s 3ms/step - loss: 1.2335 - accuracy: 0.5478\n",
            "0.5477849245071411\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 1.1249 - accuracy: 0.5846\n",
            "0.5845639705657959\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 1.1172 - accuracy: 0.6010\n",
            "0.6010030508041382\n",
            "113/113 [==============================] - 0s 3ms/step - loss: 1.0703 - accuracy: 0.5960\n",
            "0.5959877371788025\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD03KLPUZTaR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "outputId": "ceeed06e-d3c0-4500-eb22-ba8c9fe67d4d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist_dropout1.history['val_loss'], label='dropout rate 0.1')\n",
        "plt.plot(hist_dropout2.history['val_loss'], label='dropout rate 0.2')\n",
        "plt.plot(hist_dropout3.history['val_loss'], label='dropout rate 0.3')\n",
        "plt.plot(hist_dropout4.history['val_loss'], label='dropout rate 0.4')\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"validation losses\")\n",
        "plt.show()\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(hist_dropout1.history['val_accuracy'], label='dropout rate 0.1')\n",
        "plt.plot(hist_dropout2.history['val_accuracy'], label='dropout rate 0.2')\n",
        "plt.plot(hist_dropout3.history['val_accuracy'], label='dropout rate 0.3')\n",
        "plt.plot(hist_dropout4.history['val_accuracy'], label='dropout rate 0.4')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"validation accuracy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'validation accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15aTlwHKmUzM"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "dropoutf = Sequential()\n",
        "\n",
        "dropoutf.add(Conv2D(filters=32,kernel_size=2,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "dropoutf.add(BatchNormalization())\n",
        "dropoutf.add(MaxPooling2D(pool_size=2))\n",
        "dropoutf.add(Dropout(0.35))\n",
        "dropoutf.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'))\n",
        "dropoutf.add(BatchNormalization())\n",
        "dropoutf.add(MaxPooling2D(pool_size=2))\n",
        "dropoutf.add(Dropout(0.35))\n",
        "dropoutf.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
        "dropoutf.add(BatchNormalization())\n",
        "dropoutf.add(MaxPooling2D(pool_size=2))\n",
        "dropoutf.add(Dropout(0.35))\n",
        "dropoutf.add(Flatten())\n",
        "dropoutf.add(Dense(256,activation='relu'))\n",
        "dropoutf.add(BatchNormalization())\n",
        "dropoutf.add(Dropout(0.35))\n",
        "dropoutf.add(Dense(512,activation='relu'))\n",
        "dropoutf.add(BatchNormalization())\n",
        "dropoutf.add(Dropout(0.35))\n",
        "dropoutf.add(Dense(7,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enXj8hF3mU3C"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "\n",
        "dropoutf.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGZcGrlUmU6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb5fdd45-97d8-4e2e-800a-7188ff2ee9a1"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 100\n",
        "\n",
        "checkpointer_dropoutf = ModelCheckpoint(filepath='dropoutf.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_dropoutf = dropoutf.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(v_inputs , v_targets),\n",
        "          epochs= epoch, batch_size=128, callbacks=[checkpointer_dropoutf], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 2.0981 - accuracy: 0.2637\n",
            "Epoch 00001: val_loss improved from inf to 3.81051, saving model to dropoutf.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 2.0962 - accuracy: 0.2641 - val_loss: 3.8105 - val_accuracy: 0.2494\n",
            "Epoch 2/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.6941 - accuracy: 0.3539\n",
            "Epoch 00002: val_loss improved from 3.81051 to 2.56195, saving model to dropoutf.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.6929 - accuracy: 0.3545 - val_loss: 2.5619 - val_accuracy: 0.2371\n",
            "Epoch 3/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.5492 - accuracy: 0.4055\n",
            "Epoch 00003: val_loss improved from 2.56195 to 1.72378, saving model to dropoutf.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.5489 - accuracy: 0.4056 - val_loss: 1.7238 - val_accuracy: 0.3792\n",
            "Epoch 4/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.4675 - accuracy: 0.4318\n",
            "Epoch 00004: val_loss improved from 1.72378 to 1.45460, saving model to dropoutf.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.4672 - accuracy: 0.4325 - val_loss: 1.4546 - val_accuracy: 0.4444\n",
            "Epoch 5/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.4092 - accuracy: 0.4562\n",
            "Epoch 00005: val_loss improved from 1.45460 to 1.32472, saving model to dropoutf.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.4092 - accuracy: 0.4562 - val_loss: 1.3247 - val_accuracy: 0.4932\n",
            "Epoch 6/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.3671 - accuracy: 0.4722\n",
            "Epoch 00006: val_loss did not improve from 1.32472\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.3675 - accuracy: 0.4722 - val_loss: 1.5025 - val_accuracy: 0.4472\n",
            "Epoch 7/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.3334 - accuracy: 0.4855\n",
            "Epoch 00007: val_loss improved from 1.32472 to 1.30987, saving model to dropoutf.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.3326 - accuracy: 0.4860 - val_loss: 1.3099 - val_accuracy: 0.4943\n",
            "Epoch 8/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.3024 - accuracy: 0.5017\n",
            "Epoch 00008: val_loss did not improve from 1.30987\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.3024 - accuracy: 0.5017 - val_loss: 1.3130 - val_accuracy: 0.5046\n",
            "Epoch 9/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.2744 - accuracy: 0.5145\n",
            "Epoch 00009: val_loss improved from 1.30987 to 1.23427, saving model to dropoutf.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.2746 - accuracy: 0.5144 - val_loss: 1.2343 - val_accuracy: 0.5280\n",
            "Epoch 10/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.2484 - accuracy: 0.5243\n",
            "Epoch 00010: val_loss improved from 1.23427 to 1.21115, saving model to dropoutf.hdf5\n",
            "225/225 [==============================] - 3s 16ms/step - loss: 1.2484 - accuracy: 0.5243 - val_loss: 1.2111 - val_accuracy: 0.5313\n",
            "Epoch 11/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.2238 - accuracy: 0.5360\n",
            "Epoch 00011: val_loss did not improve from 1.21115\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2238 - accuracy: 0.5360 - val_loss: 1.2185 - val_accuracy: 0.5347\n",
            "Epoch 12/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.2051 - accuracy: 0.5413\n",
            "Epoch 00012: val_loss did not improve from 1.21115\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.2052 - accuracy: 0.5413 - val_loss: 1.2836 - val_accuracy: 0.5152\n",
            "Epoch 13/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.1784 - accuracy: 0.5521\n",
            "Epoch 00013: val_loss did not improve from 1.21115\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1784 - accuracy: 0.5521 - val_loss: 1.3105 - val_accuracy: 0.5052\n",
            "Epoch 14/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.1615 - accuracy: 0.5582\n",
            "Epoch 00014: val_loss did not improve from 1.21115\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1615 - accuracy: 0.5582 - val_loss: 1.2251 - val_accuracy: 0.5422\n",
            "Epoch 15/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.1346 - accuracy: 0.5708\n",
            "Epoch 00015: val_loss did not improve from 1.21115\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1346 - accuracy: 0.5708 - val_loss: 1.2601 - val_accuracy: 0.5252\n",
            "Epoch 16/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.1192 - accuracy: 0.5726\n",
            "Epoch 00016: val_loss improved from 1.21115 to 1.18890, saving model to dropoutf.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1195 - accuracy: 0.5724 - val_loss: 1.1889 - val_accuracy: 0.5598\n",
            "Epoch 17/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.1011 - accuracy: 0.5837\n",
            "Epoch 00017: val_loss did not improve from 1.18890\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.1014 - accuracy: 0.5835 - val_loss: 1.3611 - val_accuracy: 0.5163\n",
            "Epoch 18/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.0795 - accuracy: 0.5909\n",
            "Epoch 00018: val_loss did not improve from 1.18890\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0789 - accuracy: 0.5908 - val_loss: 1.2371 - val_accuracy: 0.5366\n",
            "Epoch 19/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.5982\n",
            "Epoch 00019: val_loss did not improve from 1.18890\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0619 - accuracy: 0.5982 - val_loss: 1.3267 - val_accuracy: 0.5177\n",
            "Epoch 20/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.0434 - accuracy: 0.6061\n",
            "Epoch 00020: val_loss did not improve from 1.18890\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0434 - accuracy: 0.6061 - val_loss: 1.2153 - val_accuracy: 0.5481\n",
            "Epoch 21/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 1.0315 - accuracy: 0.6098\n",
            "Epoch 00021: val_loss did not improve from 1.18890\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0330 - accuracy: 0.6093 - val_loss: 1.2153 - val_accuracy: 0.5567\n",
            "Epoch 22/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0086 - accuracy: 0.6223\n",
            "Epoch 00022: val_loss did not improve from 1.18890\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 1.0086 - accuracy: 0.6223 - val_loss: 1.1956 - val_accuracy: 0.5639\n",
            "Epoch 23/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9862 - accuracy: 0.6293\n",
            "Epoch 00023: val_loss did not improve from 1.18890\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9864 - accuracy: 0.6292 - val_loss: 1.3573 - val_accuracy: 0.5224\n",
            "Epoch 24/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9720 - accuracy: 0.6342\n",
            "Epoch 00024: val_loss improved from 1.18890 to 1.17040, saving model to dropoutf.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9725 - accuracy: 0.6342 - val_loss: 1.1704 - val_accuracy: 0.5717\n",
            "Epoch 25/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9600 - accuracy: 0.6403\n",
            "Epoch 00025: val_loss did not improve from 1.17040\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9604 - accuracy: 0.6401 - val_loss: 1.2085 - val_accuracy: 0.5731\n",
            "Epoch 26/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9433 - accuracy: 0.6456\n",
            "Epoch 00026: val_loss did not improve from 1.17040\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9436 - accuracy: 0.6456 - val_loss: 1.2184 - val_accuracy: 0.5539\n",
            "Epoch 27/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9255 - accuracy: 0.6526\n",
            "Epoch 00027: val_loss did not improve from 1.17040\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9259 - accuracy: 0.6525 - val_loss: 1.3230 - val_accuracy: 0.5333\n",
            "Epoch 28/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.9057 - accuracy: 0.6591\n",
            "Epoch 00028: val_loss did not improve from 1.17040\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9064 - accuracy: 0.6588 - val_loss: 1.1747 - val_accuracy: 0.5784\n",
            "Epoch 29/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.8988 - accuracy: 0.6612\n",
            "Epoch 00029: val_loss did not improve from 1.17040\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.9000 - accuracy: 0.6601 - val_loss: 1.1803 - val_accuracy: 0.5787\n",
            "Epoch 30/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.8757 - accuracy: 0.6693\n",
            "Epoch 00030: val_loss improved from 1.17040 to 1.16821, saving model to dropoutf.hdf5\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8772 - accuracy: 0.6686 - val_loss: 1.1682 - val_accuracy: 0.5823\n",
            "Epoch 31/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.8597 - accuracy: 0.6777\n",
            "Epoch 00031: val_loss improved from 1.16821 to 1.16153, saving model to dropoutf.hdf5\n",
            "225/225 [==============================] - 3s 16ms/step - loss: 0.8596 - accuracy: 0.6776 - val_loss: 1.1615 - val_accuracy: 0.5943\n",
            "Epoch 32/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8538 - accuracy: 0.6785\n",
            "Epoch 00032: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8538 - accuracy: 0.6785 - val_loss: 1.1896 - val_accuracy: 0.5846\n",
            "Epoch 33/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.8359 - accuracy: 0.6862\n",
            "Epoch 00033: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8360 - accuracy: 0.6861 - val_loss: 1.1918 - val_accuracy: 0.5893\n",
            "Epoch 34/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.8272 - accuracy: 0.6890\n",
            "Epoch 00034: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8275 - accuracy: 0.6889 - val_loss: 1.2202 - val_accuracy: 0.5826\n",
            "Epoch 35/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.8151 - accuracy: 0.6933\n",
            "Epoch 00035: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8150 - accuracy: 0.6933 - val_loss: 1.1850 - val_accuracy: 0.5862\n",
            "Epoch 36/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.8006 - accuracy: 0.7018\n",
            "Epoch 00036: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.8006 - accuracy: 0.7018 - val_loss: 1.2418 - val_accuracy: 0.5762\n",
            "Epoch 37/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7932 - accuracy: 0.7004\n",
            "Epoch 00037: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7930 - accuracy: 0.7005 - val_loss: 1.1848 - val_accuracy: 0.5918\n",
            "Epoch 38/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.7761 - accuracy: 0.7103\n",
            "Epoch 00038: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7782 - accuracy: 0.7096 - val_loss: 1.2312 - val_accuracy: 0.5834\n",
            "Epoch 39/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7667 - accuracy: 0.7125\n",
            "Epoch 00039: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7667 - accuracy: 0.7125 - val_loss: 1.1861 - val_accuracy: 0.5929\n",
            "Epoch 40/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7596 - accuracy: 0.7181\n",
            "Epoch 00040: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7597 - accuracy: 0.7181 - val_loss: 1.2112 - val_accuracy: 0.5963\n",
            "Epoch 41/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7379 - accuracy: 0.7260\n",
            "Epoch 00041: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7378 - accuracy: 0.7260 - val_loss: 1.2688 - val_accuracy: 0.5815\n",
            "Epoch 42/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7338 - accuracy: 0.7278\n",
            "Epoch 00042: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7338 - accuracy: 0.7278 - val_loss: 1.2229 - val_accuracy: 0.5977\n",
            "Epoch 43/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7111 - accuracy: 0.7364\n",
            "Epoch 00043: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7121 - accuracy: 0.7362 - val_loss: 1.2723 - val_accuracy: 0.5765\n",
            "Epoch 44/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7089 - accuracy: 0.7337\n",
            "Epoch 00044: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7089 - accuracy: 0.7337 - val_loss: 1.2358 - val_accuracy: 0.5965\n",
            "Epoch 45/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7079 - accuracy: 0.7374\n",
            "Epoch 00045: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.7079 - accuracy: 0.7374 - val_loss: 1.2067 - val_accuracy: 0.5940\n",
            "Epoch 46/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6941 - accuracy: 0.7427\n",
            "Epoch 00046: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6944 - accuracy: 0.7426 - val_loss: 1.2157 - val_accuracy: 0.6030\n",
            "Epoch 47/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6837 - accuracy: 0.7497\n",
            "Epoch 00047: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6842 - accuracy: 0.7495 - val_loss: 1.3820 - val_accuracy: 0.5623\n",
            "Epoch 48/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6799 - accuracy: 0.7469\n",
            "Epoch 00048: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6788 - accuracy: 0.7474 - val_loss: 1.2194 - val_accuracy: 0.6010\n",
            "Epoch 49/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6635 - accuracy: 0.7539\n",
            "Epoch 00049: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6639 - accuracy: 0.7539 - val_loss: 1.2483 - val_accuracy: 0.6007\n",
            "Epoch 50/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6539 - accuracy: 0.7561\n",
            "Epoch 00050: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6541 - accuracy: 0.7560 - val_loss: 1.3450 - val_accuracy: 0.5662\n",
            "Epoch 51/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6544 - accuracy: 0.7587\n",
            "Epoch 00051: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6544 - accuracy: 0.7587 - val_loss: 1.2973 - val_accuracy: 0.5868\n",
            "Epoch 52/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6479 - accuracy: 0.7592\n",
            "Epoch 00052: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6479 - accuracy: 0.7591 - val_loss: 1.2429 - val_accuracy: 0.6016\n",
            "Epoch 53/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6256 - accuracy: 0.7695\n",
            "Epoch 00053: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6251 - accuracy: 0.7698 - val_loss: 1.2961 - val_accuracy: 0.5935\n",
            "Epoch 54/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.6258 - accuracy: 0.7704\n",
            "Epoch 00054: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6253 - accuracy: 0.7705 - val_loss: 1.2358 - val_accuracy: 0.6077\n",
            "Epoch 55/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6195 - accuracy: 0.7709\n",
            "Epoch 00055: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6195 - accuracy: 0.7709 - val_loss: 1.2872 - val_accuracy: 0.6013\n",
            "Epoch 56/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6250 - accuracy: 0.7704\n",
            "Epoch 00056: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6250 - accuracy: 0.7704 - val_loss: 1.2726 - val_accuracy: 0.5904\n",
            "Epoch 57/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6152 - accuracy: 0.7757\n",
            "Epoch 00057: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6150 - accuracy: 0.7759 - val_loss: 1.2567 - val_accuracy: 0.6024\n",
            "Epoch 58/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6008 - accuracy: 0.7763\n",
            "Epoch 00058: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6007 - accuracy: 0.7762 - val_loss: 1.3040 - val_accuracy: 0.5965\n",
            "Epoch 59/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6028 - accuracy: 0.7754\n",
            "Epoch 00059: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.6027 - accuracy: 0.7754 - val_loss: 1.2891 - val_accuracy: 0.5996\n",
            "Epoch 60/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5878 - accuracy: 0.7846\n",
            "Epoch 00060: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5878 - accuracy: 0.7846 - val_loss: 1.2781 - val_accuracy: 0.6043\n",
            "Epoch 61/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5832 - accuracy: 0.7867\n",
            "Epoch 00061: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5832 - accuracy: 0.7867 - val_loss: 1.2814 - val_accuracy: 0.6080\n",
            "Epoch 62/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5724 - accuracy: 0.7891\n",
            "Epoch 00062: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5722 - accuracy: 0.7892 - val_loss: 1.5154 - val_accuracy: 0.5684\n",
            "Epoch 63/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5750 - accuracy: 0.7874\n",
            "Epoch 00063: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5750 - accuracy: 0.7874 - val_loss: 1.4059 - val_accuracy: 0.5857\n",
            "Epoch 64/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5787 - accuracy: 0.7884\n",
            "Epoch 00064: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5787 - accuracy: 0.7882 - val_loss: 1.4079 - val_accuracy: 0.5823\n",
            "Epoch 65/100\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.5592 - accuracy: 0.7957\n",
            "Epoch 00065: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5592 - accuracy: 0.7959 - val_loss: 1.2879 - val_accuracy: 0.5988\n",
            "Epoch 66/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5589 - accuracy: 0.7932\n",
            "Epoch 00066: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5589 - accuracy: 0.7932 - val_loss: 1.2782 - val_accuracy: 0.6046\n",
            "Epoch 67/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5477 - accuracy: 0.7985\n",
            "Epoch 00067: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5477 - accuracy: 0.7985 - val_loss: 1.3173 - val_accuracy: 0.6043\n",
            "Epoch 68/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5432 - accuracy: 0.8018\n",
            "Epoch 00068: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5432 - accuracy: 0.8018 - val_loss: 1.3329 - val_accuracy: 0.5999\n",
            "Epoch 69/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5455 - accuracy: 0.8001\n",
            "Epoch 00069: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5458 - accuracy: 0.8000 - val_loss: 1.3436 - val_accuracy: 0.5606\n",
            "Epoch 70/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5434 - accuracy: 0.8012\n",
            "Epoch 00070: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5436 - accuracy: 0.8011 - val_loss: 1.3452 - val_accuracy: 0.6032\n",
            "Epoch 71/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5317 - accuracy: 0.8069\n",
            "Epoch 00071: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5318 - accuracy: 0.8070 - val_loss: 1.3441 - val_accuracy: 0.6071\n",
            "Epoch 72/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5298 - accuracy: 0.8083\n",
            "Epoch 00072: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5300 - accuracy: 0.8083 - val_loss: 1.3355 - val_accuracy: 0.6105\n",
            "Epoch 73/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5263 - accuracy: 0.8050\n",
            "Epoch 00073: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5262 - accuracy: 0.8051 - val_loss: 1.3533 - val_accuracy: 0.5946\n",
            "Epoch 74/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5223 - accuracy: 0.8086\n",
            "Epoch 00074: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5227 - accuracy: 0.8085 - val_loss: 1.3261 - val_accuracy: 0.6024\n",
            "Epoch 75/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5208 - accuracy: 0.8084\n",
            "Epoch 00075: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5210 - accuracy: 0.8083 - val_loss: 1.3279 - val_accuracy: 0.6085\n",
            "Epoch 76/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5107 - accuracy: 0.8146\n",
            "Epoch 00076: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5108 - accuracy: 0.8146 - val_loss: 1.5017 - val_accuracy: 0.5631\n",
            "Epoch 77/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5148 - accuracy: 0.8085\n",
            "Epoch 00077: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5146 - accuracy: 0.8085 - val_loss: 1.3625 - val_accuracy: 0.5982\n",
            "Epoch 78/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5035 - accuracy: 0.8153\n",
            "Epoch 00078: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5035 - accuracy: 0.8153 - val_loss: 1.3932 - val_accuracy: 0.6094\n",
            "Epoch 79/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4967 - accuracy: 0.8212\n",
            "Epoch 00079: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4971 - accuracy: 0.8210 - val_loss: 1.3910 - val_accuracy: 0.5926\n",
            "Epoch 80/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5026 - accuracy: 0.8176\n",
            "Epoch 00080: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5020 - accuracy: 0.8178 - val_loss: 1.3569 - val_accuracy: 0.6099\n",
            "Epoch 81/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5000 - accuracy: 0.8182\n",
            "Epoch 00081: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.5000 - accuracy: 0.8182 - val_loss: 1.4165 - val_accuracy: 0.5843\n",
            "Epoch 82/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4918 - accuracy: 0.8193\n",
            "Epoch 00082: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4918 - accuracy: 0.8194 - val_loss: 1.3934 - val_accuracy: 0.6066\n",
            "Epoch 83/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4874 - accuracy: 0.8213\n",
            "Epoch 00083: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4876 - accuracy: 0.8213 - val_loss: 1.4287 - val_accuracy: 0.5996\n",
            "Epoch 84/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4776 - accuracy: 0.8263\n",
            "Epoch 00084: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4779 - accuracy: 0.8262 - val_loss: 1.4237 - val_accuracy: 0.5954\n",
            "Epoch 85/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4842 - accuracy: 0.8231\n",
            "Epoch 00085: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4844 - accuracy: 0.8231 - val_loss: 1.3623 - val_accuracy: 0.6135\n",
            "Epoch 86/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4741 - accuracy: 0.8241\n",
            "Epoch 00086: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.4738 - accuracy: 0.8242 - val_loss: 1.3661 - val_accuracy: 0.6091\n",
            "Epoch 87/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4762 - accuracy: 0.8235\n",
            "Epoch 00087: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.4762 - accuracy: 0.8235 - val_loss: 1.6277 - val_accuracy: 0.5386\n",
            "Epoch 88/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4635 - accuracy: 0.8291\n",
            "Epoch 00088: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.4637 - accuracy: 0.8291 - val_loss: 1.4316 - val_accuracy: 0.6043\n",
            "Epoch 89/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4697 - accuracy: 0.8294\n",
            "Epoch 00089: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4701 - accuracy: 0.8295 - val_loss: 1.3953 - val_accuracy: 0.6060\n",
            "Epoch 90/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4702 - accuracy: 0.8258\n",
            "Epoch 00090: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4702 - accuracy: 0.8258 - val_loss: 1.3705 - val_accuracy: 0.6071\n",
            "Epoch 91/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4583 - accuracy: 0.8324\n",
            "Epoch 00091: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4582 - accuracy: 0.8326 - val_loss: 1.3391 - val_accuracy: 0.6102\n",
            "Epoch 92/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4606 - accuracy: 0.8330\n",
            "Epoch 00092: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4602 - accuracy: 0.8331 - val_loss: 1.3751 - val_accuracy: 0.6099\n",
            "Epoch 93/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4578 - accuracy: 0.8333\n",
            "Epoch 00093: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4578 - accuracy: 0.8333 - val_loss: 1.3726 - val_accuracy: 0.6052\n",
            "Epoch 94/100\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.4553 - accuracy: 0.8337\n",
            "Epoch 00094: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4549 - accuracy: 0.8339 - val_loss: 1.3598 - val_accuracy: 0.6074\n",
            "Epoch 95/100\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4426 - accuracy: 0.8409\n",
            "Epoch 00095: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4423 - accuracy: 0.8409 - val_loss: 1.3714 - val_accuracy: 0.6172\n",
            "Epoch 96/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4602 - accuracy: 0.8353\n",
            "Epoch 00096: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4603 - accuracy: 0.8353 - val_loss: 1.3582 - val_accuracy: 0.6144\n",
            "Epoch 97/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4472 - accuracy: 0.8370\n",
            "Epoch 00097: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4473 - accuracy: 0.8370 - val_loss: 1.3582 - val_accuracy: 0.6091\n",
            "Epoch 98/100\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.4551 - accuracy: 0.8330\n",
            "Epoch 00098: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4551 - accuracy: 0.8330 - val_loss: 1.4139 - val_accuracy: 0.5896\n",
            "Epoch 99/100\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4394 - accuracy: 0.8397\n",
            "Epoch 00099: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4394 - accuracy: 0.8397 - val_loss: 1.5282 - val_accuracy: 0.5712\n",
            "Epoch 100/100\n",
            "221/225 [============================>.] - ETA: 0s - loss: 0.4424 - accuracy: 0.8392\n",
            "Epoch 00100: val_loss did not improve from 1.16153\n",
            "225/225 [==============================] - 3s 15ms/step - loss: 0.4417 - accuracy: 0.8396 - val_loss: 1.3710 - val_accuracy: 0.6077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvWax29Jn03L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7ce39df-2b7a-4f5a-b9c1-5499c4d4ad72"
      },
      "source": [
        "loaded = load_model(\"dropoutf.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113/113 [==============================] - 0s 4ms/step - loss: 1.0920 - accuracy: 0.6063\n",
            "0.6062970161437988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qle7DkoeZUmn"
      },
      "source": [
        "## Test kernel number and neuron number\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pbfLI4_j2Vs"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
        "from keras import layers\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.35))\n",
        "model.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.35))\n",
        "model.add(Conv2D(filters=256,kernel_size=2,padding='same',activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(Dropout(0.35))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.35))\n",
        "model.add(Dense(512,activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.35))\n",
        "model.add(Dense(7,activation='softmax'))\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clwQRjONj2Vw"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "import keras\n",
        "opt = Adam(lr=0.001)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-3IgfEfj2V0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62840813-8d3a-4db0-9eb2-ef22a636f825"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "checkpointer_double_layer = ModelCheckpoint(filepath='double_layer.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_double_layer = model.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(v_inputs , v_targets),\n",
        "          epochs=epoch, batch_size=128, callbacks=[checkpointer_double_layer], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 2.0899 - accuracy: 0.2649\n",
            "Epoch 00001: val_loss improved from inf to 4.01029, saving model to double_layer.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 2.0879 - accuracy: 0.2652 - val_loss: 4.0103 - val_accuracy: 0.2123\n",
            "Epoch 2/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.6636 - accuracy: 0.3715\n",
            "Epoch 00002: val_loss improved from 4.01029 to 2.25132, saving model to double_layer.hdf5\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 1.6641 - accuracy: 0.3713 - val_loss: 2.2513 - val_accuracy: 0.2678\n",
            "Epoch 3/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.4886 - accuracy: 0.4265\n",
            "Epoch 00003: val_loss improved from 2.25132 to 1.49608, saving model to double_layer.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.4889 - accuracy: 0.4262 - val_loss: 1.4961 - val_accuracy: 0.4054\n",
            "Epoch 4/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.3906 - accuracy: 0.4670\n",
            "Epoch 00004: val_loss improved from 1.49608 to 1.38930, saving model to double_layer.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.3911 - accuracy: 0.4669 - val_loss: 1.3893 - val_accuracy: 0.4525\n",
            "Epoch 5/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.3290 - accuracy: 0.4947\n",
            "Epoch 00005: val_loss improved from 1.38930 to 1.35895, saving model to double_layer.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.3288 - accuracy: 0.4944 - val_loss: 1.3590 - val_accuracy: 0.4868\n",
            "Epoch 6/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.2805 - accuracy: 0.5114\n",
            "Epoch 00006: val_loss improved from 1.35895 to 1.28117, saving model to double_layer.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.2803 - accuracy: 0.5113 - val_loss: 1.2812 - val_accuracy: 0.5065\n",
            "Epoch 7/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.2401 - accuracy: 0.5259\n",
            "Epoch 00007: val_loss improved from 1.28117 to 1.26728, saving model to double_layer.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.2399 - accuracy: 0.5261 - val_loss: 1.2673 - val_accuracy: 0.5174\n",
            "Epoch 8/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1994 - accuracy: 0.5422\n",
            "Epoch 00008: val_loss improved from 1.26728 to 1.18621, saving model to double_layer.hdf5\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 1.1995 - accuracy: 0.5420 - val_loss: 1.1862 - val_accuracy: 0.5464\n",
            "Epoch 9/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1729 - accuracy: 0.5531\n",
            "Epoch 00009: val_loss did not improve from 1.18621\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 1.1730 - accuracy: 0.5531 - val_loss: 1.2115 - val_accuracy: 0.5341\n",
            "Epoch 10/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1409 - accuracy: 0.5665\n",
            "Epoch 00010: val_loss did not improve from 1.18621\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 1.1406 - accuracy: 0.5665 - val_loss: 1.3259 - val_accuracy: 0.5135\n",
            "Epoch 11/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1132 - accuracy: 0.5783\n",
            "Epoch 00011: val_loss did not improve from 1.18621\n",
            "225/225 [==============================] - 6s 29ms/step - loss: 1.1140 - accuracy: 0.5781 - val_loss: 1.9310 - val_accuracy: 0.4338\n",
            "Epoch 12/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0796 - accuracy: 0.5912\n",
            "Epoch 00012: val_loss did not improve from 1.18621\n",
            "225/225 [==============================] - 6s 29ms/step - loss: 1.0791 - accuracy: 0.5915 - val_loss: 1.2089 - val_accuracy: 0.5472\n",
            "Epoch 13/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0460 - accuracy: 0.6044\n",
            "Epoch 00013: val_loss did not improve from 1.18621\n",
            "225/225 [==============================] - 6s 29ms/step - loss: 1.0453 - accuracy: 0.6048 - val_loss: 1.2834 - val_accuracy: 0.5531\n",
            "Epoch 14/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0173 - accuracy: 0.6169\n",
            "Epoch 00014: val_loss did not improve from 1.18621\n",
            "225/225 [==============================] - 6s 29ms/step - loss: 1.0171 - accuracy: 0.6169 - val_loss: 1.3478 - val_accuracy: 0.5400\n",
            "Epoch 15/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9960 - accuracy: 0.6268\n",
            "Epoch 00015: val_loss improved from 1.18621 to 1.14055, saving model to double_layer.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.9962 - accuracy: 0.6268 - val_loss: 1.1405 - val_accuracy: 0.5709\n",
            "Epoch 16/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9587 - accuracy: 0.6425\n",
            "Epoch 00016: val_loss did not improve from 1.14055\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.9586 - accuracy: 0.6424 - val_loss: 1.2032 - val_accuracy: 0.5812\n",
            "Epoch 17/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9277 - accuracy: 0.6494\n",
            "Epoch 00017: val_loss did not improve from 1.14055\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.9270 - accuracy: 0.6495 - val_loss: 1.4115 - val_accuracy: 0.5202\n",
            "Epoch 18/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9072 - accuracy: 0.6608\n",
            "Epoch 00018: val_loss did not improve from 1.14055\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.9073 - accuracy: 0.6608 - val_loss: 1.1528 - val_accuracy: 0.5885\n",
            "Epoch 19/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8677 - accuracy: 0.6726\n",
            "Epoch 00019: val_loss did not improve from 1.14055\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.8682 - accuracy: 0.6724 - val_loss: 1.3421 - val_accuracy: 0.5347\n",
            "Epoch 20/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8426 - accuracy: 0.6844\n",
            "Epoch 00020: val_loss did not improve from 1.14055\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.8427 - accuracy: 0.6843 - val_loss: 1.2852 - val_accuracy: 0.5603\n",
            "Epoch 21/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8186 - accuracy: 0.6943\n",
            "Epoch 00021: val_loss improved from 1.14055 to 1.13769, saving model to double_layer.hdf5\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.8187 - accuracy: 0.6942 - val_loss: 1.1377 - val_accuracy: 0.5999\n",
            "Epoch 22/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7901 - accuracy: 0.7065\n",
            "Epoch 00022: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.7902 - accuracy: 0.7061 - val_loss: 1.1454 - val_accuracy: 0.6046\n",
            "Epoch 23/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7532 - accuracy: 0.7180\n",
            "Epoch 00023: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 6s 29ms/step - loss: 0.7546 - accuracy: 0.7176 - val_loss: 1.1606 - val_accuracy: 0.5949\n",
            "Epoch 24/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7259 - accuracy: 0.7288\n",
            "Epoch 00024: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.7259 - accuracy: 0.7289 - val_loss: 1.1835 - val_accuracy: 0.6004\n",
            "Epoch 25/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7082 - accuracy: 0.7376\n",
            "Epoch 00025: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 6s 29ms/step - loss: 0.7086 - accuracy: 0.7374 - val_loss: 1.1756 - val_accuracy: 0.6032\n",
            "Epoch 26/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6821 - accuracy: 0.7455\n",
            "Epoch 00026: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.6822 - accuracy: 0.7453 - val_loss: 1.2155 - val_accuracy: 0.6096\n",
            "Epoch 27/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6584 - accuracy: 0.7588\n",
            "Epoch 00027: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.6587 - accuracy: 0.7588 - val_loss: 1.2129 - val_accuracy: 0.5979\n",
            "Epoch 28/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6345 - accuracy: 0.7651\n",
            "Epoch 00028: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.6349 - accuracy: 0.7651 - val_loss: 1.1989 - val_accuracy: 0.6038\n",
            "Epoch 29/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6159 - accuracy: 0.7739\n",
            "Epoch 00029: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.6166 - accuracy: 0.7736 - val_loss: 1.2381 - val_accuracy: 0.6066\n",
            "Epoch 30/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5993 - accuracy: 0.7802\n",
            "Epoch 00030: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5992 - accuracy: 0.7801 - val_loss: 1.2100 - val_accuracy: 0.6169\n",
            "Epoch 31/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5765 - accuracy: 0.7909\n",
            "Epoch 00031: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5767 - accuracy: 0.7909 - val_loss: 1.2436 - val_accuracy: 0.6091\n",
            "Epoch 32/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5700 - accuracy: 0.7892\n",
            "Epoch 00032: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5700 - accuracy: 0.7891 - val_loss: 1.3257 - val_accuracy: 0.5848\n",
            "Epoch 33/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5468 - accuracy: 0.7979\n",
            "Epoch 00033: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5477 - accuracy: 0.7974 - val_loss: 1.4483 - val_accuracy: 0.5848\n",
            "Epoch 34/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5262 - accuracy: 0.8056\n",
            "Epoch 00034: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5258 - accuracy: 0.8057 - val_loss: 1.4432 - val_accuracy: 0.5717\n",
            "Epoch 35/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5133 - accuracy: 0.8151\n",
            "Epoch 00035: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5140 - accuracy: 0.8148 - val_loss: 1.2935 - val_accuracy: 0.6088\n",
            "Epoch 36/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4888 - accuracy: 0.8218\n",
            "Epoch 00036: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 6s 29ms/step - loss: 0.4888 - accuracy: 0.8219 - val_loss: 1.3153 - val_accuracy: 0.6119\n",
            "Epoch 37/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4909 - accuracy: 0.8203\n",
            "Epoch 00037: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4912 - accuracy: 0.8202 - val_loss: 1.3094 - val_accuracy: 0.6094\n",
            "Epoch 38/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4647 - accuracy: 0.8279\n",
            "Epoch 00038: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4648 - accuracy: 0.8279 - val_loss: 1.3244 - val_accuracy: 0.5985\n",
            "Epoch 39/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4581 - accuracy: 0.8308\n",
            "Epoch 00039: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4592 - accuracy: 0.8303 - val_loss: 1.3853 - val_accuracy: 0.6035\n",
            "Epoch 40/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4487 - accuracy: 0.8347\n",
            "Epoch 00040: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4485 - accuracy: 0.8348 - val_loss: 1.3549 - val_accuracy: 0.6163\n",
            "Epoch 41/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4357 - accuracy: 0.8405\n",
            "Epoch 00041: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4359 - accuracy: 0.8404 - val_loss: 1.4044 - val_accuracy: 0.6046\n",
            "Epoch 42/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4199 - accuracy: 0.8482\n",
            "Epoch 00042: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4198 - accuracy: 0.8482 - val_loss: 1.3890 - val_accuracy: 0.6152\n",
            "Epoch 43/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4229 - accuracy: 0.8463\n",
            "Epoch 00043: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4235 - accuracy: 0.8461 - val_loss: 1.3508 - val_accuracy: 0.6199\n",
            "Epoch 44/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4095 - accuracy: 0.8531\n",
            "Epoch 00044: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 6s 29ms/step - loss: 0.4097 - accuracy: 0.8531 - val_loss: 1.3918 - val_accuracy: 0.6155\n",
            "Epoch 45/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4009 - accuracy: 0.8542\n",
            "Epoch 00045: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4005 - accuracy: 0.8544 - val_loss: 1.4093 - val_accuracy: 0.6130\n",
            "Epoch 46/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3958 - accuracy: 0.8557\n",
            "Epoch 00046: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.3958 - accuracy: 0.8558 - val_loss: 1.4316 - val_accuracy: 0.6127\n",
            "Epoch 47/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3817 - accuracy: 0.8630\n",
            "Epoch 00047: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.3823 - accuracy: 0.8627 - val_loss: 1.4230 - val_accuracy: 0.6244\n",
            "Epoch 48/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3777 - accuracy: 0.8639\n",
            "Epoch 00048: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.3778 - accuracy: 0.8638 - val_loss: 2.0126 - val_accuracy: 0.5024\n",
            "Epoch 49/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3623 - accuracy: 0.8691\n",
            "Epoch 00049: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.3628 - accuracy: 0.8690 - val_loss: 2.6106 - val_accuracy: 0.4257\n",
            "Epoch 50/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3606 - accuracy: 0.8704\n",
            "Epoch 00050: val_loss did not improve from 1.13769\n",
            "225/225 [==============================] - 6s 29ms/step - loss: 0.3609 - accuracy: 0.8702 - val_loss: 1.4326 - val_accuracy: 0.6202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3QVV_yzqSX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f19b43-7c03-4778-c826-7dfabc5d6835"
      },
      "source": [
        "loaded = load_model(\"double_layer.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113/113 [==============================] - 1s 5ms/step - loss: 1.0919 - accuracy: 0.6052\n",
            "0.6051825284957886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WC3oWD_oqUF_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUW6Bg5gj2V2"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
        "from keras import layers\n",
        "\n",
        "model_m1 = Sequential()\n",
        "model_m1.add(Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "model_m1.add(BatchNormalization())\n",
        "model_m1.add(MaxPooling2D(pool_size=2))\n",
        "model_m1.add(Dropout(0.35))\n",
        "model_m1.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
        "model_m1.add(BatchNormalization())\n",
        "model_m1.add(MaxPooling2D(pool_size=2))\n",
        "model_m1.add(Dropout(0.35))\n",
        "model_m1.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
        "model_m1.add(BatchNormalization())\n",
        "model_m1.add(MaxPooling2D(pool_size=2))\n",
        "model_m1.add(Dropout(0.35))\n",
        "model_m1.add(Flatten())\n",
        "model_m1.add(Dense(256,activation='relu'))\n",
        "model_m1.add(BatchNormalization())\n",
        "model_m1.add(Dropout(0.35))\n",
        "model_m1.add(Dense(512,activation='relu'))\n",
        "model_m1.add(BatchNormalization())\n",
        "model_m1.add(Dropout(0.35))\n",
        "model_m1.add(Dense(7,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u01rlT4Py3Bl"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "import keras\n",
        "opt = Adam(lr=0.001)\n",
        "\n",
        "model_m1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Egi0RWsHy3Hw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a41dffa-6825-4077-fa3e-27c0fdaae1e5"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "checkpointer_m1 = ModelCheckpoint(filepath='m1.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_m1 = model_m1.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(v_inputs , v_targets),\n",
        "          epochs=epoch, batch_size=128, callbacks=[checkpointer_m1], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 2.0991 - accuracy: 0.2659\n",
            "Epoch 00001: val_loss improved from inf to 3.21620, saving model to m1.hdf5\n",
            "225/225 [==============================] - 4s 17ms/step - loss: 2.0977 - accuracy: 0.2662 - val_loss: 3.2162 - val_accuracy: 0.1530\n",
            "Epoch 2/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.6620 - accuracy: 0.3695\n",
            "Epoch 00002: val_loss improved from 3.21620 to 1.79680, saving model to m1.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.6618 - accuracy: 0.3695 - val_loss: 1.7968 - val_accuracy: 0.3123\n",
            "Epoch 3/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.4949 - accuracy: 0.4248\n",
            "Epoch 00003: val_loss improved from 1.79680 to 1.59441, saving model to m1.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.4949 - accuracy: 0.4248 - val_loss: 1.5944 - val_accuracy: 0.3898\n",
            "Epoch 4/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.3962 - accuracy: 0.4604\n",
            "Epoch 00004: val_loss improved from 1.59441 to 1.36822, saving model to m1.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.3962 - accuracy: 0.4603 - val_loss: 1.3682 - val_accuracy: 0.4804\n",
            "Epoch 5/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.3342 - accuracy: 0.4874\n",
            "Epoch 00005: val_loss improved from 1.36822 to 1.36730, saving model to m1.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.3335 - accuracy: 0.4877 - val_loss: 1.3673 - val_accuracy: 0.4859\n",
            "Epoch 6/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.2872 - accuracy: 0.5031\n",
            "Epoch 00006: val_loss improved from 1.36730 to 1.32096, saving model to m1.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.2869 - accuracy: 0.5032 - val_loss: 1.3210 - val_accuracy: 0.5032\n",
            "Epoch 7/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.2520 - accuracy: 0.5178\n",
            "Epoch 00007: val_loss improved from 1.32096 to 1.21866, saving model to m1.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.2520 - accuracy: 0.5179 - val_loss: 1.2187 - val_accuracy: 0.5333\n",
            "Epoch 8/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.2150 - accuracy: 0.5363\n",
            "Epoch 00008: val_loss did not improve from 1.21866\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.2150 - accuracy: 0.5363 - val_loss: 1.4018 - val_accuracy: 0.4845\n",
            "Epoch 9/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.1826 - accuracy: 0.5508\n",
            "Epoch 00009: val_loss improved from 1.21866 to 1.17413, saving model to m1.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.1828 - accuracy: 0.5508 - val_loss: 1.1741 - val_accuracy: 0.5620\n",
            "Epoch 10/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 1.1537 - accuracy: 0.5607\n",
            "Epoch 00010: val_loss did not improve from 1.17413\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.1530 - accuracy: 0.5609 - val_loss: 1.2707 - val_accuracy: 0.5196\n",
            "Epoch 11/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1321 - accuracy: 0.5711\n",
            "Epoch 00011: val_loss did not improve from 1.17413\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.1321 - accuracy: 0.5711 - val_loss: 1.2147 - val_accuracy: 0.5450\n",
            "Epoch 12/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.0960 - accuracy: 0.5815\n",
            "Epoch 00012: val_loss did not improve from 1.17413\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.0962 - accuracy: 0.5815 - val_loss: 1.2026 - val_accuracy: 0.5522\n",
            "Epoch 13/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0783 - accuracy: 0.5915\n",
            "Epoch 00013: val_loss improved from 1.17413 to 1.12709, saving model to m1.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.0783 - accuracy: 0.5917 - val_loss: 1.1271 - val_accuracy: 0.5779\n",
            "Epoch 14/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0536 - accuracy: 0.6000\n",
            "Epoch 00014: val_loss did not improve from 1.12709\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.0536 - accuracy: 0.6000 - val_loss: 1.1328 - val_accuracy: 0.5751\n",
            "Epoch 15/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0242 - accuracy: 0.6127\n",
            "Epoch 00015: val_loss improved from 1.12709 to 1.11325, saving model to m1.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.0247 - accuracy: 0.6126 - val_loss: 1.1133 - val_accuracy: 0.5885\n",
            "Epoch 16/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 1.0103 - accuracy: 0.6173\n",
            "Epoch 00016: val_loss did not improve from 1.11325\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 1.0103 - accuracy: 0.6173 - val_loss: 1.1305 - val_accuracy: 0.5787\n",
            "Epoch 17/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9921 - accuracy: 0.6264\n",
            "Epoch 00017: val_loss did not improve from 1.11325\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.9922 - accuracy: 0.6265 - val_loss: 1.2254 - val_accuracy: 0.5486\n",
            "Epoch 18/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9600 - accuracy: 0.6393\n",
            "Epoch 00018: val_loss did not improve from 1.11325\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.9596 - accuracy: 0.6394 - val_loss: 1.1471 - val_accuracy: 0.5854\n",
            "Epoch 19/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9433 - accuracy: 0.6431\n",
            "Epoch 00019: val_loss improved from 1.11325 to 1.09756, saving model to m1.hdf5\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.9433 - accuracy: 0.6431 - val_loss: 1.0976 - val_accuracy: 0.6055\n",
            "Epoch 20/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.9255 - accuracy: 0.6529\n",
            "Epoch 00020: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.9255 - accuracy: 0.6529 - val_loss: 1.1839 - val_accuracy: 0.5709\n",
            "Epoch 21/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.8953 - accuracy: 0.6632\n",
            "Epoch 00021: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.8961 - accuracy: 0.6628 - val_loss: 1.2111 - val_accuracy: 0.5628\n",
            "Epoch 22/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.8733 - accuracy: 0.6734\n",
            "Epoch 00022: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.8744 - accuracy: 0.6730 - val_loss: 1.1073 - val_accuracy: 0.6060\n",
            "Epoch 23/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.8491 - accuracy: 0.6797\n",
            "Epoch 00023: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.8491 - accuracy: 0.6797 - val_loss: 1.1264 - val_accuracy: 0.5921\n",
            "Epoch 24/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8372 - accuracy: 0.6862\n",
            "Epoch 00024: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.8370 - accuracy: 0.6863 - val_loss: 1.1538 - val_accuracy: 0.5979\n",
            "Epoch 25/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.8155 - accuracy: 0.6944\n",
            "Epoch 00025: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.8153 - accuracy: 0.6946 - val_loss: 1.2797 - val_accuracy: 0.5486\n",
            "Epoch 26/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7994 - accuracy: 0.7021\n",
            "Epoch 00026: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.7992 - accuracy: 0.7021 - val_loss: 1.1020 - val_accuracy: 0.6147\n",
            "Epoch 27/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7929 - accuracy: 0.7036\n",
            "Epoch 00027: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.7929 - accuracy: 0.7036 - val_loss: 1.1143 - val_accuracy: 0.6096\n",
            "Epoch 28/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7692 - accuracy: 0.7101\n",
            "Epoch 00028: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.7694 - accuracy: 0.7100 - val_loss: 1.1725 - val_accuracy: 0.6007\n",
            "Epoch 29/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.7605 - accuracy: 0.7172\n",
            "Epoch 00029: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.7607 - accuracy: 0.7168 - val_loss: 1.1159 - val_accuracy: 0.6135\n",
            "Epoch 30/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7312 - accuracy: 0.7275\n",
            "Epoch 00030: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.7310 - accuracy: 0.7276 - val_loss: 1.1340 - val_accuracy: 0.6177\n",
            "Epoch 31/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7187 - accuracy: 0.7337\n",
            "Epoch 00031: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.7190 - accuracy: 0.7335 - val_loss: 1.1245 - val_accuracy: 0.6069\n",
            "Epoch 32/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.7064 - accuracy: 0.7365\n",
            "Epoch 00032: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.7064 - accuracy: 0.7365 - val_loss: 1.1243 - val_accuracy: 0.6180\n",
            "Epoch 33/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.6831 - accuracy: 0.7458\n",
            "Epoch 00033: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.6835 - accuracy: 0.7458 - val_loss: 1.2114 - val_accuracy: 0.6057\n",
            "Epoch 34/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.6763 - accuracy: 0.7483\n",
            "Epoch 00034: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.6771 - accuracy: 0.7477 - val_loss: 1.2104 - val_accuracy: 0.6027\n",
            "Epoch 35/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.6606 - accuracy: 0.7570\n",
            "Epoch 00035: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.6606 - accuracy: 0.7570 - val_loss: 1.1953 - val_accuracy: 0.6124\n",
            "Epoch 36/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6585 - accuracy: 0.7541\n",
            "Epoch 00036: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.6591 - accuracy: 0.7540 - val_loss: 1.1904 - val_accuracy: 0.6088\n",
            "Epoch 37/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6380 - accuracy: 0.7623\n",
            "Epoch 00037: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.6379 - accuracy: 0.7622 - val_loss: 1.2665 - val_accuracy: 0.5968\n",
            "Epoch 38/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6361 - accuracy: 0.7665\n",
            "Epoch 00038: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.6361 - accuracy: 0.7665 - val_loss: 1.2117 - val_accuracy: 0.6113\n",
            "Epoch 39/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6081 - accuracy: 0.7757\n",
            "Epoch 00039: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.6080 - accuracy: 0.7757 - val_loss: 1.3874 - val_accuracy: 0.5578\n",
            "Epoch 40/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.6021 - accuracy: 0.7764\n",
            "Epoch 00040: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.6027 - accuracy: 0.7763 - val_loss: 1.2200 - val_accuracy: 0.6144\n",
            "Epoch 41/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5877 - accuracy: 0.7835\n",
            "Epoch 00041: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.5877 - accuracy: 0.7835 - val_loss: 1.2383 - val_accuracy: 0.6082\n",
            "Epoch 42/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5817 - accuracy: 0.7883\n",
            "Epoch 00042: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.5817 - accuracy: 0.7883 - val_loss: 1.2438 - val_accuracy: 0.6088\n",
            "Epoch 43/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.5700 - accuracy: 0.7885\n",
            "Epoch 00043: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.5710 - accuracy: 0.7887 - val_loss: 1.2538 - val_accuracy: 0.6038\n",
            "Epoch 44/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5630 - accuracy: 0.7939\n",
            "Epoch 00044: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.5627 - accuracy: 0.7941 - val_loss: 1.2363 - val_accuracy: 0.6222\n",
            "Epoch 45/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5597 - accuracy: 0.7952\n",
            "Epoch 00045: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.5597 - accuracy: 0.7952 - val_loss: 1.2622 - val_accuracy: 0.6096\n",
            "Epoch 46/50\n",
            "222/225 [============================>.] - ETA: 0s - loss: 0.5411 - accuracy: 0.8034\n",
            "Epoch 00046: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.5423 - accuracy: 0.8031 - val_loss: 1.2472 - val_accuracy: 0.6105\n",
            "Epoch 47/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5398 - accuracy: 0.8022\n",
            "Epoch 00047: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.5398 - accuracy: 0.8022 - val_loss: 1.3094 - val_accuracy: 0.6021\n",
            "Epoch 48/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 0.5313 - accuracy: 0.8053\n",
            "Epoch 00048: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.5313 - accuracy: 0.8053 - val_loss: 1.2614 - val_accuracy: 0.6191\n",
            "Epoch 49/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5234 - accuracy: 0.8081\n",
            "Epoch 00049: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.5236 - accuracy: 0.8079 - val_loss: 1.4646 - val_accuracy: 0.5692\n",
            "Epoch 50/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5115 - accuracy: 0.8123\n",
            "Epoch 00050: val_loss did not improve from 1.09756\n",
            "225/225 [==============================] - 4s 16ms/step - loss: 0.5122 - accuracy: 0.8121 - val_loss: 1.2584 - val_accuracy: 0.6213\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX9dqbW65yhA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b96b9a5f-84a3-478c-a8b9-1cd96acfbd3c"
      },
      "source": [
        "loaded = load_model(\"m1.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113/113 [==============================] - 0s 4ms/step - loss: 1.0693 - accuracy: 0.6007\n",
            "0.600724458694458\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqR2C_Isy3Jj"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
        "from keras import layers\n",
        "\n",
        "model_m2 = Sequential()\n",
        "\n",
        "model_m2.add(Conv2D(filters=32,kernel_size=2,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "model_m2.add(BatchNormalization())\n",
        "model_m2.add(Conv2D(filters=32,kernel_size=2,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "model_m2.add(BatchNormalization())\n",
        "model_m2.add(MaxPooling2D(pool_size=2))\n",
        "model_m2.add(Dropout(0.35))\n",
        "model_m2.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'))\n",
        "model_m2.add(BatchNormalization())\n",
        "model_m2.add(Conv2D(filters=64,kernel_size=2,padding='same',activation='relu'))\n",
        "model_m2.add(BatchNormalization())\n",
        "model_m2.add(MaxPooling2D(pool_size=2))\n",
        "model_m2.add(Dropout(0.35))\n",
        "model_m2.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
        "model_m2.add(BatchNormalization())\n",
        "model_m2.add(Conv2D(filters=128,kernel_size=2,padding='same',activation='relu'))\n",
        "model_m2.add(BatchNormalization())\n",
        "model_m2.add(MaxPooling2D(pool_size=2))\n",
        "model_m2.add(Dropout(0.35))\n",
        "model_m2.add(Flatten())\n",
        "model_m2.add(Dense(256,activation='relu'))\n",
        "model_m2.add(BatchNormalization())\n",
        "model_m2.add(Dropout(0.35))\n",
        "model_m2.add(Dense(512,activation='relu'))\n",
        "model_m2.add(BatchNormalization())\n",
        "model_m2.add(Dropout(0.35))\n",
        "model_m2.add(Dense(7,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X1p4k-i5uRk"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "import keras\n",
        "opt = Adam(lr=0.001)\n",
        "\n",
        "model_m2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Bq80lLH5uUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01cf1e05-290d-47a0-8432-a52ff65b0d9e"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "checkpointer_m2 = ModelCheckpoint(filepath='m2.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_m2 = model_m2.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(v_inputs , v_targets),\n",
        "          epochs=epoch, batch_size=128, callbacks=[checkpointer_m2], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "  1/225 [..............................] - ETA: 4s - loss: 3.0819 - accuracy: 0.1172"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0155s vs `on_train_batch_end` time: 0.0236s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "225/225 [==============================] - ETA: 0s - loss: 2.0795 - accuracy: 0.2776\n",
            "Epoch 00001: val_loss improved from inf to 1.90817, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 2.0795 - accuracy: 0.2776 - val_loss: 1.9082 - val_accuracy: 0.1697\n",
            "Epoch 2/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.6499 - accuracy: 0.3732\n",
            "Epoch 00002: val_loss improved from 1.90817 to 1.79132, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.6487 - accuracy: 0.3739 - val_loss: 1.7913 - val_accuracy: 0.2875\n",
            "Epoch 3/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.4770 - accuracy: 0.4350\n",
            "Epoch 00003: val_loss improved from 1.79132 to 1.42647, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.4768 - accuracy: 0.4351 - val_loss: 1.4265 - val_accuracy: 0.4425\n",
            "Epoch 4/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.3771 - accuracy: 0.4704\n",
            "Epoch 00004: val_loss improved from 1.42647 to 1.30230, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.3765 - accuracy: 0.4707 - val_loss: 1.3023 - val_accuracy: 0.4999\n",
            "Epoch 5/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.3190 - accuracy: 0.4926\n",
            "Epoch 00005: val_loss improved from 1.30230 to 1.27836, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.3190 - accuracy: 0.4926 - val_loss: 1.2784 - val_accuracy: 0.5143\n",
            "Epoch 6/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.2665 - accuracy: 0.5178\n",
            "Epoch 00006: val_loss improved from 1.27836 to 1.19460, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.2662 - accuracy: 0.5180 - val_loss: 1.1946 - val_accuracy: 0.5372\n",
            "Epoch 7/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.2281 - accuracy: 0.5281\n",
            "Epoch 00007: val_loss did not improve from 1.19460\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.2285 - accuracy: 0.5280 - val_loss: 1.2036 - val_accuracy: 0.5447\n",
            "Epoch 8/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1922 - accuracy: 0.5445\n",
            "Epoch 00008: val_loss improved from 1.19460 to 1.18025, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.1922 - accuracy: 0.5443 - val_loss: 1.1802 - val_accuracy: 0.5567\n",
            "Epoch 9/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1606 - accuracy: 0.5578\n",
            "Epoch 00009: val_loss improved from 1.18025 to 1.15218, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.1604 - accuracy: 0.5580 - val_loss: 1.1522 - val_accuracy: 0.5570\n",
            "Epoch 10/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1366 - accuracy: 0.5664\n",
            "Epoch 00010: val_loss did not improve from 1.15218\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 1.1371 - accuracy: 0.5663 - val_loss: 1.1889 - val_accuracy: 0.5525\n",
            "Epoch 11/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1178 - accuracy: 0.5793\n",
            "Epoch 00011: val_loss improved from 1.15218 to 1.12175, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.1170 - accuracy: 0.5795 - val_loss: 1.1218 - val_accuracy: 0.5765\n",
            "Epoch 12/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0818 - accuracy: 0.5898\n",
            "Epoch 00012: val_loss improved from 1.12175 to 1.12055, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.0814 - accuracy: 0.5900 - val_loss: 1.1206 - val_accuracy: 0.5826\n",
            "Epoch 13/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0686 - accuracy: 0.5954\n",
            "Epoch 00013: val_loss improved from 1.12055 to 1.10063, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.0679 - accuracy: 0.5955 - val_loss: 1.1006 - val_accuracy: 0.5893\n",
            "Epoch 14/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0331 - accuracy: 0.6083\n",
            "Epoch 00014: val_loss improved from 1.10063 to 1.08932, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.0332 - accuracy: 0.6083 - val_loss: 1.0893 - val_accuracy: 0.5843\n",
            "Epoch 15/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0146 - accuracy: 0.6150\n",
            "Epoch 00015: val_loss improved from 1.08932 to 1.08336, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.0147 - accuracy: 0.6151 - val_loss: 1.0834 - val_accuracy: 0.5996\n",
            "Epoch 16/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9792 - accuracy: 0.6274\n",
            "Epoch 00016: val_loss did not improve from 1.08336\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.9794 - accuracy: 0.6271 - val_loss: 1.0882 - val_accuracy: 0.6007\n",
            "Epoch 17/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9711 - accuracy: 0.6347\n",
            "Epoch 00017: val_loss did not improve from 1.08336\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.9706 - accuracy: 0.6348 - val_loss: 1.1858 - val_accuracy: 0.5681\n",
            "Epoch 18/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9439 - accuracy: 0.6477\n",
            "Epoch 00018: val_loss improved from 1.08336 to 1.07245, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.9447 - accuracy: 0.6475 - val_loss: 1.0724 - val_accuracy: 0.6066\n",
            "Epoch 19/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9217 - accuracy: 0.6549\n",
            "Epoch 00019: val_loss did not improve from 1.07245\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.9226 - accuracy: 0.6546 - val_loss: 1.0728 - val_accuracy: 0.6043\n",
            "Epoch 20/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8948 - accuracy: 0.6606\n",
            "Epoch 00020: val_loss improved from 1.07245 to 1.04755, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.8950 - accuracy: 0.6605 - val_loss: 1.0476 - val_accuracy: 0.6133\n",
            "Epoch 21/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8812 - accuracy: 0.6682\n",
            "Epoch 00021: val_loss did not improve from 1.04755\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.8816 - accuracy: 0.6683 - val_loss: 1.0693 - val_accuracy: 0.6130\n",
            "Epoch 22/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8548 - accuracy: 0.6798\n",
            "Epoch 00022: val_loss did not improve from 1.04755\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.8554 - accuracy: 0.6797 - val_loss: 1.1134 - val_accuracy: 0.5974\n",
            "Epoch 23/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8381 - accuracy: 0.6853\n",
            "Epoch 00023: val_loss did not improve from 1.04755\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.8389 - accuracy: 0.6851 - val_loss: 1.0579 - val_accuracy: 0.6186\n",
            "Epoch 24/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8135 - accuracy: 0.6921\n",
            "Epoch 00024: val_loss improved from 1.04755 to 1.04689, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.8134 - accuracy: 0.6922 - val_loss: 1.0469 - val_accuracy: 0.6205\n",
            "Epoch 25/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7896 - accuracy: 0.7033\n",
            "Epoch 00025: val_loss improved from 1.04689 to 1.04050, saving model to m2.hdf5\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.7895 - accuracy: 0.7034 - val_loss: 1.0405 - val_accuracy: 0.6367\n",
            "Epoch 26/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7716 - accuracy: 0.7106\n",
            "Epoch 00026: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.7719 - accuracy: 0.7108 - val_loss: 1.0780 - val_accuracy: 0.6322\n",
            "Epoch 27/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7543 - accuracy: 0.7178\n",
            "Epoch 00027: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.7549 - accuracy: 0.7179 - val_loss: 1.0518 - val_accuracy: 0.6289\n",
            "Epoch 28/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7333 - accuracy: 0.7277\n",
            "Epoch 00028: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.7334 - accuracy: 0.7278 - val_loss: 1.0451 - val_accuracy: 0.6431\n",
            "Epoch 29/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7141 - accuracy: 0.7312\n",
            "Epoch 00029: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.7141 - accuracy: 0.7311 - val_loss: 1.0794 - val_accuracy: 0.6177\n",
            "Epoch 30/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6949 - accuracy: 0.7420\n",
            "Epoch 00030: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.6947 - accuracy: 0.7423 - val_loss: 1.0639 - val_accuracy: 0.6397\n",
            "Epoch 31/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6813 - accuracy: 0.7472\n",
            "Epoch 00031: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.6820 - accuracy: 0.7470 - val_loss: 1.0560 - val_accuracy: 0.6339\n",
            "Epoch 32/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6629 - accuracy: 0.7542\n",
            "Epoch 00032: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.6625 - accuracy: 0.7543 - val_loss: 1.0695 - val_accuracy: 0.6411\n",
            "Epoch 33/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6371 - accuracy: 0.7626\n",
            "Epoch 00033: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.6375 - accuracy: 0.7627 - val_loss: 1.1210 - val_accuracy: 0.6255\n",
            "Epoch 34/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6241 - accuracy: 0.7684\n",
            "Epoch 00034: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.6243 - accuracy: 0.7684 - val_loss: 1.1251 - val_accuracy: 0.6272\n",
            "Epoch 35/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6157 - accuracy: 0.7705\n",
            "Epoch 00035: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 6s 29ms/step - loss: 0.6156 - accuracy: 0.7707 - val_loss: 1.1020 - val_accuracy: 0.6289\n",
            "Epoch 36/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6002 - accuracy: 0.7759\n",
            "Epoch 00036: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.6010 - accuracy: 0.7758 - val_loss: 1.0892 - val_accuracy: 0.6383\n",
            "Epoch 37/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5823 - accuracy: 0.7843\n",
            "Epoch 00037: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5823 - accuracy: 0.7843 - val_loss: 1.1205 - val_accuracy: 0.6378\n",
            "Epoch 38/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5693 - accuracy: 0.7874\n",
            "Epoch 00038: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5700 - accuracy: 0.7872 - val_loss: 1.1179 - val_accuracy: 0.6442\n",
            "Epoch 39/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5589 - accuracy: 0.7944\n",
            "Epoch 00039: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5594 - accuracy: 0.7942 - val_loss: 1.1470 - val_accuracy: 0.6319\n",
            "Epoch 40/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5437 - accuracy: 0.7993\n",
            "Epoch 00040: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5442 - accuracy: 0.7992 - val_loss: 1.1288 - val_accuracy: 0.6330\n",
            "Epoch 41/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5370 - accuracy: 0.8002\n",
            "Epoch 00041: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5375 - accuracy: 0.8001 - val_loss: 1.1291 - val_accuracy: 0.6403\n",
            "Epoch 42/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5248 - accuracy: 0.8061\n",
            "Epoch 00042: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5255 - accuracy: 0.8058 - val_loss: 1.1911 - val_accuracy: 0.6378\n",
            "Epoch 43/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5197 - accuracy: 0.8113\n",
            "Epoch 00043: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5201 - accuracy: 0.8110 - val_loss: 1.1479 - val_accuracy: 0.6459\n",
            "Epoch 44/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4999 - accuracy: 0.8168\n",
            "Epoch 00044: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5002 - accuracy: 0.8167 - val_loss: 1.1695 - val_accuracy: 0.6375\n",
            "Epoch 45/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5014 - accuracy: 0.8174\n",
            "Epoch 00045: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.5018 - accuracy: 0.8173 - val_loss: 1.1779 - val_accuracy: 0.6439\n",
            "Epoch 46/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4893 - accuracy: 0.8193\n",
            "Epoch 00046: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4899 - accuracy: 0.8192 - val_loss: 1.2033 - val_accuracy: 0.6322\n",
            "Epoch 47/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4806 - accuracy: 0.8249\n",
            "Epoch 00047: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4811 - accuracy: 0.8247 - val_loss: 1.1991 - val_accuracy: 0.6328\n",
            "Epoch 48/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4672 - accuracy: 0.8276\n",
            "Epoch 00048: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4667 - accuracy: 0.8278 - val_loss: 1.1997 - val_accuracy: 0.6361\n",
            "Epoch 49/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4596 - accuracy: 0.8336\n",
            "Epoch 00049: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4595 - accuracy: 0.8336 - val_loss: 1.1978 - val_accuracy: 0.6459\n",
            "Epoch 50/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4575 - accuracy: 0.8334\n",
            "Epoch 00050: val_loss did not improve from 1.04050\n",
            "225/225 [==============================] - 7s 29ms/step - loss: 0.4574 - accuracy: 0.8335 - val_loss: 1.1931 - val_accuracy: 0.6408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wPKYCZd5yFG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8bd2e12-0484-4f85-c3fc-63cc5e100b03"
      },
      "source": [
        "loaded = load_model(\"m2.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113/113 [==============================] - 0s 4ms/step - loss: 1.0128 - accuracy: 0.6414\n",
            "0.6414042711257935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIIWKQUyUjLU"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
        "from keras import layers\n",
        "\n",
        "model_m3 = Sequential()\n",
        "\n",
        "model_m3.add(Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "model_m3.add(BatchNormalization())\n",
        "model_m3.add(Conv2D(filters=32,kernel_size=3,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "model_m3.add(BatchNormalization())\n",
        "model_m3.add(MaxPooling2D(pool_size=2))\n",
        "model_m3.add(Dropout(0.35))\n",
        "model_m3.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
        "model_m3.add(BatchNormalization())\n",
        "model_m3.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu'))\n",
        "model_m3.add(BatchNormalization())\n",
        "model_m3.add(MaxPooling2D(pool_size=2))\n",
        "model_m3.add(Dropout(0.35))\n",
        "model_m3.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
        "model_m3.add(BatchNormalization())\n",
        "model_m3.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
        "model_m3.add(BatchNormalization())\n",
        "model_m3.add(MaxPooling2D(pool_size=2))\n",
        "model_m3.add(Dropout(0.35))\n",
        "model_m3.add(Flatten())\n",
        "model_m3.add(Dense(256,activation='relu'))\n",
        "model_m3.add(BatchNormalization())\n",
        "model_m3.add(Dropout(0.35))\n",
        "model_m3.add(Dense(512,activation='relu'))\n",
        "model_m3.add(BatchNormalization())\n",
        "model_m3.add(Dropout(0.35))\n",
        "model_m3.add(Dense(7,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNhqgQFEUjaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5aa323d-7215-4d57-8387-2a8536ced7f1"
      },
      "source": [
        "model_m3.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model_m3.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_52\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_164 (Conv2D)          (None, 48, 48, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_88 (Batc (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_165 (Conv2D)          (None, 48, 48, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_89 (Batc (None, 48, 48, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_149 (MaxPoolin (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_203 (Dropout)        (None, 24, 24, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_166 (Conv2D)          (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_90 (Batc (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_167 (Conv2D)          (None, 24, 24, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_91 (Batc (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_150 (MaxPoolin (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_204 (Dropout)        (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_168 (Conv2D)          (None, 12, 12, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_92 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_169 (Conv2D)          (None, 12, 12, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_93 (Batc (None, 12, 12, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_151 (MaxPoolin (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_205 (Dropout)        (None, 6, 6, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_52 (Flatten)         (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_154 (Dense)            (None, 256)               1179904   \n",
            "_________________________________________________________________\n",
            "batch_normalization_94 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_206 (Dropout)        (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_155 (Dense)            (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_95 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_207 (Dropout)        (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_156 (Dense)            (None, 7)                 3591      \n",
            "=================================================================\n",
            "Total params: 1,606,375\n",
            "Trainable params: 1,603,943\n",
            "Non-trainable params: 2,432\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1oHhfF0Ujh2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7def51-a3c3-4a1b-e69e-9ff6bf42f7c9"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "checkpointer_m3 = ModelCheckpoint(filepath='m3.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_m3 = model_m3.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(t_inputs , t_targets),\n",
        "          epochs=epoch, batch_size=128, callbacks=[checkpointer_m3], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "225/225 [==============================] - ETA: 0s - loss: 2.0657 - accuracy: 0.2782\n",
            "Epoch 00001: val_loss improved from inf to 2.12794, saving model to m3.hdf5\n",
            "225/225 [==============================] - 7s 33ms/step - loss: 2.0657 - accuracy: 0.2782 - val_loss: 2.1279 - val_accuracy: 0.1744\n",
            "Epoch 2/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.5989 - accuracy: 0.3981\n",
            "Epoch 00002: val_loss improved from 2.12794 to 1.83669, saving model to m3.hdf5\n",
            "225/225 [==============================] - 7s 32ms/step - loss: 1.5981 - accuracy: 0.3984 - val_loss: 1.8367 - val_accuracy: 0.2778\n",
            "Epoch 3/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.4024 - accuracy: 0.4639\n",
            "Epoch 00003: val_loss improved from 1.83669 to 1.50372, saving model to m3.hdf5\n",
            "225/225 [==============================] - 7s 32ms/step - loss: 1.4019 - accuracy: 0.4639 - val_loss: 1.5037 - val_accuracy: 0.4291\n",
            "Epoch 4/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.3008 - accuracy: 0.5009\n",
            "Epoch 00004: val_loss improved from 1.50372 to 1.26997, saving model to m3.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.3006 - accuracy: 0.5010 - val_loss: 1.2700 - val_accuracy: 0.5054\n",
            "Epoch 5/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.2213 - accuracy: 0.5331\n",
            "Epoch 00005: val_loss improved from 1.26997 to 1.18365, saving model to m3.hdf5\n",
            "225/225 [==============================] - 7s 32ms/step - loss: 1.2214 - accuracy: 0.5333 - val_loss: 1.1836 - val_accuracy: 0.5453\n",
            "Epoch 6/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1739 - accuracy: 0.5547\n",
            "Epoch 00006: val_loss improved from 1.18365 to 1.13198, saving model to m3.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.1735 - accuracy: 0.5550 - val_loss: 1.1320 - val_accuracy: 0.5662\n",
            "Epoch 7/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1249 - accuracy: 0.5699\n",
            "Epoch 00007: val_loss improved from 1.13198 to 1.08913, saving model to m3.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.1252 - accuracy: 0.5697 - val_loss: 1.0891 - val_accuracy: 0.5857\n",
            "Epoch 8/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0956 - accuracy: 0.5842\n",
            "Epoch 00008: val_loss did not improve from 1.08913\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.0956 - accuracy: 0.5843 - val_loss: 1.1241 - val_accuracy: 0.5779\n",
            "Epoch 9/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0602 - accuracy: 0.5974\n",
            "Epoch 00009: val_loss improved from 1.08913 to 1.08237, saving model to m3.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.0604 - accuracy: 0.5974 - val_loss: 1.0824 - val_accuracy: 0.5921\n",
            "Epoch 10/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0339 - accuracy: 0.6104\n",
            "Epoch 00010: val_loss improved from 1.08237 to 1.06801, saving model to m3.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.0343 - accuracy: 0.6102 - val_loss: 1.0680 - val_accuracy: 0.6016\n",
            "Epoch 11/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0009 - accuracy: 0.6201\n",
            "Epoch 00011: val_loss improved from 1.06801 to 1.02194, saving model to m3.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.0011 - accuracy: 0.6200 - val_loss: 1.0219 - val_accuracy: 0.6199\n",
            "Epoch 12/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9721 - accuracy: 0.6350\n",
            "Epoch 00012: val_loss did not improve from 1.02194\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.9726 - accuracy: 0.6349 - val_loss: 1.0564 - val_accuracy: 0.6085\n",
            "Epoch 13/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9399 - accuracy: 0.6452\n",
            "Epoch 00013: val_loss did not improve from 1.02194\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.9404 - accuracy: 0.6451 - val_loss: 1.0373 - val_accuracy: 0.6124\n",
            "Epoch 14/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9141 - accuracy: 0.6568\n",
            "Epoch 00014: val_loss did not improve from 1.02194\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.9139 - accuracy: 0.6569 - val_loss: 1.0260 - val_accuracy: 0.6233\n",
            "Epoch 15/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8872 - accuracy: 0.6684\n",
            "Epoch 00015: val_loss did not improve from 1.02194\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.8861 - accuracy: 0.6688 - val_loss: 1.0780 - val_accuracy: 0.6074\n",
            "Epoch 16/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8603 - accuracy: 0.6798\n",
            "Epoch 00016: val_loss improved from 1.02194 to 0.99081, saving model to m3.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.8598 - accuracy: 0.6801 - val_loss: 0.9908 - val_accuracy: 0.6431\n",
            "Epoch 17/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8315 - accuracy: 0.6891\n",
            "Epoch 00017: val_loss did not improve from 0.99081\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.8320 - accuracy: 0.6889 - val_loss: 1.0138 - val_accuracy: 0.6408\n",
            "Epoch 18/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8089 - accuracy: 0.6971\n",
            "Epoch 00018: val_loss did not improve from 0.99081\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.8089 - accuracy: 0.6972 - val_loss: 1.0282 - val_accuracy: 0.6272\n",
            "Epoch 19/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7778 - accuracy: 0.7101\n",
            "Epoch 00019: val_loss did not improve from 0.99081\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.7783 - accuracy: 0.7099 - val_loss: 1.0562 - val_accuracy: 0.6286\n",
            "Epoch 20/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7618 - accuracy: 0.7155\n",
            "Epoch 00020: val_loss improved from 0.99081 to 0.98293, saving model to m3.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.7619 - accuracy: 0.7155 - val_loss: 0.9829 - val_accuracy: 0.6509\n",
            "Epoch 21/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7357 - accuracy: 0.7265\n",
            "Epoch 00021: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.7357 - accuracy: 0.7265 - val_loss: 1.0228 - val_accuracy: 0.6330\n",
            "Epoch 22/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7030 - accuracy: 0.7364\n",
            "Epoch 00022: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.7035 - accuracy: 0.7362 - val_loss: 1.0163 - val_accuracy: 0.6498\n",
            "Epoch 23/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6799 - accuracy: 0.7434\n",
            "Epoch 00023: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.6803 - accuracy: 0.7435 - val_loss: 1.0264 - val_accuracy: 0.6506\n",
            "Epoch 24/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6447 - accuracy: 0.7617\n",
            "Epoch 00024: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.6451 - accuracy: 0.7616 - val_loss: 1.0641 - val_accuracy: 0.6364\n",
            "Epoch 25/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6346 - accuracy: 0.7629\n",
            "Epoch 00025: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.6353 - accuracy: 0.7625 - val_loss: 1.0380 - val_accuracy: 0.6378\n",
            "Epoch 26/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6213 - accuracy: 0.7712\n",
            "Epoch 00026: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.6218 - accuracy: 0.7708 - val_loss: 1.0593 - val_accuracy: 0.6428\n",
            "Epoch 27/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6022 - accuracy: 0.7769\n",
            "Epoch 00027: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.6031 - accuracy: 0.7766 - val_loss: 1.0936 - val_accuracy: 0.6453\n",
            "Epoch 28/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5793 - accuracy: 0.7848\n",
            "Epoch 00028: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.5802 - accuracy: 0.7845 - val_loss: 1.0314 - val_accuracy: 0.6598\n",
            "Epoch 29/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5517 - accuracy: 0.7963\n",
            "Epoch 00029: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.5521 - accuracy: 0.7963 - val_loss: 1.0623 - val_accuracy: 0.6551\n",
            "Epoch 30/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5364 - accuracy: 0.8026\n",
            "Epoch 00030: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.5370 - accuracy: 0.8025 - val_loss: 1.0712 - val_accuracy: 0.6562\n",
            "Epoch 31/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5251 - accuracy: 0.8073\n",
            "Epoch 00031: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.5250 - accuracy: 0.8073 - val_loss: 1.1269 - val_accuracy: 0.6484\n",
            "Epoch 32/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5002 - accuracy: 0.8153\n",
            "Epoch 00032: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.4995 - accuracy: 0.8157 - val_loss: 1.0815 - val_accuracy: 0.6587\n",
            "Epoch 33/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4870 - accuracy: 0.8214\n",
            "Epoch 00033: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.4876 - accuracy: 0.8211 - val_loss: 1.1026 - val_accuracy: 0.6484\n",
            "Epoch 34/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4792 - accuracy: 0.8225\n",
            "Epoch 00034: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.4793 - accuracy: 0.8225 - val_loss: 1.1114 - val_accuracy: 0.6617\n",
            "Epoch 35/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4697 - accuracy: 0.8271\n",
            "Epoch 00035: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.4697 - accuracy: 0.8271 - val_loss: 1.1186 - val_accuracy: 0.6567\n",
            "Epoch 36/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4475 - accuracy: 0.8337\n",
            "Epoch 00036: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.4477 - accuracy: 0.8336 - val_loss: 1.1533 - val_accuracy: 0.6542\n",
            "Epoch 37/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4458 - accuracy: 0.8367\n",
            "Epoch 00037: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.4455 - accuracy: 0.8369 - val_loss: 1.1696 - val_accuracy: 0.6629\n",
            "Epoch 38/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4298 - accuracy: 0.8436\n",
            "Epoch 00038: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.4301 - accuracy: 0.8435 - val_loss: 1.1828 - val_accuracy: 0.6481\n",
            "Epoch 39/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4174 - accuracy: 0.8474\n",
            "Epoch 00039: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.4172 - accuracy: 0.8475 - val_loss: 1.2357 - val_accuracy: 0.6542\n",
            "Epoch 40/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4102 - accuracy: 0.8495\n",
            "Epoch 00040: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.4105 - accuracy: 0.8495 - val_loss: 1.1488 - val_accuracy: 0.6592\n",
            "Epoch 41/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4041 - accuracy: 0.8522\n",
            "Epoch 00041: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.4043 - accuracy: 0.8521 - val_loss: 1.1656 - val_accuracy: 0.6578\n",
            "Epoch 42/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3921 - accuracy: 0.8581\n",
            "Epoch 00042: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.3920 - accuracy: 0.8580 - val_loss: 1.1771 - val_accuracy: 0.6645\n",
            "Epoch 43/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3976 - accuracy: 0.8536\n",
            "Epoch 00043: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.3976 - accuracy: 0.8536 - val_loss: 1.2114 - val_accuracy: 0.6503\n",
            "Epoch 44/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3701 - accuracy: 0.8651\n",
            "Epoch 00044: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.3700 - accuracy: 0.8651 - val_loss: 1.2355 - val_accuracy: 0.6539\n",
            "Epoch 45/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3562 - accuracy: 0.8734\n",
            "Epoch 00045: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.3563 - accuracy: 0.8732 - val_loss: 1.2100 - val_accuracy: 0.6676\n",
            "Epoch 46/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3563 - accuracy: 0.8693\n",
            "Epoch 00046: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.3566 - accuracy: 0.8691 - val_loss: 1.2014 - val_accuracy: 0.6592\n",
            "Epoch 47/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3457 - accuracy: 0.8734\n",
            "Epoch 00047: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.3459 - accuracy: 0.8733 - val_loss: 1.3402 - val_accuracy: 0.6456\n",
            "Epoch 48/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3455 - accuracy: 0.8727\n",
            "Epoch 00048: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.3463 - accuracy: 0.8724 - val_loss: 1.2515 - val_accuracy: 0.6539\n",
            "Epoch 49/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3363 - accuracy: 0.8774\n",
            "Epoch 00049: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.3364 - accuracy: 0.8773 - val_loss: 1.2508 - val_accuracy: 0.6590\n",
            "Epoch 50/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3203 - accuracy: 0.8854\n",
            "Epoch 00050: val_loss did not improve from 0.98293\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.3205 - accuracy: 0.8854 - val_loss: 1.2886 - val_accuracy: 0.6565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwesRPLanxUg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e772503-14c3-4bde-a1ab-6d80cd0cbbf8"
      },
      "source": [
        "loaded = load_model(\"m3.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113/113 [==============================] - 1s 5ms/step - loss: 0.9829 - accuracy: 0.6509\n",
            "0.6508776545524597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbIW1lyz7Ka1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "outputId": "c2ec284a-8527-484e-a53a-558beffb2a09"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(hist_dropoutf.history['val_loss'][:50], label='FER-CNN6 Kernel Size (2,2)')\n",
        "plt.plot(hist_m1.history['val_loss'], label='FER-CNN6 Kernel Size (3,3)')\n",
        "plt.plot(hist_m2.history['val_loss'], label='FER-CNN9 Kernel Size (2,2)')\n",
        "plt.plot(hist_m3.history['val_loss'], label='FER-CNN9 Kernel Size (3,3)')\n",
        "\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"validation losses\")\n",
        "plt.show()\n",
        "\n",
        "plt.clf()\n",
        "plt.plot(hist_dropoutf.history['val_accuracy'][:50], label='FER-CNN6 Kernel Size (2,2)')\n",
        "plt.plot(hist_m1.history['val_accuracy'], label='FER-CNN6 Kernel Size (3,3)')\n",
        "plt.plot(hist_m2.history['val_accuracy'], label='FER-CNN9 Kernel Size (2,2)')\n",
        "plt.plot(hist_m3.history['val_accuracy'], label='FER-CNN9 Kernel Size (3,3)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"validation accuracy\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3hx_YGxAWQE"
      },
      "source": [
        "## Test number of filters and number of neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGEposnM_qVu"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
        "from keras import layers\n",
        "\n",
        "model_m4 = Sequential()\n",
        "model_m4.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "model_m4.add(BatchNormalization())\n",
        "model_m4.add(MaxPooling2D(pool_size=2))\n",
        "model_m4.add(Dropout(0.35))\n",
        "model_m4.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
        "model_m4.add(BatchNormalization())\n",
        "model_m4.add(MaxPooling2D(pool_size=2))\n",
        "model_m4.add(Dropout(0.35))\n",
        "model_m4.add(Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
        "model_m4.add(BatchNormalization())\n",
        "model_m4.add(MaxPooling2D(pool_size=2))\n",
        "model_m4.add(Dropout(0.35))\n",
        "model_m4.add(Flatten())\n",
        "model_m4.add(Dense(512,activation='relu'))\n",
        "model_m4.add(BatchNormalization())\n",
        "model_m4.add(Dropout(0.35))\n",
        "model_m4.add(Dense(1024,activation='relu'))\n",
        "model_m4.add(BatchNormalization())\n",
        "model_m4.add(Dropout(0.35))\n",
        "model_m4.add(Dense(7,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL9PmJsb_qbT"
      },
      "source": [
        "model_m4.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-gF3l2y_qhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e3c7373-6de9-4644-815f-e1c605cffca6"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "checkpointer_m4 = ModelCheckpoint(filepath='m4.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_m4 = model_m4.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(t_inputs , t_targets),\n",
        "          epochs=epoch, batch_size=128, callbacks=[checkpointer_m4], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 2.1230 - accuracy: 0.2693\n",
            "Epoch 00001: val_loss improved from inf to 4.54363, saving model to m4.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 2.1212 - accuracy: 0.2695 - val_loss: 4.5436 - val_accuracy: 0.2424\n",
            "Epoch 2/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.6614 - accuracy: 0.3858\n",
            "Epoch 00002: val_loss improved from 4.54363 to 3.98323, saving model to m4.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.6610 - accuracy: 0.3859 - val_loss: 3.9832 - val_accuracy: 0.1789\n",
            "Epoch 3/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.4301 - accuracy: 0.4568\n",
            "Epoch 00003: val_loss improved from 3.98323 to 1.53498, saving model to m4.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.4295 - accuracy: 0.4568 - val_loss: 1.5350 - val_accuracy: 0.4110\n",
            "Epoch 4/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.3184 - accuracy: 0.5004\n",
            "Epoch 00004: val_loss improved from 1.53498 to 1.47478, saving model to m4.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.3189 - accuracy: 0.5000 - val_loss: 1.4748 - val_accuracy: 0.4620\n",
            "Epoch 5/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.2351 - accuracy: 0.5320\n",
            "Epoch 00005: val_loss improved from 1.47478 to 1.37684, saving model to m4.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.2352 - accuracy: 0.5320 - val_loss: 1.3768 - val_accuracy: 0.4765\n",
            "Epoch 6/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1676 - accuracy: 0.5570\n",
            "Epoch 00006: val_loss improved from 1.37684 to 1.17144, saving model to m4.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.1685 - accuracy: 0.5563 - val_loss: 1.1714 - val_accuracy: 0.5581\n",
            "Epoch 7/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.1223 - accuracy: 0.5741\n",
            "Epoch 00007: val_loss improved from 1.17144 to 1.16680, saving model to m4.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 1.1224 - accuracy: 0.5740 - val_loss: 1.1668 - val_accuracy: 0.5581\n",
            "Epoch 8/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0963 - accuracy: 0.5850\n",
            "Epoch 00008: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.0970 - accuracy: 0.5846 - val_loss: 1.3549 - val_accuracy: 0.4865\n",
            "Epoch 9/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 1.0291 - accuracy: 0.6113\n",
            "Epoch 00009: val_loss did not improve from 1.16680\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 1.0297 - accuracy: 0.6112 - val_loss: 1.1787 - val_accuracy: 0.5720\n",
            "Epoch 10/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9878 - accuracy: 0.6296\n",
            "Epoch 00010: val_loss improved from 1.16680 to 1.12018, saving model to m4.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.9876 - accuracy: 0.6299 - val_loss: 1.1202 - val_accuracy: 0.5901\n",
            "Epoch 11/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.9410 - accuracy: 0.6473\n",
            "Epoch 00011: val_loss did not improve from 1.12018\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.9419 - accuracy: 0.6470 - val_loss: 1.1798 - val_accuracy: 0.5768\n",
            "Epoch 12/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8874 - accuracy: 0.6665\n",
            "Epoch 00012: val_loss did not improve from 1.12018\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.8873 - accuracy: 0.6663 - val_loss: 1.1422 - val_accuracy: 0.5993\n",
            "Epoch 13/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.8420 - accuracy: 0.6857\n",
            "Epoch 00013: val_loss did not improve from 1.12018\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.8417 - accuracy: 0.6859 - val_loss: 1.3744 - val_accuracy: 0.5238\n",
            "Epoch 14/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7916 - accuracy: 0.7049\n",
            "Epoch 00014: val_loss did not improve from 1.12018\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.7922 - accuracy: 0.7047 - val_loss: 1.2520 - val_accuracy: 0.5690\n",
            "Epoch 15/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.7453 - accuracy: 0.7232\n",
            "Epoch 00015: val_loss did not improve from 1.12018\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.7462 - accuracy: 0.7228 - val_loss: 1.2701 - val_accuracy: 0.5637\n",
            "Epoch 16/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6978 - accuracy: 0.7420\n",
            "Epoch 00016: val_loss improved from 1.12018 to 1.12006, saving model to m4.hdf5\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.6986 - accuracy: 0.7416 - val_loss: 1.1201 - val_accuracy: 0.6283\n",
            "Epoch 17/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6524 - accuracy: 0.7585\n",
            "Epoch 00017: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.6531 - accuracy: 0.7582 - val_loss: 1.1895 - val_accuracy: 0.6155\n",
            "Epoch 18/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.6126 - accuracy: 0.7758\n",
            "Epoch 00018: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.6129 - accuracy: 0.7759 - val_loss: 1.1882 - val_accuracy: 0.6074\n",
            "Epoch 19/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5802 - accuracy: 0.7871\n",
            "Epoch 00019: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.5803 - accuracy: 0.7871 - val_loss: 1.1846 - val_accuracy: 0.6099\n",
            "Epoch 20/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5347 - accuracy: 0.8042\n",
            "Epoch 00020: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.5351 - accuracy: 0.8040 - val_loss: 1.4499 - val_accuracy: 0.5492\n",
            "Epoch 21/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.5119 - accuracy: 0.8143\n",
            "Epoch 00021: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.5125 - accuracy: 0.8142 - val_loss: 1.2196 - val_accuracy: 0.6169\n",
            "Epoch 22/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4651 - accuracy: 0.8322\n",
            "Epoch 00022: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.4658 - accuracy: 0.8318 - val_loss: 1.2536 - val_accuracy: 0.6261\n",
            "Epoch 23/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4466 - accuracy: 0.8363\n",
            "Epoch 00023: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.4467 - accuracy: 0.8363 - val_loss: 1.2641 - val_accuracy: 0.6300\n",
            "Epoch 24/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4306 - accuracy: 0.8430\n",
            "Epoch 00024: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.4313 - accuracy: 0.8428 - val_loss: 1.3237 - val_accuracy: 0.6007\n",
            "Epoch 25/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.4241 - accuracy: 0.8488\n",
            "Epoch 00025: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.4240 - accuracy: 0.8489 - val_loss: 1.3355 - val_accuracy: 0.6319\n",
            "Epoch 26/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3801 - accuracy: 0.8598\n",
            "Epoch 00026: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.3801 - accuracy: 0.8600 - val_loss: 1.4195 - val_accuracy: 0.6080\n",
            "Epoch 27/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3697 - accuracy: 0.8647\n",
            "Epoch 00027: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.3702 - accuracy: 0.8645 - val_loss: 1.3366 - val_accuracy: 0.5854\n",
            "Epoch 28/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3566 - accuracy: 0.8719\n",
            "Epoch 00028: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.3564 - accuracy: 0.8718 - val_loss: 1.4204 - val_accuracy: 0.6269\n",
            "Epoch 29/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3408 - accuracy: 0.8774\n",
            "Epoch 00029: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.3409 - accuracy: 0.8774 - val_loss: 1.4103 - val_accuracy: 0.6269\n",
            "Epoch 30/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3222 - accuracy: 0.8841\n",
            "Epoch 00030: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.3226 - accuracy: 0.8840 - val_loss: 1.4711 - val_accuracy: 0.6071\n",
            "Epoch 31/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3065 - accuracy: 0.8904\n",
            "Epoch 00031: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.3066 - accuracy: 0.8903 - val_loss: 1.4414 - val_accuracy: 0.6258\n",
            "Epoch 32/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.8950\n",
            "Epoch 00032: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.2912 - accuracy: 0.8949 - val_loss: 1.3974 - val_accuracy: 0.6227\n",
            "Epoch 33/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.8911\n",
            "Epoch 00033: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.3033 - accuracy: 0.8911 - val_loss: 1.4648 - val_accuracy: 0.6311\n",
            "Epoch 34/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2680 - accuracy: 0.9035\n",
            "Epoch 00034: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.2686 - accuracy: 0.9033 - val_loss: 1.5610 - val_accuracy: 0.6250\n",
            "Epoch 35/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2700 - accuracy: 0.9030\n",
            "Epoch 00035: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.2699 - accuracy: 0.9030 - val_loss: 1.5395 - val_accuracy: 0.6328\n",
            "Epoch 36/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2591 - accuracy: 0.9073\n",
            "Epoch 00036: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.2585 - accuracy: 0.9075 - val_loss: 1.5387 - val_accuracy: 0.6431\n",
            "Epoch 37/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2531 - accuracy: 0.9108\n",
            "Epoch 00037: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.2535 - accuracy: 0.9107 - val_loss: 1.5929 - val_accuracy: 0.6314\n",
            "Epoch 38/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2584 - accuracy: 0.9107\n",
            "Epoch 00038: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.2590 - accuracy: 0.9105 - val_loss: 1.4891 - val_accuracy: 0.6372\n",
            "Epoch 39/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9192\n",
            "Epoch 00039: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.2258 - accuracy: 0.9192 - val_loss: 1.5552 - val_accuracy: 0.6336\n",
            "Epoch 40/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2330 - accuracy: 0.9187\n",
            "Epoch 00040: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.2334 - accuracy: 0.9186 - val_loss: 1.5413 - val_accuracy: 0.6353\n",
            "Epoch 41/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2315 - accuracy: 0.9182\n",
            "Epoch 00041: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.2314 - accuracy: 0.9183 - val_loss: 1.6180 - val_accuracy: 0.6202\n",
            "Epoch 42/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2153 - accuracy: 0.9238\n",
            "Epoch 00042: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.2152 - accuracy: 0.9237 - val_loss: 1.6115 - val_accuracy: 0.6266\n",
            "Epoch 43/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2134 - accuracy: 0.9237\n",
            "Epoch 00043: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 31ms/step - loss: 0.2139 - accuracy: 0.9236 - val_loss: 1.6840 - val_accuracy: 0.6389\n",
            "Epoch 44/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.2052 - accuracy: 0.9274\n",
            "Epoch 00044: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.2052 - accuracy: 0.9274 - val_loss: 1.6540 - val_accuracy: 0.6213\n",
            "Epoch 45/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1977 - accuracy: 0.9304\n",
            "Epoch 00045: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.1983 - accuracy: 0.9301 - val_loss: 1.6456 - val_accuracy: 0.6303\n",
            "Epoch 46/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1952 - accuracy: 0.9302\n",
            "Epoch 00046: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.1954 - accuracy: 0.9303 - val_loss: 1.6087 - val_accuracy: 0.6216\n",
            "Epoch 47/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1993 - accuracy: 0.9296\n",
            "Epoch 00047: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.1995 - accuracy: 0.9295 - val_loss: 1.5764 - val_accuracy: 0.6275\n",
            "Epoch 48/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1971 - accuracy: 0.9313\n",
            "Epoch 00048: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.1974 - accuracy: 0.9312 - val_loss: 1.6896 - val_accuracy: 0.6233\n",
            "Epoch 49/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1904 - accuracy: 0.9323\n",
            "Epoch 00049: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.1901 - accuracy: 0.9323 - val_loss: 1.6520 - val_accuracy: 0.6319\n",
            "Epoch 50/50\n",
            "223/225 [============================>.] - ETA: 0s - loss: 0.1819 - accuracy: 0.9358\n",
            "Epoch 00050: val_loss did not improve from 1.12006\n",
            "225/225 [==============================] - 7s 30ms/step - loss: 0.1824 - accuracy: 0.9357 - val_loss: 1.6522 - val_accuracy: 0.6439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_gmP4WuAhqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08c594a7-9cde-478d-a273-52e6a3fcec87"
      },
      "source": [
        "loaded = load_model(\"m4.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113/113 [==============================] - 1s 5ms/step - loss: 1.1201 - accuracy: 0.6283\n",
            "0.6283087134361267\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_TVm7Qf_qnS"
      },
      "source": [
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
        "from keras import layers\n",
        "\n",
        "model_m5 = Sequential()\n",
        "\n",
        "model_m5.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "model_m5.add(BatchNormalization())\n",
        "model_m5.add(Conv2D(filters=64,kernel_size=3,padding='same',activation='relu',input_shape=(48,48,1)))\n",
        "model_m5.add(BatchNormalization())\n",
        "model_m5.add(MaxPooling2D(pool_size=2))\n",
        "model_m5.add(Dropout(0.35))\n",
        "model_m5.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
        "model_m5.add(BatchNormalization())\n",
        "model_m5.add(Conv2D(filters=128,kernel_size=3,padding='same',activation='relu'))\n",
        "model_m5.add(BatchNormalization())\n",
        "model_m5.add(MaxPooling2D(pool_size=2))\n",
        "model_m5.add(Dropout(0.35))\n",
        "model_m5.add(Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
        "model_m5.add(BatchNormalization())\n",
        "model_m5.add(Conv2D(filters=256,kernel_size=3,padding='same',activation='relu'))\n",
        "model_m5.add(BatchNormalization())\n",
        "model_m5.add(MaxPooling2D(pool_size=2))\n",
        "model_m5.add(Dropout(0.35))\n",
        "model_m5.add(Flatten())\n",
        "model_m5.add(Dense(512,activation='relu'))\n",
        "model_m5.add(BatchNormalization())\n",
        "model_m5.add(Dropout(0.35))\n",
        "model_m5.add(Dense(1024,activation='relu'))\n",
        "model_m5.add(BatchNormalization())\n",
        "model_m5.add(Dropout(0.35))\n",
        "model_m5.add(Dense(7,activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kstFsgbo_qsm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c23d6e-34b9-45ed-a2c4-144fddd12acb"
      },
      "source": [
        "model_m5.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model_m5.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_54\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_173 (Conv2D)          (None, 48, 48, 64)        640       \n",
            "_________________________________________________________________\n",
            "batch_normalization_101 (Bat (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_174 (Conv2D)          (None, 48, 48, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_102 (Bat (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_155 (MaxPoolin (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_213 (Dropout)        (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_175 (Conv2D)          (None, 24, 24, 128)       73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_103 (Bat (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_176 (Conv2D)          (None, 24, 24, 128)       147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_104 (Bat (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_156 (MaxPoolin (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_214 (Dropout)        (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_177 (Conv2D)          (None, 12, 12, 256)       295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_105 (Bat (None, 12, 12, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_178 (Conv2D)          (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_106 (Bat (None, 12, 12, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_157 (MaxPoolin (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_215 (Dropout)        (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_54 (Flatten)         (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_160 (Dense)            (None, 512)               4719104   \n",
            "_________________________________________________________________\n",
            "batch_normalization_107 (Bat (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_216 (Dropout)        (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_161 (Dense)            (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "batch_normalization_108 (Bat (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dropout_217 (Dropout)        (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_162 (Dense)            (None, 7)                 7175      \n",
            "=================================================================\n",
            "Total params: 6,405,575\n",
            "Trainable params: 6,400,711\n",
            "Non-trainable params: 4,864\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lI3pxBw_qxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "883ffa3d-d32c-47c7-c48a-b38e7843f64d"
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "epoch = 50\n",
        "\n",
        "checkpointer_m5 = ModelCheckpoint(filepath='m5.hdf5',verbose=1,save_best_only= True)\n",
        "\n",
        "hist_m5 = model_m5.fit(tr_inputs, tr_targets , \n",
        "          validation_data=(t_inputs , t_targets),\n",
        "          epochs=epoch, batch_size=128, callbacks=[checkpointer_m5], verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "  2/225 [..............................] - ETA: 8s - loss: 2.9334 - accuracy: 0.1719"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0256s vs `on_train_batch_end` time: 0.0386s). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "225/225 [==============================] - ETA: 0s - loss: 2.0238 - accuracy: 0.3077\n",
            "Epoch 00001: val_loss improved from inf to 1.85764, saving model to m5.hdf5\n",
            "225/225 [==============================] - 16s 70ms/step - loss: 2.0238 - accuracy: 0.3077 - val_loss: 1.8576 - val_accuracy: 0.2474\n",
            "Epoch 2/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.5226 - accuracy: 0.4318\n",
            "Epoch 00002: val_loss did not improve from 1.85764\n",
            "225/225 [==============================] - 16s 69ms/step - loss: 1.5224 - accuracy: 0.4318 - val_loss: 1.8775 - val_accuracy: 0.2614\n",
            "Epoch 3/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.3110 - accuracy: 0.5075\n",
            "Epoch 00003: val_loss improved from 1.85764 to 1.40030, saving model to m5.hdf5\n",
            "225/225 [==============================] - 16s 71ms/step - loss: 1.3111 - accuracy: 0.5074 - val_loss: 1.4003 - val_accuracy: 0.4776\n",
            "Epoch 4/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.1845 - accuracy: 0.5538\n",
            "Epoch 00004: val_loss improved from 1.40030 to 1.16029, saving model to m5.hdf5\n",
            "225/225 [==============================] - 16s 70ms/step - loss: 1.1842 - accuracy: 0.5540 - val_loss: 1.1603 - val_accuracy: 0.5678\n",
            "Epoch 5/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.1156 - accuracy: 0.5777\n",
            "Epoch 00005: val_loss did not improve from 1.16029\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 1.1157 - accuracy: 0.5779 - val_loss: 1.1861 - val_accuracy: 0.5628\n",
            "Epoch 6/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.0606 - accuracy: 0.5979\n",
            "Epoch 00006: val_loss did not improve from 1.16029\n",
            "225/225 [==============================] - 15s 67ms/step - loss: 1.0607 - accuracy: 0.5979 - val_loss: 1.1766 - val_accuracy: 0.5642\n",
            "Epoch 7/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 1.0053 - accuracy: 0.6213\n",
            "Epoch 00007: val_loss did not improve from 1.16029\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 1.0050 - accuracy: 0.6214 - val_loss: 1.1977 - val_accuracy: 0.5690\n",
            "Epoch 8/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9715 - accuracy: 0.6327\n",
            "Epoch 00008: val_loss did not improve from 1.16029\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.9712 - accuracy: 0.6329 - val_loss: 1.1802 - val_accuracy: 0.5834\n",
            "Epoch 9/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.9199 - accuracy: 0.6540\n",
            "Epoch 00009: val_loss improved from 1.16029 to 1.13961, saving model to m5.hdf5\n",
            "225/225 [==============================] - 16s 69ms/step - loss: 0.9201 - accuracy: 0.6539 - val_loss: 1.1396 - val_accuracy: 0.5876\n",
            "Epoch 10/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.8668 - accuracy: 0.6772\n",
            "Epoch 00010: val_loss improved from 1.13961 to 1.10202, saving model to m5.hdf5\n",
            "225/225 [==============================] - 15s 69ms/step - loss: 0.8671 - accuracy: 0.6771 - val_loss: 1.1020 - val_accuracy: 0.6102\n",
            "Epoch 11/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.8261 - accuracy: 0.6905\n",
            "Epoch 00011: val_loss improved from 1.10202 to 1.06500, saving model to m5.hdf5\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.8259 - accuracy: 0.6906 - val_loss: 1.0650 - val_accuracy: 0.6225\n",
            "Epoch 12/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7758 - accuracy: 0.7079\n",
            "Epoch 00012: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.7758 - accuracy: 0.7080 - val_loss: 1.4396 - val_accuracy: 0.5252\n",
            "Epoch 13/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.7251 - accuracy: 0.7333\n",
            "Epoch 00013: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.7250 - accuracy: 0.7333 - val_loss: 1.0819 - val_accuracy: 0.6339\n",
            "Epoch 14/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6640 - accuracy: 0.7533\n",
            "Epoch 00014: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.6639 - accuracy: 0.7533 - val_loss: 1.1853 - val_accuracy: 0.6202\n",
            "Epoch 15/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.6328 - accuracy: 0.7631\n",
            "Epoch 00015: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.6335 - accuracy: 0.7629 - val_loss: 1.1373 - val_accuracy: 0.6286\n",
            "Epoch 16/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5782 - accuracy: 0.7850\n",
            "Epoch 00016: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.5784 - accuracy: 0.7850 - val_loss: 1.1506 - val_accuracy: 0.6319\n",
            "Epoch 17/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.5271 - accuracy: 0.8082\n",
            "Epoch 00017: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.5274 - accuracy: 0.8082 - val_loss: 1.1641 - val_accuracy: 0.6506\n",
            "Epoch 18/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4923 - accuracy: 0.8187\n",
            "Epoch 00018: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.4926 - accuracy: 0.8186 - val_loss: 1.1935 - val_accuracy: 0.6431\n",
            "Epoch 19/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4490 - accuracy: 0.8379\n",
            "Epoch 00019: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 67ms/step - loss: 0.4490 - accuracy: 0.8379 - val_loss: 1.2896 - val_accuracy: 0.6375\n",
            "Epoch 20/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.4139 - accuracy: 0.8489\n",
            "Epoch 00020: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.4139 - accuracy: 0.8489 - val_loss: 1.4529 - val_accuracy: 0.6191\n",
            "Epoch 21/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3695 - accuracy: 0.8670\n",
            "Epoch 00021: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 67ms/step - loss: 0.3696 - accuracy: 0.8670 - val_loss: 1.2926 - val_accuracy: 0.6447\n",
            "Epoch 22/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3536 - accuracy: 0.8728\n",
            "Epoch 00022: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 67ms/step - loss: 0.3535 - accuracy: 0.8728 - val_loss: 1.3182 - val_accuracy: 0.6339\n",
            "Epoch 23/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3207 - accuracy: 0.8833\n",
            "Epoch 00023: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.3209 - accuracy: 0.8832 - val_loss: 1.3502 - val_accuracy: 0.6548\n",
            "Epoch 24/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.3038 - accuracy: 0.8902\n",
            "Epoch 00024: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.3041 - accuracy: 0.8901 - val_loss: 1.3543 - val_accuracy: 0.6447\n",
            "Epoch 25/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2847 - accuracy: 0.8975\n",
            "Epoch 00025: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.2848 - accuracy: 0.8975 - val_loss: 1.3724 - val_accuracy: 0.6525\n",
            "Epoch 26/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.9080\n",
            "Epoch 00026: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.2602 - accuracy: 0.9080 - val_loss: 1.5108 - val_accuracy: 0.6486\n",
            "Epoch 27/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2565 - accuracy: 0.9089\n",
            "Epoch 00027: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 67ms/step - loss: 0.2565 - accuracy: 0.9089 - val_loss: 1.5229 - val_accuracy: 0.6408\n",
            "Epoch 28/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2256 - accuracy: 0.9197\n",
            "Epoch 00028: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.2260 - accuracy: 0.9195 - val_loss: 1.5146 - val_accuracy: 0.6520\n",
            "Epoch 29/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2341 - accuracy: 0.9163\n",
            "Epoch 00029: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.2341 - accuracy: 0.9163 - val_loss: 1.5298 - val_accuracy: 0.6367\n",
            "Epoch 30/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2231 - accuracy: 0.9203\n",
            "Epoch 00030: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.2232 - accuracy: 0.9203 - val_loss: 1.5074 - val_accuracy: 0.6509\n",
            "Epoch 31/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.2013 - accuracy: 0.9293\n",
            "Epoch 00031: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.2013 - accuracy: 0.9293 - val_loss: 1.5101 - val_accuracy: 0.6461\n",
            "Epoch 32/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1976 - accuracy: 0.9313\n",
            "Epoch 00032: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 67ms/step - loss: 0.1976 - accuracy: 0.9313 - val_loss: 1.5792 - val_accuracy: 0.6573\n",
            "Epoch 33/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1890 - accuracy: 0.9329\n",
            "Epoch 00033: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 67ms/step - loss: 0.1891 - accuracy: 0.9329 - val_loss: 1.5790 - val_accuracy: 0.6509\n",
            "Epoch 34/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1849 - accuracy: 0.9344\n",
            "Epoch 00034: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1851 - accuracy: 0.9343 - val_loss: 1.5694 - val_accuracy: 0.6587\n",
            "Epoch 35/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1716 - accuracy: 0.9408\n",
            "Epoch 00035: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1722 - accuracy: 0.9407 - val_loss: 1.6320 - val_accuracy: 0.6553\n",
            "Epoch 36/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1680 - accuracy: 0.9414\n",
            "Epoch 00036: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1681 - accuracy: 0.9414 - val_loss: 1.6216 - val_accuracy: 0.6445\n",
            "Epoch 37/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1727 - accuracy: 0.9405\n",
            "Epoch 00037: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1727 - accuracy: 0.9405 - val_loss: 1.5529 - val_accuracy: 0.6567\n",
            "Epoch 38/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1601 - accuracy: 0.9436\n",
            "Epoch 00038: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1603 - accuracy: 0.9435 - val_loss: 1.6129 - val_accuracy: 0.6578\n",
            "Epoch 39/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1502 - accuracy: 0.9471\n",
            "Epoch 00039: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1502 - accuracy: 0.9471 - val_loss: 1.6934 - val_accuracy: 0.6486\n",
            "Epoch 40/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1458 - accuracy: 0.9484\n",
            "Epoch 00040: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1458 - accuracy: 0.9484 - val_loss: 1.7599 - val_accuracy: 0.6548\n",
            "Epoch 41/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1568 - accuracy: 0.9466\n",
            "Epoch 00041: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1567 - accuracy: 0.9466 - val_loss: 1.7323 - val_accuracy: 0.6411\n",
            "Epoch 42/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1487 - accuracy: 0.9478\n",
            "Epoch 00042: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1486 - accuracy: 0.9479 - val_loss: 1.6957 - val_accuracy: 0.6531\n",
            "Epoch 43/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1483 - accuracy: 0.9482\n",
            "Epoch 00043: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1484 - accuracy: 0.9482 - val_loss: 1.7236 - val_accuracy: 0.6456\n",
            "Epoch 44/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1390 - accuracy: 0.9515\n",
            "Epoch 00044: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1389 - accuracy: 0.9515 - val_loss: 1.7125 - val_accuracy: 0.6514\n",
            "Epoch 45/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1367 - accuracy: 0.9508\n",
            "Epoch 00045: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1366 - accuracy: 0.9508 - val_loss: 1.7059 - val_accuracy: 0.6498\n",
            "Epoch 46/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9568\n",
            "Epoch 00046: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1220 - accuracy: 0.9567 - val_loss: 1.7122 - val_accuracy: 0.6556\n",
            "Epoch 47/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9582\n",
            "Epoch 00047: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1252 - accuracy: 0.9581 - val_loss: 1.7647 - val_accuracy: 0.6584\n",
            "Epoch 48/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1422 - accuracy: 0.9497\n",
            "Epoch 00048: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 69ms/step - loss: 0.1422 - accuracy: 0.9496 - val_loss: 1.6830 - val_accuracy: 0.6581\n",
            "Epoch 49/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9562\n",
            "Epoch 00049: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1260 - accuracy: 0.9561 - val_loss: 1.6605 - val_accuracy: 0.6548\n",
            "Epoch 50/50\n",
            "224/225 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9584\n",
            "Epoch 00050: val_loss did not improve from 1.06500\n",
            "225/225 [==============================] - 15s 68ms/step - loss: 0.1208 - accuracy: 0.9583 - val_loss: 1.7761 - val_accuracy: 0.6615\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc5t5ZZh_q9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6d05df6-bee2-441d-f393-ed301cd276e2"
      },
      "source": [
        "loaded = load_model(\"m5.hdf5\")\n",
        "scores = loaded.evaluate(t_inputs, t_targets)\n",
        "print(scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "113/113 [==============================] - 1s 7ms/step - loss: 1.0650 - accuracy: 0.6225\n",
            "0.6224575042724609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-vr_4B1ZZA2"
      },
      "source": [
        "#Do something with the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NChXWv13ZpaE",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "54015c6b-d878-4346-cbfd-149e2757f76e"
      },
      "source": [
        "# upload best model \n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8458f790-fd45-4f8f-839b-be7fc52e4f39\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8458f790-fd45-4f8f-839b-be7fc52e4f39\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving bestCNN.hdf5 to bestCNN.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MEhw_9MMpMz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "outputId": "5c921c26-70ef-4310-a8f5-65fb5bc797cd"
      },
      "source": [
        "from keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "import seaborn\n",
        "\n",
        "loaded = load_model(\"bestCNN.hdf5\")\n",
        "res = loaded.predict(t_inputs)\n",
        "emotion_dict = {0: 'Angry', 1:'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6:'Neutral'}\n",
        "label = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "y_true = []\n",
        "for t in t_targets:\n",
        "  y_true.append(emotion_dict[np.argmax(t)])\n",
        "y_pred = []\n",
        "for t in res:\n",
        "  ind = np.argmax(t)\n",
        "  y_pred.append(emotion_dict[ind])\n",
        "\n",
        "# make heatmap\n",
        "cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "cm.dtype = np.float64\n",
        "for i in range(len(cm)):\n",
        "  s = np.sum(cm[i])\n",
        "  cm[i] = cm[i] / s\n",
        "df_cm = pd.DataFrame(cm, index=label, columns=label)\n",
        "seaborn.heatmap(df_cm, annot=True, cmap='coolwarm')\n",
        "plt.xlabel(\"prediction\")\n",
        "plt.ylabel(\"true label\")\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czXnlj5Xp-Zv",
        "outputId": "7ca4e7a7-70bc-4eac-ff54-5a8a43cbd21a"
      },
      "source": [
        "report = metrics.classification_report(y_true, y_pred, target_names=label)\n",
        "print(report)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Angry       0.60      0.52      0.56       491\n",
            "     Disgust       0.70      0.67      0.69        55\n",
            "        Fear       0.58      0.40      0.47       528\n",
            "       Happy       0.81      0.88      0.84       879\n",
            "         Sad       0.58      0.68      0.62       626\n",
            "    Surprise       0.49      0.54      0.52       594\n",
            "     Neutral       0.77      0.75      0.76       416\n",
            "\n",
            "    accuracy                           0.65      3589\n",
            "   macro avg       0.65      0.63      0.64      3589\n",
            "weighted avg       0.65      0.65      0.65      3589\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}